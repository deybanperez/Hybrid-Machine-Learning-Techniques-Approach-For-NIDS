---
title: "Pruebas de Redes Neuronales en el Conjunto de Entrenamiento"
author: "Deyban Andrés Pérez Abreu"
date: "August 28, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Resumen
Este documento recopila las actividades realizadas para las pruebas de rendimiento de **Redes Neuronales** de una capa intermedia con 20 neuronas en la tarea de la detección de anomalías en redes de computadoras. La selección del número de capas y de neuronas no está normado en ningún lugar, sin embargo,  según **Andrew Ng** en sus cursos de ML en la **Universidad de Stanford** se menciona que una arquitectura de red neuronal con 1 capa oculta funcionará de buena manera para la mayoría de los problemas. Para pequelas cantidades de entradas se utiliza el doble de neuronas en la capa intermedia, y para modelos con gran cantidad de entradas se utiliza la mitad de neuronas intermedias. Si se desea utilizar una segunda capa, entonces se recomienda que esta tenga la misma cantidad de neuronal que la capa oculta 1. 

Como consideraciones, es importante resaltar que si se utilizan muchas neuronas se tendrá un prblema de **sobre-ajuste** de los datos. Por otra parte si no se utilizan la cantidad suficiente de neuronas, entonces se tendrá un problema de **sub-ajuste**. Darse cuenta de esto es un problema empírico de ensayo, error y análisis de resultados por parte del especialista de ML.

#Actividades Realizadas
Se empezará por limpiar el ambiente de trabajo y cargar el archivo de funciones a utilizar
```{r}
rm(list = ls())
source("../../../functions/functions.R")
```
El paquete utilizado para las redes neuronales se llama **nnet**, si este paquete no se encuentra instalado debe hacer uso del comando:

```{r, eval=FALSE}
install.packages("nnet")
```
En este caso el paquete ya se encuentra instalado y nos limitaremos a cargarlo.
```{r}
library("nnet")
```

A continuación cargaremos el conjunto de entrenamiento, que es el conjunto de datos que vamos a utilizar para evaluar el desempeño de las redes neuronales. Es necesario aclarar que el conjunto de prueba no se utiliza en el proceso de entrenamiento del modelo, este es exclusivamente para evaluar el desempeño del modelo una vez que este haya sido examinado y sus características se hayan seleccionado de forma definitiva. Cómo en este punto se está en una fase de evaluación utilizaremos el conjunto de entrenamiento para entrenar y probar el desempeño.

```{r}
dataset.training = read.csv("../../../../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)
```

Una vez cargado el conjunto de datos, eliminaremos las etiquetas que no se utilizarán. Para este modelo se utilizarán 5 clases objetivos para poder acotar el tipo de anomalía dentro de una de las clases **DoS**, **normal**, **Probing**, **R2L**, **U2R**. De esta manera al especialista se le hará más sencillo determinar la falla de seguridad. Dicho esto, sólo utilizaremos la columna de etiquetas **Label_Normal_ClassAttack**.

```{r}
dataset = dataset.training
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```
Es necesario que todos los predictores del conjunto de datos sean de tipo **numérico** para poder entrenar a la red neuronal, por este motivo transformaremos todos los predictores a tipo **numérico**, y la columna objetivo la transformaremos a tipo **factor**. A la columna objetivo ser de tipo factor, el algoritmo realiza clasificación, en caso contrario realizaría regresión.

```{r}
for (i in 1 : (ncol(dataset) -1) )
  dataset[,i] = as.numeric(dataset[,i])

dataset[,ncol(dataset)] = as.factor(dataset[,ncol(dataset)])
```
Para reducir el tiempo de entrenamiento y mejorar la precisión del algoritmo, es una buena práctica escalar el conjunto de valores de los distintos predictores dentro de un mismo rango de valores. Por este motivo,  los valores serán escalados para que todos estén centrados en el origen y posean desviación estándard 1.

```{r}
dataset = ScaleSet(dataset)
```

## Entrenando el modelo
La estrategia adoptada para la prueba del modelo será la utilización de **validación cruzada de 10 conjuntos**. La función **CVSet** toma un conjunto de datos y establece 10 divisiones iguales del conjunto de datos y las devuelve en una lista de dataframes.

```{r}
cv.sets = CVSet(dataset, k = 10, seed = 22)
length(cv.sets)
```
Podemos ver que la longitud de la lista es de 10, debido a que en cada posición se encuentra un dataframe que corresponde a un subconjutno del conjunto de datos original. Todos los registros entre los diferentes dataframes son diferentes, debido a que el muestreo se hizo sin reemplazo. A continuación se inicializarán algunas variables.

```{r}
results = vector(mode = "numeric", length = 10)
list.results = list(0, 0, 0, 0)
names(list.results) = c("results", "best_model", "best_testing_set", "best_predictions")
best.accuracy = 0
```

El proceso de entrenamiento y de prueba mediante validación cruzada de 10 conjuntos es largo, debido a esto se almacenaŕan en una lista los **resultados** de la tasa de acierto de cada iteración, el **mejor modelo**, el **conjunto de datos de prueba** que originó la predicción y el **mejor conjunto de predicciones**. De esta manera la lista puede ser guardada y exportada como un objeto que posteriormente se puede cargar sin necesidad de esperar a que el proceso de entrenamiento y de prueba se realice de nuevo. A continuación se presenta el fragmento de código que realiza el proceso mencionado previamente.

```{r, eval=FALSE}
for (i in 1:10)
{
  #Extrayendo el conjunto de datos
  testingset = as.data.frame(cv.sets[[i]])
  trainingset = cv.sets
  trainingset[[i]] = NULL
  trainingset = do.call(rbind, trainingset)
  
  #Entrenamiento de la red neuronal
  model = nnet(Label ~ .,
                     data = trainingset,
                     size = 20,
                     maxit = 100)
      
  #Realizando las predicciones
  predictions = predict(model, testingset[, 1:(ncol(testingset)-1)], type = "class")
  
  
  #Calculando la tasa de aciertos
  accuracy = mean(testingset[, ncol(testingset)] == predictions)
  
  #Almacenando el resultado
  results[i] = accuracy
  
  #Almacenando el mejor resultado
  if(best.accuracy < accuracy)
  {
    list.results$best_model = model
    list.results$best_testing_set = testingset
    list.results$best_predictions = predictions
    best.accuracy = accuracy
  }
}
```
Una vez que se termina el proceso de entrenamiento, se almacenan en la lista los resultados parciales y se exporta el modelo.

```{r, eval=FALSE}
list.results$results = results
saveRDS(list.results, "normal_model/NN/Tests/list_results.rds")
```

## Evaluando el modelo
Empezaremos por limpiar el ambiente de trabajo, cargar las funciones y el paquete a utilizar.
```{r}
rm(list = ls())
library("nnet")
source("../../../functions/functions.R")
```

Ahora cargaremos la lista guardada en la sección anterior.

```{r}
list.results = readRDS("list_results.rds")
```
Ahora visualizaremos los diferentes resultados obtenidos y calcularemos la media de la tasa de aciertos.

```{r}
list.results$results
mean(list.results$results) * 100
```
La media de aciertos es de **99.52%**. Una tasa de aciertos bastante alta que demuestra que las redes neuronales pueden tener un buen desempeño en la tarea de detección de intrusos en redes de computadoras. A continuación crearemos una matriz de confusión para observar de manera gráfica el desempeño del modelo.

```{r}
confusion.matrix = table(Real = list.results$best_testing_set[,ncol(list.results$best_testing_set)],
                         Prediction = list.results$best_predictions)
confusion.matrix
```
Se puede ver un rendimiento extremadamente bueno, con muy pocos fallos a la hora de categorizar los distintos registros. Ahora calcularemos la tasa de aciertos para el mejor modelo.

```{r}
accuracy = mean(list.results$best_testing_set[,ncol(list.results$best_testing_set)] == 
                  list.results$best_predictions)

accuracy * 100
ErrorRate(accuracy) * 100
```
La mejor tasa de aciertos fue de **99.6%** y la tasa de fallo es su complemento y corresponde al **0.4%**. Ahora veamos la eficacia por etiqueta a la hora de clasificar. La salida corresponde a un vector donde cada posición correspode a **DoS**, **normal**, **Probing**, **R2L**, **U2R**. Esto debido a que las etiquetas se ordenaron alfabeticamente.

```{r}
AccuracyPerLabel(confusion.matrix, list.results$best_testing_set)
```
Se observa que la eficacia con las clases **DoS**, **normal**, **Probing** y **R2L** está por encima del 99.3%, mientras que para **U2R** la tasa de aciertos es de sólo 33.3%. Este comportamiento se debe a que para **U2R** la cantidad de ejemplos para el entrenamiento son menores y por eso el algoritmo no es capaz de generalizar de la mejor manera, en otras palabras, se espera que con mayor cantidad de información las predicciones para esta clase mejore.

Para poder calcular medidas binarias tales como **Sensitividad**, **Especificidad**, **Precisión** y la graficación de la curva ROC es necesario llevar la matriz de 5 clases a 2 clases, es decir, **Attack** vs **normal**.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(list.results$best_testing_set,
                                                             list.results$best_predictions)
attack.normal.confusion.matrix
```
De esta manera se puede observar que sólo existen 28 errores en la clasificación, de los cuales 12 pertenecen a falsos negativos y 16 a falsos positivos. Es importante resaltar que el modelo está realizado para que el objetivo sea la detección de ataques. Ahora que tenemos la matriz de confusión binaria podemos calcular las medidas mencionadas con anterioridad.

```{r}
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```
En las tres medidas se obtuvo un excelente desempeño, todas por encima del 99.5% en promedio. Esto habla bien del modelo y de la capacidad que tiene para clasificar en este escenario. Ahora graficaremos una **Curva ROC** que mide la tasa de acierto contra la tasa de fallos. Se eligió este enfoque para ver bajo cuáles tasas de probabilidad el modelo es más certero. Este gráfico mide la proporción de aciertos sobre la proporción de fallos. La función identidad corresponde a la línea de azar que indica que todo lo que pase sobre esa línea tiene rendimeinto igual al azar. Para crear la curva roc se necesita de un vector de probabilidades, el vector de probabilidades será extraído del proceso de predicción del modelo, posteriormente se crearán tres vectores **probabilidad**, **real**, **predicción**. Que serán utilizados para generación de la curva ROC.

```{r, fig.align="center"}
probabilities = predict(list.results$best_model,
                        list.results$best_testing_set[, 1:(ncol(list.results$best_testing_set)-1)])

roc.data = DataROC(list.results$best_testing_set, probabilities,
                   list.results$best_predictions)

generate_ROC(scores = roc.data$Prob, real = roc.data$Label,
             pred = roc.data$Prediction)
```
En la curva se aprecia cómo la mayoría de los acierto son producidos con una alta probabilidad. Conforme la probabilidad de la predicción disminuye el modelo comete pocos errores hasta que se vuelve a acertar bastante y al final las predicciones con baja probabilidad son categorizadas como fallos en la clasificación.

### Añadiendo el segundo nivel de clasificación de K-Medias
El modelo anterior presentó 12 **falsos negativos**, esto quiere decir que se clasificaron 12 registros cómo **normal** que en verdad pertenecían a la etiqueta **Attack**. Se tomarán todas aquellas predicciones realizadas por el modelo de red neuronal con la etiqueta **normal** y se utilizará **k-medias** para tratar de separar esos 12 falsos negativos de los registros normales. Se utilizará k-medias con 2 clusters debido a que se verificó en el **informe de kmedias** que la varianza acumulada llegaba a su mejor representación en el codo de jambu con 2 clusters en vez de con 5.

```{r}
kmeans.set = list.results$best_testing_set[list.results$best_predictions == "normal",]
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```
Podemos observar cómo despues de tomar las etiquetas clasificadas cómo normales, se extrayeron los 12 falsos negativos y las 4417 etiquetas restantes corresponden a las verdaderas etiquetas normales.

En el **informe de k-medias** se mencionó que utilizando 2 centroides se alcanzaban en varias iteraciones el mismo resultado, esto es debido a que el algoritmo convergió todas esas veces a una solución óptima. K-Medias es un algoritmo donde la inicialización de los centroides se realiza de manera aleatoria por defecto. Debido a esto, se pueden obtener diferentes resultados en distintas ejecuciones del algoritmo. Por lo tanto, se precalcularán los centroides ejecutando el algoritmo 100 veces y promediando la posición de los centroides finales. De esta manera es bastante probable que los centroides queden alineados en una posición cercana a la óptima y se pueda maximizar el acierto al correr el algoritmo.

```{r}
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)

#Training the absolute model
matrix.centers = matrix.centers/100
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)
```

Una vez que el modelo fue entrenado, entonces veamos sus predicciones.

```{r}
predictions = OrderKmeans(kmeans.model)

confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)

confusion.matrix.kmeans.model
```
Podemos ver que 0 de los 12 ataques fueron detectados, volvió a tener 12 falsos negativos e incrementó en una unidad la cantidad de falsos positivos. A simple vista no parece que aplicar K-medias sirviera de mucho, sin embargo, se espera que este rendimiento mejore conforme haya mayor cantidad de ataques. Ahora calcularemos las tasas de acierto y de error.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)*100
```
La tasa de aciertos es de 99.7% y la tasa de error de 0.3%, aparentemente tiene un buen resultado, también hay que considerar que las clases están desbalanceadas en este punto. Ahora veamos la eficacia por etiqueta. Recordemos que la salidad es un vector que corresponde a **Attack** y **normal**.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```
Se obtuvo 99.98% de acierto en la etiqueta normal y 0% en ataques, este resultado no es bueno, debido a que que el objetivo es detectar ataques y los 12 ataques fueron clasificados de mala manera.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```
La **especificidad** es bastante alta, quiere decir que el algoritmo tiene gran desempeño detectando los **verdaderos negativos**, por otra parte la **Sensitividad** y la **Precisión** son nulas, lo que quiere decir que el modelo no presentó un buen desempeño calculando los **verdaderos positivos** fue nula.

#### Estadísticas totales
A continuación se unificarán las estadísticas de ambos niveles para analizar el desempeño conjunto. Se empezará por mezclarar las dos matrices de confusión de los distintos modelos.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix, confusion.matrix.kmeans.model)
```

A partir de acá se pueden calcular todas las medidas utilizadas anteriormente.

```{r}
confusion.matrix.two.labels 
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
```
Se observa que la mayoría de las medidas se ven invariantes debido a que el proceso de aplicar k-medias fue nulo en este caso. Sin embargo, se espera que ante mayor cantidad de ataques no clasificados por parte del modelo de red neuronal, k-medias posea un buen desempeño.

#Conclusiones

1. El algoritmo de redes neuronales tiene un desempeño excelente, con bajas tasas de falsos positivos, falsos negativos y alta tasa de aciertos.

2. El segundo nivel de k-Medias no es del todo bueno, pero parece ser prometedor en escenarios donde el primer nivel de clasificación no sea tan eficaz para clasificar.

3. La combinación de ambos modelos es igual a sólo utilizar redes neuronales.