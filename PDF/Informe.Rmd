---
title: "Implementación y Análisis de Técnicas Híbridas de Aprendizaje Automático en la Detección de Intrusos en Redes de Computadoras"
author: "Deyban Andrés Pérez Abreu"
header-includes:
  - \usepackage[spanish]{babel}
output: 
  pdf_document: 
    number_sections: yes
    toc: yes
    toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Introducción
El presente documento recopila las actividades realizadas en la elaboración del Trabajo Especial de Grado de mi persona (autor del documento). Este tiene como tema el análisis e implementación de técnicas híbridas de aprendizaje automático en la detección de intrusos en redes de computadoras haciendo uso del conjunto de datos NSL-KDD.

Los objetivos que se buscan lograr con este trabajo son la implementación y análisis de modelos basados en la firma del ataque, como lo son las redes neuronales (NN - *Neural Network*) y máquinas de vectores de soporte (SVM - *Support Vector Machine*) en conjunto con técnicas basadas en anomalías como lo es K-Medias. La idea de esta mezcla de enfoques es la complementación de estos con la esperanza de mejorar el rendimiento desde un punto de vista de eficacia a la hora de detectar anomalías en una red de computadoras. Específicamente, con la utilización de técnicas basadas en la firma del ataque se busca detectar aquellos ataques conocidos que fueron provistos en el conjunto de entrenamiento y con las técnicas basadas en anomalías se busca capturar aquellas nuevas anomalías que no fueron provistas en la fase de entrenamiento a los modelos.

El conjunto de datos NSL-KDD consta de un conjunto de entrenamiento y un conjunto de prueba excluyentes; es decir, ningún registro está duplicado entre conjuntos. Adicionalmente, el conjunto de datos de prueba posee ataques que no son proporcionados en el conjunto de entrenamiento, así que la idea es evaluar la capacidad de generalización de los modelos creados simulando un ambiente real de prueba, donde nuevos ataques surgen constantemente.

La tareas a realizar se pueden dividir en cuatro grandes grupos que se mencionarán a continuación:

1. Esta fase corresponde al pre-procesamiento de los datos, esto aplica tanto al conjunto de entrenamiento como al conjunto de prueba. En este paso se busca crear una vista minable que facilite la manipulación de la información y estandarice los tipos de datos a ser utilizados a lo largo de la investigación.

2. Esta fase corresponde a la demostración de la eficacia de la propuesta planteada con anterioridad, es decir, la prueba de los modelos híbridos a la hora de realizar las tareas de detección. Esta fase se dividirá en dos conjuntos.
  + **Análisis sobre el conjunto de entrenamiento**: acá se realizarán las pruebas extrayendo un subconjunto de los datos para la prueba y el restante para el entrenamiento y se evaluará el rendimiento de cada uno de los modelos.
  + **Análisis sobre el conjunto de prueba**: acá se tomará el conjunto de entrenamiento en su totalidad para realizar las tareas de entrenamiento y se hará la prueba sobre el conjunto total de prueba provisto por el conjunto de datos NSL-KDD.

**NOTA**: En esta fase los modelos serán entrenados haciendo uso de parámetros por defecto.

3. Esta fase corresponde al proceso de selección de características y selección de parámetros, en esta fase se analizan los resultados obtenidos del proceso de reducción de características y ajuste de los parámetros para los modelos.

4. Esta fase recolecta los resultados obtenidos en la fase anterios (fase 3) correpondientes a la selección de características y parámetros para entrenar los algoritmos definitivos y ensamblarlos de forma definitiva para su posterios evaluación.

#Pre-Procesamiento de los datos
En esta sección se listan las actividades realizadas concernientes al proceso de pre-procesamiento de los datos. Esta tarea aplica para los conjuntos de datos de entrenamiento y de prueba, debido a que ambos conjuntos de datos deben poseer el mismo formato para poder realizar el proceso de aprendizaje automático.

Comenzaremos con la configuración del ambiente de trabajo, donde se eliminarán las variables del ambiente de trabajo. Y se cargará un archivo con funciones llamado *functions.R*, este archivo posee una leyenda donde se explica a cabalidad el funcionamiento de cada una de las funciones ilustradas en dicho documento.

```{r}
rm(list = ls())
source("../source/functions/functions.R")
```

A continuación se cargarán los conjuntos de prueba y de entrenamiento a ser utilizados.

```{r}
dataset.training = read.csv(file = "../dataset/KDDTrain+.txt", sep = ",", header = FALSE)
dataset.testing = read.csv(file = "../dataset/KDDTest+.txt", sep = ",", header = FALSE)
```

En la variable *dataset.training* se encuentra cargado el conjunto de entrenamiento y en la variable *dataset.testing* se tiene cargado el conjunto de prueba. Veamos las dimensiones de los conjuntos de datos.

```{r}
dim(dataset.training)
dim(dataset.testing)
```

El *conjunto de entrenamiento* tiene 125973 filas y 43 columnas. Por otra parte, el *conjunto de prueba* tiene 22544 filas y 43 columnas. Es importante mencionar que de las 43 columnas, la *columna 42* corresponde a la etiqueta del ataque y la *columna 43* corresponde a la cantidad de clasificadores que acertaron a la hora de clasificar dicho registro en el proceso de creación del conjunto de datos NSL-KDD. En el proceso previamente mencionado se utilizaron 21 clasificadores, por dicho motivo, el rango de valores en esta columna está comprendido entre [0,21]. A continuación veamos si los conjuntos de datos poseen valores faltantes, para ello haremos uso de la función *complete.cases*.

```{r}
sum(complete.cases(dataset.training)) == nrow(dataset.training)
sum(complete.cases(dataset.testing)) == nrow(dataset.testing)
```

Se observa que la cantidad de casos completos es igual a la cantidad de filas de ambos conjuntos de datos, por tal motivo no existen valores faltantes. Ahora veamos los tipos de ataques por conjuntos de datos. Empezaremos por con el conjunto de entrenamiento.

```{r}
attacks.training = unique(dataset.training$V42)
attacks.training = sort(as.character(attacks.training))
length(attacks.training)
```

Se observa que el conjunto de entrenamiento consta de 23 etiquetas, donde una corresponde a la etiqueta de tráfico normal, y las otras 22 corresponden a ataques. Ahora veamos el conjunto de prueba.

```{r}
attacks.testing = unique(dataset.testing$V42)
attacks.testing = sort(as.character(attacks.testing))
length(attacks.testing)
```

Se observan 38 etiquetas en el conjunto de prueba, donde una corresponde a la etiqueta de tráfico normal y las otras 37 corresponden a ataques. En este punto se puede observar como hay mayor cantidad de ataques en el conjunto de prueba que en el conjunto de entrenamiento, esto es debido a que el conjunto de prueba busca medir la habilidad del modelo de aprendizaje automático para generalizar ante ataques no vistos en el conjunto de entrenamiento con anterioridad. A continuación se observan cuales son los ataques presentes en el conjunto de entrenamiento que no están presentes en el conjunto de prueba y viceversa. Se empezará con examinar la cantidad total de ataques presentes entre ambos conjuntos.

```{r}
total.attacks = sort(unique(c(attacks.training, attacks.testing)))
length(total.attacks)
```

Entre ambos conjuntos se observan 40 etiquetas, donde una corresponde al tráfico normal y las otras 39 corresponden a etiquetas de ataques. De lo anterior se puede concluir que hay 17 tipos de ataques presentes en el conjunto de prueba que no están presentes en el conjunto de entrenamiento, y que hay dos tipos de ataques en el conjunto de entrenamiento que no están presentes en el conjunto de prueba. A continuación se listan aquellas etiquetas comunes entre ambos conjuntos de datos.

```{r}
total.attacks = sort(unique(c(attacks.training, attacks.testing)))
length(total.attacks)
total.attacks
```

Se observa que existen 21 etiquetas comunes entre ambos conjuntos de datos, donde una corresponde a la etiqueta de tráfico normal y las otras 20 corresponde a ataques. Todas las etiquetas fueron listadas. A continuación se listarán aquellos ataques que están presentes en el conjunto de prueba y no en el conjunto de entrenamiento.

```{r}
index.attacks = which(attacks.testing %in% attacks.training)
length(attacks.testing[-index.attacks])
attacks.testing[-index.attacks]
```

Son 17 los ataques presentes en el conjunto de prueba que no están presentes en el conjunto de entrenamiento, los mismos fueron listados. A continuación se listarán aquellos ataques presentes en el conjunto de entrenamiento que no lo están en el conjunto de prueba.

```{r}
index.attacks.training = which(attacks.training %in% attacks.testing)
length(attacks.training[-index.attacks.training])
attacks.training[-index.attacks.training]
```

Son sólo 2 los ataques en el conjunto de entrenamiento que no están presentes en el conjunto de prueba. Estos corresponden a *spy* y *warezclient*.

## Extracción de características
En este documento se clasifican las anomalías en cuatro grupos DoS, Probing, R2L y U2R, es decir, habrán cinco etiquetas, donde cuatro corresponden a los tipos de ataques mencionados previamente y la quinta etiqueta corresponde a la etiqueta normal.

Para facilitar el trabajo se debe asociar cada uno de los ataques a cada una de las clases mencionadas con anterioridad. Para esto se hará uso de la función ClassLabelAttack que recibe como parámetro un *dataframe* y retorna una columna con la clase de cada tipo de ataque para cada registro. Estos nombres colocados acordes a la investigación hecha por Bhavsar.

```{r}
dataset.training$V44 = ClassLabelAttack(dataset.training)
dataset.testing$V44 = ClassLabelAttack(dataset.testing)
```

De esta manera, tanto el conjunto de entrenamiento como el conjunto de prueba tienen una nueva columna en la que cada registro tiene asociada la respectiva clase a la que pertenece. Adicionalmente se agregó una nueva columna que corresponde a una nueva etiqueta que identifica a cada registro como ataque o normal. De esta manera se tiene una clase general para la asociación de los registros.

```{r}
dataset.training$V45 = NormalAttackLabel(dataset.training) 
dataset.testing$V45 = NormalAttackLabel(dataset.testing)
```

Ahora se dividirá el conjunto de datos en *dataframes* individuales para cada clase: DoS, normal, R2L, U2R.

```{r}
training.split = split(dataset.training, dataset.training$V44)
testing.split = split(dataset.testing, dataset.testing$V44)
summary(training.split)
summary(testing.split)
```

Las variables *training.split* y *testing.split* contienen una lista de sub-conjuntos por etiqueta de las clases de los ataques en ambos conjuntos de datos. A continuación se lista el número de cada clase en el conjunto de entrenamiento.

```{r}
nrow(training.split$DoS)
nrow(training.split$normal)
nrow(training.split$Probing)
nrow(training.split$R2L)
nrow(training.split$U2R)
```

Se observa que la clase normal es la que más registros posee en el conjunto de datos de entrenamiento, seguido por la clase DoS. Lo anterior nos da una idea de cuáles son las clases de ataques más comunes y menos comunes. A continuación se presenta un gráfico que ilustra lo anterior y permite visualizar mejor la distribución de las clases.

```{r, fig.align="center"}
barplot(table(dataset.training$V44), main = "Frecuencia de las Clases en el Conjunto
        de Entrenamiento")
```

A continuación se repiten los pasos anteriores para el conjunto de prueba.

```{r}
nrow(testing.split$DoS)
nrow(testing.split$normal)
nrow(testing.split$Probing)
nrow(testing.split$R2L)
nrow(testing.split$U2R)
```

En esta oportunidad la clase normal sigue siendo la clase con mayor cantidad de registros. En contraste con el conjunto de entrenamiento, se observa que en esta ocasión las clases Probing y R2L están más equilibradas; adicionalmente, la clase U2R posee una cantidad mucho mayor de registros que la presentada en el conjunto de entrenamiento. A continuación se presenta un gráfico con las  distribuciones de las clases en el conjunto de prueba.

```{r, fig.align="center"}
barplot(table(dataset.testing$V44), main = "Frecuencia de las Clases en el Conjunto
        de Prueba")
```

##Renombramiento de las columnas
Se hará uso de la función *ColumnNames* que asigna a los conjuntos de datos los nombres respectivos, estos nombres colocados acordes a la investigación hecha por Bhavsar.


```{r}
dataset.training = ColumnNames(dataset.training)
dataset.testing = ColumnNames(dataset.testing)
```

##Eliminación de características no importantes
En esta sección se examinarán posibles características inútiles; esto es, aquellas características que solo tienen un nivel de valores, por ejemplo, una característica de tipo numérico donde en todos los registros el valor es cero, es decir, el rango viene dado por [0]. Para dicho propósito se utilizará la función *CheckFeaturesLevels* que toma como entrada un *dataframe* y retorna la posición (si existe) de la característica que no aporta información.

```{r}
index.dummy.variables.training = CheckFeaturesLevels(dataset.training)
index.dummy.variables.testing = CheckFeaturesLevels(dataset.testing)
names(dataset.training)[index.dummy.variables.training]
names(dataset.testing)[index.dummy.variables.testing]
```

Se observa que en ambos conjuntos de datos la columna *Num_outbound_cmds* es inútil, en consecuencia, la misma será eliminada del conjunto de datos.

```{r}
dataset.training[,index.dummy.variables.training] = NULL
dataset.testing[, index.dummy.variables.testing] = NULL
```

##Tranformación de los datos
Las columnas *Protocol_type*, *Service* y *Flag* tienen tipos de datos categóricos, las mismas serán transformadas a tipo numérico. La transformación tiene su justificación en el hecho de que los algoritmos a utilizar que son NN, SVM y K-Medias funcionan con variables predictoras (características) numéricas. Dicho esto es obligatorio transformar las columnas de tipo categórico a tipo numérico.

1. **Protocol_type**: esta característica posee 3 niveles, los cuales son listados alfabeticamente a continuación.

```{r}
sort(unique(dataset.training$Protocol_type))
sort(unique(dataset.testing$Protocol_type))
```

Los mismos se tranformarán en los valores 1,2,3 respectivamente. La función *ProtocolTransformation* es la encargada de realizar dicho trabajo.

```{r}
dataset.training = ProtocolTransformation(dataset.training)
dataset.testing = ProtocolTransformation(dataset.testing)
```

2. **Service**: esta característica posee una mayor cantidad de niveles con respecto a *Protocol_type*, los mismos son listados a continuación.

```{r}
sort(unique(dataset.training$Service))
sort(unique(dataset.testing$Service))
```

Se observa que en el conjunto de entrenamiento hay un total de 70 niveles, contra 64 niveles presentes en el conjunto de prueba. Observemos la cantidad total de servicios uniendo ambos conjuntos.

```{r}
sort(unique(dataset.training$Service))
```

Se observa que el total de servicios es de 70, es decir, el conjunto de servicios en el conjunto de entrenamiento corresponde al universo de todos los servicios en los conjuntos de datos.

Los niveles serán enumerados en en rango [1,70] en orden alfabético, tal como se muestra a continuación.

```{r}
sort(unique(c(as.character(unique(dataset.testing$Service)),
              as.character(unique(dataset.training$Service)))))
```

Se utilizará la función *ServiceTransformation* para enumerar cada uno de los servicios listados previamente.

```{r}
dataset.training = ServiceTransformation(dataset.training)
dataset.testing = ServiceTransformation(dataset.testing)
```

3. **Flag**: es la característica categórica restante. Observemos los niveles de esta características.

```{r}
sort(unique(dataset.training$Flag))
sort(unique(dataset.testing$Flag))
length(sort(unique(c(as.character(unique(dataset.testing$Flag)),
                  as.character(unique(dataset.training$Flag))))))
```

Se observa que hay 11 niveles en ambos conjuntos y que la unión de los niveles de ambos conjuntos de datos arroja el mismo resultado. Dicho esto, las etiquetas serán enumeradas por orden alfabético, tal como se muestra a continuación.

```{r}
sort(unique(c(as.character(unique(dataset.testing$Flag)),
              as.character(unique(dataset.training$Flag)))))
```

Se utilizará la función *FlagTransformation* para dicho propósito.

```{r}
dataset.training = FlagTransformation(dataset.training)
dataset.testing = FlagTransformation(dataset.testing)
```

##Guardando la vista minable
En este punto la vista minable ya fue creada, las columnas poseen un formato adecuado para los algoritmos que serán utilizados y se agregaron nuevas columnas que facilitarán tareas futuras en la investigación. Debido a que no hay más tareas por hacer, se procede a guardar los conjuntos de datos para cargar los datos preprocesados y no tener que repetir dicho procedimiento luego.

```{r, eval=FALSE}
write.csv(dataset.training,
          file = "../dataset/NSLKDD_Training_New.csv", row.names = FALSE)
write.csv(dataset.testing,
          file = "../dataset/NSLKDD_Testing_New.csv", row.names = FALSE)
```

# Implementación de modelos híbridos sobre el conjunto de datos original usando parámetros por defecto
La propuesta del Trabajo Especial de Grado consta del entrenamiento de dos modelos híbridos de aprendizaje automático. El primer modelo (I) consta de una red neuronal en el primer nivel y K-Medias en el segundo nivel. Por otra parte, el segundo modelo (II) consta de una máquina de vectores de soporte en el primer nivel y K-Medias en el segundo nivel.

En esta sección los modelos serán entrenados con los parámetros por defecto. Porteriormente se hará selección de características y parámetros, y se analizará el impacto con respecto a los modelos creados en esta sección.

Esta sección será divida en dos apartados, uno concerniente al entrenamiento y evaluación de los modelos utilizando el conjunto de entrenamiento. Posteriormente, se hará el entrenamiento haciendo uso total del conjunto de entrenamiento y se hará evaluación de los modelos haciendo uso del conjunto de prueba. En ambos casos se hará uso de la técnica de validación cruzada de 10 conjuntos para evaluar los modelos.

Con la estrategia descrita previamente se podrán observar tres aspectos relevantes:

1. La eficacia de los modelos contra ataques conocidos.
2. La eficacia de los modelos contra ataques no conocidos.
3. Diferencias de rendimiento entre ambos modelos.

##Análisis sobre el conjunto de entrenamiento
En esta sección se listan las actividades concernientes al entrenamiento y evaluación de los modelos híbridos haciendo uso exclusivo del conjunto de entrenamiento y de la técnica de validación cruzada de 10 conjuntos para la evaluación de modelos. 

Inicialmente iniciaremos con una evaluación del rendimiento de K-Medias. Se elegirán los centroides y se evaluará su desempeño en la tarea de detección de intrusos.

###K-Medias
Se empezará por establecer el ambiente de trabajo eliminando las variables parciales, cargando el archivo de funciones y la vista minable del conjunto de entrenamiento pre-procesado previamente.

```{r}
rm(list = ls())
source("../source/functions/functions.R")
dataset = read.csv("../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)
```

Para esta sección solo se necesitan las etiquetas *Label_Normal_ClassAttack* y *Label_Normal_or_Attack*, por lo tanto las otras etiquetas serán eliminadas.

```{r}
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
```

A continuación se le asignará el tipo numérico a todas las variables predictoras, y el tipo *factor* a las variables objetivo.

```{r}
Labels = dataset[, (ncol(dataset)-1):ncol(dataset)]
dataset = as.data.frame(apply(dataset[, c(-41, -42)], 2, as.numeric))
dataset = cbind(dataset, Label = Labels[,1])
dataset = ScaleSet(dataset)
```

Se crearán dos nuevos conjuntos de datos, *dataset.two* que tendrá como etiqueta la columna *Label_Normal_or_Attack*, columna que solo tiene dos niveles categóricos *Attack* o *normal*. Por otra parte, se creará un segundo conjunto de datos llamado *dataset.five* el cual contendrá como etiqueta la columna *Label_Normal_ClassAttack*, columna que tiene cinco niveles categóricos DoS, *normal*, *Probing*, *R2L* y *U2R*.

```{r}
dataset.five = cbind(dataset[, -ncol(dataset)], Label = Labels[,1])
dataset.two = cbind(dataset[, -ncol(dataset)], Label = Labels[,2])
remove(list = c("Labels"))
```

#### Codo de Jambu
Hasta este punto ya se tienen los conjuntos de datos listos para ser utilizados. El algoritmo K-Medias amerita que se le pasen como argumentos el número de centroides o la posición inicial de los centroides. Estos corresponden al número de conjuntos que se esperan identificar en el conjunto de datos. En el escenario que tenemos actualmente esta tarea es sencilla debido a que el conjunto de datos tiene cada uno de los registros etiquetados, sin embargo, este paso no siempre es sencillo. Adicionalmente es importante recordar que el algoritmo de K-Medias es un algoritmo de enfoque no-supervisado y no hace uso de estas etiquetas para separar los conjuntos.

Si bien es cierto que tenemos etiquetas que nos dicen de antemano cuales son los niveles de los conjuntos, existe un dilema con respecto al rendimiento del algoritmo con dos o cinco clases objetivo. El problema de la selección del número de clusters siempre ha existido y es por eso que existe un método llamado codo de Jambu que es utilizado para capturar la varianza acumulada con respecto al número de grupos usados. Al graficar la cantidad de varianza acumulada por el número de grupos, se verá que llega un punto en el que la gráfica tiene un comportamiento que emula la articulación de un codo y es en ese punto, donde se empieza a formar la articulación que se indica que el uso de mayor cantidad de grupos no aporta mayor información al algoritmo.

Existen cuatro tipos de algoritmos para calcular las distancias de K-Medias, estas son *Hartigan-Wong*, *Lloyd*, *Forgy* y *Macqueen*. A continuación se aplicará cada una de estas técnicas variando la cantidad de centroides en el rango [1,30]. Luego, se graficarán los resultados y se analizarán los mismos.

```{r, eval=FALSE}
IIC.Hartigan = vector(mode = "numeric", length = 30)
IIC.Lloyd = vector(mode = "numeric", length = 30)
IIC.Forgy = vector(mode = "numeric", length = 30)
IIC.MacQueen = vector(mode = "numeric", length = 30)

for (k in 1:30)
{
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "Hartigan-Wong")
  IIC.Hartigan[k] = groups$tot.withinss
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "Lloyd")
  IIC.Lloyd[k] = groups$tot.withinss
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "Forgy")
  IIC.Forgy[k] = groups$tot.withinss
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "MacQueen")
  IIC.MacQueen[k] = groups$tot.withinss
}

#Creando una lista para almacenar los resultados
jambu.results = list(IIC.Hartigan = IIC.Hartigan, IIC.Lloyd = IIC.Lloyd,
     IIC.Forgy = IIC.Forgy, IIC.MacQueen = IIC.MacQueen)
#Guardando los resultados del codo de Jambu
saveRDS(object = jambu.results, file = "source/default_parameters/original_set/KMEANS/jambu_results.rds")
```

El proceso anterior dura algunos minutos y por ello es que los resultados fueron almacenados en un objeto para su posterior carga y evaluación, sin la necesidad de que el proceso de codo de jambu sea realizado cada vez que se desee analizar el proceso.

Para poder observar los resultados se creará una gráfica que permita ver la varianza contra la cantidad de grupos o de centroides.

```{r, fig.align="center"}
jambu.results = readRDS("../source/default_parameters/original_set/KMEANS/jambu_results.rds")
plot(jambu.results$IIC.Hartigan, col = "blue", type = "b", pch = 19, main = "Codo de Jambu",
     xlab = "Número de Centroides", ylab = "Varianza", log = "y")
points(jambu.results$IIC.Lloyd, col = "red", type = "b", pch = 19)
points(jambu.results$IIC.Forgy, col = "green", type = "b", pch = 19)
points(jambu.results$IIC.MacQueen, col = "magenta", type = "b", pch= 19)
legend("topright", legend = c("Hartigan", "Lloyd", "Forgy", "MacQueen"),
       col = c("blue","red", "green", "magenta"), pch = 19)
```

El gráfico no presenta la verdadera esencia de lo que es el Codo de Jambu, debido a que la articulación que hace analogía al codo entre el brazo y el antebrazo no está presente. Como aspecto resaltante se puede observar que cualquiera de los algortimos parece tener el mismo desempeño, por este motivo, el uso de uno u otro parece ser indiferente. Para mayor información consultar el siguiente enlace: [Codo de Jambu](http://rstudio-pubs-static.s3.amazonaws.com/13318_150e1d31dd954caf89af3108df9d8755.html).

Por lo dicho en el párrafo anterior, aún queda inconcluso cuál es número óptimo de centroides a establecer en el algoritmo de K-Medias para maximizar el desempeño a la hora de clasificar intrusos. Se sabe a priori que hay dos opciones: cinco o dos clases. A continuación se seleccionará la mejor medida de distancia para K-Medias para dos y cinco centroides. Se ilustrará como el desempeño de las diferentes medidas de distancia es similar.

```{r, eval=FALSE}
#Seleccionando la mejor medida de distancia
measure.two = lapply(MeasuareKMeans(dataset, 2), max)
measure.five = lapply(MeasuareKMeans(dataset, 5), max)
#Creando una lista para almacenar los resultados
measures.results = list(measure.two = measure.two, measure.five = measure.five)
#Salvando las medidas de rendimiento obtenidas en un objeto
saveRDS(object = measures.results, file = "../source/default_parameters/original_set/KMEANS/measures_results.rds")
```

El paso anterior fue realizado haciendo uso de la función *MeasureKMeans*, que permite promediar la inercia inter-grupos. Seleccionando aquella medida que maximize esta medida de rendimiento. Esta función es algo costosa en tiempo y por eso los resultados fueron exportados a un objeto para el posterior análisis. A continuación se cargan los resultados y se examinan los mismos.

```{r}
measures.results = readRDS("../source/default_parameters/original_set/KMEANS/measures_results.rds")
measures.results$measure.two
measures.results$measure.two[1]
measures.results$measure.five
measures.results$measure.five[1]
```

El resultado para *measures.two* y *measures.five* corresponden a vectores ordenados de mayor a menor. Se puede observar como en ambos casos la medida *Hartigan* es el que mejores resultados obtiene (algoritmo utilizado por defecto). Sin embargo, se puede observar que de hecho los otros algoritmos se comportan de forma muy similar.

En las proximas secciones se probará el rendimiento del algoritmo utilizando cinco y dos grupos. De esta manera, se podrá determinar definitivamente cuál es el número de grupos óptimo que identifica el algoritmo de K-Medias.

####K-Medias (cinco grupos)
Empezaremos con cinco grupos, debido a que aparentemente es el que tiene peor rendimiento. La metodología es la siguiente, se aplicará 10 veces el algoritmo y se promediará la tasa de acierto para evaluar el desempeño.

```{r}
results.five = vector(mode = "numeric", length = 10)
best.accuracy.five = 0

for (i in 1:length(results.five))
{
  set.seed(i)
  model.kmeans.five = kmeans(dataset.five[,1:(ncol(dataset.five)-1)],
                             5, iter.max = 100)
  
  prediction.five = OrderKmeans(model.kmeans.five)
  accuracy.five = mean(prediction.five == dataset.five$Label)
  
  results.five[i] = accuracy.five
  
  if(best.accuracy.five < accuracy.five)
  {
    best.prediction.five = prediction.five
    best.accuracy.five = accuracy.five
  }
}
```

La variable *results.five* contiene los resultados de la tasa de aciertos de cada iteración y las variables *best.prediction.five* y *best.accuracy.five* contienen las mejores predicciones y la mejor tasa de aciertos respectivamente. Veamos los resultados.

```{r}
results.five * 100
mean(results.five) * 100
```

Se observa que el promedio de acierto fue de 71.87%. Este rendimiento no parece estar mal; sin embargo, es necesario esperar a la comparación con el modelo de dos grupos para poder tener comparar ambos modelos y emitir una opinión al respecto. Mientras tanto crearemos una matriz de confusión para ilustrar el desempeño del modelo de manera gráfica.

```{r}
confusion.matrix.five = table(Real = dataset.five$Label, Prediction = best.prediction.five)
confusion.matrix.five
```

La matriz de confusión se ve bastante desordenada, y no acertó en la predicción de ningún registro para las clases R2L y U2R. Veamos la mejor tasa se acierto y la peor tasa de aciertos.

```{r}
best.accuracy.five*100
```

La mejor tasa de aciertos fue de 78.61%, no parece ser un mal rendimiento, pero debemos esperar a la comparación con el otro modelo (dos grupos). Como aspecto importante a resaltar, la diferencia entre los resultados se debe a que la inicialización de los centroides en el algoritmo de K-Medias se hace de forma aleatoria y dependiendo de la posición iniciales de los centroides, el algoritmo puede converger a diferentes mínimos locales. Por tal motivo, se colocaron semillas, de tal manera que las pruebas puedan ser recreables. A continuación calcularemos la eficacia por etiqueta. La salida es un vector numérico que representa a las clases ordenadas en orden alfabético de la siguiente forma: DoS, normal, Probing, R2L y U2R.

```{r}
AccuracyPerLabel(confusion.matrix.five, dataset.five)
```

Se aprecia el hecho de que el algoritmo solo clasifica bien las clases DoS y normal. Estas dos clases corresponden a la mayoría de registros del conjunto de datos y por eso es que el promedio de aciertos es elevado; sin embargo, el rendimiento con las otras tres etiquetas correspondientes a Probing, R2L y U2R es pobre.

Lo siguiente será transformar la matriz de confusión de cinco clases a dos clases. Esto con la finalidad de poder calcular medidas de rendimiento binarias como lo son sensitividad, especificidad y precisión.

```{r}
attack.normal.confusion.matrix.five = AttackNormalConfusionMatrix(dataset.five,
                                                             best.prediction.five)
attack.normal.confusion.matrix.five
```

Se observa como hay una gran cantidad de falsos negativos y falsos positivos, a pesar de que la mayoría de los registros son clasificados de buena manera. Ahora veamos la eficacia por etiqueta. La salida corresponde a un vector numérico donde la primera posición es Attack y la segunda normal.

```{r}
AccuracyPerLabel(attack.normal.confusion.matrix.five, dataset.two)
```

La eficacia a la hora de detectar tráfico normal es bastante elevada, de 94.78%. Para la detección de los ataques es menor, esta corresponde a 80.34%, que es un número elevado; sin embargo, hay que recordar que este número está sesgado desde el punto de vista que solo se clasificaron ataques de tipo DoS. A continuación se calcularán las medidas de rendimiento binarias.

```{r}
Sensitivity(attack.normal.confusion.matrix.five) * 100
Especificity(attack.normal.confusion.matrix.five) * 100
Precision(attack.normal.confusion.matrix.five) * 100
```

Se observa que el modelo es bueno detectando tráfico normal; sin embargo, a la hora de detectar ataques el rendimiento se ve mermado.

####K-Medias (dos grupos)
Ahora se implementarán los pasos realizados con cinco grupos pero ahora con dos grupos. Es decir, se correrá el algoritmo de K-Medias 10 veces con dos grupos.

```{r}
results.two = vector(mode = "numeric", length = 10)
best.accuracy.two = 0

for (i in 1:length(results.two))
{
  set.seed(i)
  model.kmeans.two = kmeans(dataset.two[,1:(ncol(dataset.two)-1)],
                             2, iter.max = 100)
  
  prediction.two = OrderKmeans(model.kmeans.two)
  accuracy.two = mean(prediction.two == dataset.two$Label)
  
  results.two[i] = accuracy.two
  
  if(best.accuracy.two < accuracy.two)
  {
    best.prediction.two = prediction.two
    best.accuracy.two = accuracy.two
  }
}
```

La variable *results.two* contiene la tasa de aciertos en cada iteración del algoritmo.

```{r}
results.two * 100
```

Se observa que la tasa de aciertos es mayor que con cinco grupos. Adicionalmente, hubo iteraciones en la que los resultados se repitieron. Esto es debido a que la inicialización de los centroides al inicio del algoritmo hizo que en esas ocasiones se alcazara el mismo mínimo local. Este comportamiento da indicios de que la solución al conjunto de datos se representa con dos grupos y no con cinco. A continuación calculemos el promedio de acierto.

```{r}
mean(results.two) * 100
```

La media de acierto es de 76.55%, este resultado es mayor al promedio con cinco grupos. Se creará una matriz de confusión para visualizar gráficamente el desempeño del algoritmo en la tarea de clasificación.

```{r}
confusion.matrix.two = table(Real = dataset.two$Label, Prediction = best.prediction.two)
confusion.matrix.two
```

Se aprecia que contiene un alto número de falsos negativos, en realidad es una cantidad similar a la matriz de confusión con cinco clusters. Por otra parte, la cantidad de falsos positivos se vio reducida notablemente. Ahora imprimiremos la tasa de aciertos y la tasa de error del mejor modelo obtenido.

```{r}
best.accuracy.two * 100
ErrorRate(best.accuracy.two) * 100
```

Se obtuvo una tasa de aciertos de 90.51% una tasa bastante alta, mucho mayor que el modelo con cinco grupos. Por consecuente, la tasa de errores será también menor. Ahora veamos la tasa de aciertos por etiquetas. Es importante recordar que la salida corresponde a un vector numérico donde la primera posición corresponde a la etiqueta Attack y la segunda a la etiqueta normal.

```{r}
AccuracyPerLabel(confusion.matrix.two, dataset.two)
```

Se obtuvo una eficacia similar en la detección de ataques que en la evaluación con cinco clusters. La verdadera mejora vino en la eficacia a la hora de clasificar el tráfico normal, donde se obtuvo un 98.99% de acierto. En esta oportunidad, la matriz de confusión ya es binaria, por consecuente se pueden calcular las medidas de rendimiento correspondientes.

```{r}
Sensitivity(confusion.matrix.two) * 100
Especificity(confusion.matrix.two) * 100
Precision(confusion.matrix.two) * 100
```

La sensitividad nos dice que el modelo fue muy bueno clasificando el tráfico normal, por otra parte clasificando los ataques es menos efectivo. La medida de sensitividad con respecto al modelo con cinco grupos se vio incrementada en 4%, por otra parte, la especificidad y la precisión es bastante parecida en ambos modelos.

#### Conclusión
En general ambos modelos tienen altos valores de eficacia, sin embargo, el modelo con dos clusters obtuvo mejores resultados con respecto a la eficacia y sensitividad de manera notoria y algunas mejoras simples en las medidas de especificidad y precisión. Adicionalmente, se observó que la cantidad de falsos positivos fue reducida en el modelo con dos clusters. Finalmente, el algoritmo de Codo de Jambu no fue muy fructífero en este caso, este hecho puede recaer en que quizás haya variables predictoras que aporten ruido. Se podría esperar que con los conjuntos de datos generados al seleccionar las características este comportamiento mejore demostrando la esencia del Codo de Jambu. Por otra parte, se puede observar como dos centroides o grupos es el número óptimo para la clasificación haciendo uso del algoritmo k-Medias.

### Redes Neuronales
En esta sección se describirán las actividades realizadas para el entrenamiento y evaluación de redes neuronales en el ámbito de detección de intrusos en redes de computadoras. Esta sección de subdivide en dos grandes partes Entrenamiento del Modelo y Evaluación del Modelo. Esto debido a que los pasos y observaciones serán realizadas de manera individual.

#### Entrenamiento del modelo
Para el entrenamiento de la red neuronal se propone una arquitectura con 40 neuronas en la capa de entrada, una capa intermedia con 20 neuronas, y 5 neuronas para la capa de salida. La razón para la selección de la arquitectura descrita previamente corresponde a que inicialmente se tienen 40 variables predictoras que corresponden a la capa de entrada. Por otra parte, se tienen cinco clases objetivo que corresponden a las cinco neuronas de la capa de salida. El número de capas intermedias y el número de neuronas por capas que debe poseer una red neuronal no está normado en ningún lugar, sólo existen recomendaciones hechas por expertos. Andrew Ng profesor de la Universidad de Stanford, menciona en uno de sus cursos de aprendizaje automático en Coursera que un modelo con una capa intermedia es suficiente para resolver una gran cantidad de problemas, adicionalmente comenta que en caso de querer utilizar una segunda capa intermedia, es recomendable que ambas capas posean igual cantidad de neuronas.

En este modelo se utilizaron 20 neuronas debido a que al haber una gran cantidad de neuronas de entrada, entonces la mitad de las neuronas de entrada parece suficiente. El paquete utilizado para la implementación de redes neuronales se llama *nnet*. Se probó otro paquete llamado *neuralnet*, a diferencia de *nnet*, *neuralnet* permite crear modelos con múltiples capas intermedias, pero es mucho más lento y para las tareas de clasificación hace el proceso más engorroso. Por otra parte, *nnet* a pesar de que solo permite hacer uso de una capa intermedia, es mucho más rápido para el entrenamiento y el proceso de preparación de los datos para ser pasados como parámetros es más directo.

Se empezará por establecer el ambiente de trabajo eliminando variables parciales, cargando el archivo de funciones y la vista minable del conjunto de entrenamiento.

```{r}
rm(list = ls())
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)
source("../source/functions/functions.R")
```

Es importante mencionar que en esta sección se hará el entrenamiento y la evaluación del modelo haciendo uso únicamente del conjunto de entrenamiento haciendo uso de la técnica de validación de modelos llamada validación cruzada de 10 conjuntos.

Como se mencionó previamente el paquete utilizado es *nnet*, a continuación se procederá a instalarlo  y a cargarlo en el ámbiente de trabajo.

```{r, eval=FALSE}
install.packages("nnet")
library("nnet")
```

Una vez que tenemos nuestro ambiente de trabajo preparado se eliminarán aquellas etiquetas del conjunto de datos que no van a ser utilizadas a lo largo del proceso de entrenamiento del modelo. Este nivel posee cinco clases objetivos que son DoS, normal, Probing, R2L y U2R, esto con la finalidad de que la salida para el especialista sea más entendible y pueda identificar la falla se seguridad acontándola dentro de estas cuatro clases de ataques. Dicho esto eliminaremos el resto de las etiquetas.

```{r}
dataset = dataset.training
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Es obligatorio para el uso de las redes neuronales que todas las variables predictoras sean de tipo numérico. Por tal motivo, las columnas serán transformadas a tipo numérico. Adicionalmente, como haremos tareas de clasificación, se establecerá la columna objetivo como tipo *factor*.

```{r}
for (i in 1 : (ncol(dataset) -1) )
  dataset[,i] = as.numeric(dataset[,i])

dataset[,ncol(dataset)] = as.factor(dataset[,ncol(dataset)])
```

Para reducir el tiempo de entrenamiento y mejorar la precisión de las predicciones es buena práctica escalar el conjunto de datos a valores que posean media cero y desviación estándar uno.

```{r}
dataset = ScaleSet(dataset)
```

La estrategia adoptada para la evaluación del modelo será la utilización de validación cruzada de 10 conjuntos. La función *CVSet* toma un conjunto de datos y establece 10 divisiones de igual longitud del conjunto de datos y las devuelve en una lista de *dataframes*.

```{r}
cv.sets = CVSet(dataset, k = 10, seed = 22)
length(cv.sets)
```

Se observa que la longitud de la lista es de 10 posiciones, esto debido a que en cada posición se posee un *dataframe* que corresponde a un subconjunto del conjunto de datos original. Todos los registros entre los diferentes *dataframes* son diferentes debido a que el muestreo se hizo sin reemplazo. Para seguir con las tareas se procederá a inicializar algunas variables.

```{r, eval=FALSE}
results = vector(mode = "numeric", length = 10)
list.results = list(0, 0, 0, 0)
names(list.results) = c("results", "best_model", "best_testing_set", "best_predictions")
best.accuracy = 0
```

El proceso de entrenamiento y de validación del modelo es bastante largo y por esto se almacenarán en una lista los resultados correspondiendes a la eficacia de cada modelo en cada iteración, el mejor modelo, el conjunto de datos de prueba que originó la predicción, y el mejor conjunto de predicciones. De esta manera la lista puede ser guardada como un objeto y ser exportada a un archivo que posteriormente puede cargarse, y no es necesario esperar a la realización de este paso cada vez que se deseen analizar los resultados. El siguiente fragmento de código es el encargado de realizar las tareas mencionadas con anterioridad.

```{r, eval=FALSE}
for (i in 1:10)
{
  #Extrayendo el conjunto de datos
  testingset = as.data.frame(cv.sets[[i]])
  trainingset = cv.sets
  trainingset[[i]] = NULL
  trainingset = do.call(rbind, trainingset)
  
  #Entrenamiento de la red neuronal
  model = nnet(Label ~ .,
                     data = trainingset,
                     size = 20,
                     maxit = 100)
      
  #Realizando las predicciones
  predictions = predict(model, testingset[, 1:(ncol(testingset)-1)], type = "class")
  
  #Calculando la tasa de aciertos
  accuracy = mean(testingset[, ncol(testingset)] == predictions)
  
  #Almacenando el resultado
  results[i] = accuracy
  
  #Almacenando el mejor resultado
  if(best.accuracy < accuracy)
  {
    list.results$best_model = model
    list.results$best_testing_set = testingset
    list.results$best_predictions = predictions
    best.accuracy = accuracy
  }
}
```

Una vez que se culmina el proceso de entrenamiento, se almacenan en la lista los resultados parciales de cada iteración y se exporta el modelo.

```{r, eval=FALSE}
list.results$results = results
saveRDS(list.results, "../source/default_parameters/original_set/NN/Tests/list_results.rds")
```

####Evaluación del modelo
En esta sección se hará la evaluación de los resultados obtenidos en la sección anterior, adicionalmente se tomará el mejor modelo y las mejores predicciones obtenidas para agregarle el segundo nivel de clasificación correspondiente al algoritmo *K-Medias*.

Se empezará por establecer el ambiente de trabajo eliminando variables parciales, cargando el paquete *nnet*, cargando el archivo de funciones y la lista con información exportada previamente.

```{r}
rm(list = ls())
library("nnet")
source("../source/functions/functions.R")
list.results = readRDS("../source/default_parameters/original_set/NN/Tests/list_results.rds")
```

A continuación se visualizará la eficacia obtenida del proceso de validación cruzada y se calculará la media de los resultados obtenidos.

```{r}
list.results$results
mean(list.results$results) * 100
```

La media de aciertos es de 99.52%. Esta tasa de aciertos es bastante alta, lo que demuestra que las redes neuronales pueden tener un buen desempeño en la tarea de detección de intrusos en redes de computadoras. A continuación se creará una matriz de confusión del mejor modelo obtenido en el proceso para visualizar gráficamente el desempeño del algoritmo.

```{r}
confusion.matrix = table(Real =
                           list.results$best_testing_set[,ncol(list.results$best_testing_set)],
                         Prediction = list.results$best_predictions)
confusion.matrix
```

Se observa una matriz de confusión bastante ordenada con pocos registros fuera de la diagonal; es decir, con pocos fallos de clasificación. Ahora se calculará la tasa de acierto y de error.

```{r}
accuracy = mean(list.results$best_testing_set[,ncol(list.results$best_testing_set)] == 
                  list.results$best_predictions)

accuracy * 100
ErrorRate(accuracy) * 100
```

La mejor tasa de aciertos fue de 99.6%. Una muy buena tasa de aciertos proporcionada por el modelo de red neuronal. Ahora veamos la eficacia del modelo por etiqueta. Recordemos que la salida corresponde a un vector numérico donde el orden es el siguiente: DoS, normal, Probing, R2L y U2R. Es decir, el orden alfabético de las etiquetas.

```{r}
AccuracyPerLabel(confusion.matrix, list.results$best_testing_set)
```

Para las clases DoS, normal, Probing y R2L, la tasa de aciertos está por encima del 99.3%. Mientras que para la clase U2R es de solo el 33.3%. Lo último se debe a la poca cantidad de ejemplos para entrenamiento proporcionados que hace que el algoritmo no pueda generalizar de la manera adecuada. Sin embargo, se espera que la eficacia para esta clase incremente conforme se agreguen mayor cantidad de ejemplos para el entrenamiento.

Para poder calcular las medidas de rendimiendo binarias correspondientes a la sensitividad, especificidad, precisión y la graficación de la curva ROC es necesario llevar la matriz de confusión de cinco clases a una una matriz binaria, es decir, con clases Attack y normal.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(list.results$best_testing_set,
                                                             list.results$best_predictions)
attack.normal.confusion.matrix
```

De esta manera se observa que sólo existen 28 errores en el proceso de clasificación, donde 12 pertenecen a falsos negativos y 16 corresponden a falsos positivos. Es importante resaltar que el modelo está realizado para que la clase objetivo sea la detección de ataques, esto dicho para la correcta interpretación de la matriz de confusión. Ahora que se tiene la matriz de confusión binaria es posible calcular las medidas de rendimiento binarias mencionadas con anterioridad.

```{r}
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

En las tres medidas se obtuvo un excelente desempeño, todas estas tuvieron un porcentaje superior a 99.5%. Esto nos indica que el modelo es bueno clasificando cualqueir tipo de tráfico de manera correcta. Es decir, acierta de forma correcta identificando los ataques y el tráfico normal.

A continuación se graficará la curva ROC que nos dará una perspectiva gráfica con la que el modelo clasifica. En este gráfico se grafica la proporción de aciertos contra la proporción de fallos. La correcta interpretación de este gráfico es la siguiente: medir la certeza con la que algoritmo toma sus decisiones. Esto debido a que las predicciones fueron ordenadas de manera descendente utilizando la probabilidad de predicción de los registros, en el inicio del *eje X* se tienen las predicciones realizadas con mayor probabilidad, y a medida que nos desplazamos hacia la derecha del mismo eje la probabilidad de las predicciones va decrementando. Dicho esto, para la creación de la curva ROC se necesita un vector de probabilidades, un vector de predicciones y un vector real que corresponde al correcto nombramiento de un registro.

```{r, fig.align="center"}
probabilities = predict(list.results$best_model,
                        list.results$best_testing_set[, 1:(ncol(list.results$best_testing_set)-1)])

roc.data = DataROC(list.results$best_testing_set, probabilities,
                   list.results$best_predictions)

generate_ROC(scores = roc.data$Prob, real = roc.data$Label,
             pred = roc.data$Prediction)
```

En la curva se observa que la mayoría de los aciertos son logrados con una alta probabilidad, conforme la probabilidad va decrementando, el modelo empieza a cometer algunos pocos errores. Al final la mayoría de los errores son cometidos por aquellas predicciones realizadas con baja probabilidad.

####Segundo nivel de clasificación (K-Medias)

A continuación se añadirá el segundo nivel de clasificación que corresponde al uso de K-Medias para tomar todos aquellos registros clasificados como normal para tratar de corregir los falsos negativos producidos por la red neuronal. El algoritmo de K-Medias será implementado con dos grupos debido a que en la sección de K-Medias se ilustra que con dos clusters la tasa de aciertos es mayor.

```{r}
kmeans.set = list.results$best_testing_set[list.results$best_predictions == "normal",]
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

Acá se observa como fueron extraidos los 12 registros que no fueron correctamente clasificados, y los otros 4417 registros que si fueron correctamente clasificados como normal. El objetivo es clasificar la mayor cantidad de esos 12 registros que son ataques como ataques.

En la sección de K-Medias se mencionó que utilizando dos centroides en varias iteraciones se obtuvo el mismo resultado. También se mencionó que esto fue debido a que el algoritmo convergió todas esas veces al mismo mínimo local. K-Medias es un algoritmo en el que la preselección de los centroides se hace de manera aleatoria, y es posible obtener diferentes resultados si se hacen múltiples corridas del algoritmo. Por lo anterior, precalcularemos los centroides ejecutando el algoritmo de K-Medias 100 veces y luego promediaremos la posición de los centroides finales. De esta manera, tendremos mejor posicionados los centroides del mejor mínimo local y podremos obtener mejores resultados.

```{r}
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)

#Promediando los centroides
matrix.centers = matrix.centers/100
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)
```

Una vez que el modelo fue entrenado, veamos sus predicciones.

```{r}
predictions = OrderKmeans(kmeans.model)

confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)

confusion.matrix.kmeans.model
```

Se observa que cero de los 12 ataques fueron detectados.E es decir, volvemos a tener 12 falsos negativos. Dicho esto, aparentemente el uso de K-Medias en esta ocación no fue eficaz debido a que el desempeño del modelo quedo intacto, esto da indicios a pensar que esos 12 falsos negativos están mezclados dentro de lo que es el tráfico normal y no son notablemente separables. Por otra parte hay un aspecto positivo a destacar que es el hecho de que el uso de K-Medias no deterioró de gran manera el trabajo hecho por el modelo de redes neuronales. Se espera que este comportamiento mejore conforme haya mayor cantidad de falsos negativos luego de pasar el primer nivel de clasificación. A continuación se calculará la tasa de aciertos y de errores del modelo.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)*100
```

Evidentemente la tasa de aciertos es bastante alta debido a que la gran mayoría del tráfico correspondía a tráfico normal y el algoritmo clasificó todos los registros salvo uno como tráfico normal. A continuación veamos la eficacia por etiquetas. Las posiciones del vector de salida corresponden a la eficacia de las etiquetas Attack y normal respectivamente.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

Se obtuvo 0% de acierto en la predicción de ataques. Esto no es bueno debido a que el objetivo es la detección de los ataques. Sin embargo, es bueno que la tasa aciertos en el tráfico normal sea tan alta, ya que esto da indicio de que no se generaron muchos falsos positivos ni falsos negativos. Ahora veamos las medidas de sensitividad, especificidad y precisión.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

La especificidad es bastante alta, esto quiere decir que el algoritmo tiene alta precisión detectando el tráfico normal, por otra parte, la sensitividad y precisión son 0. Esto nos dice que el algoritmo no clasificó bien ningún ataque.

#### Estadísticas totales
En esta sección se unificarán las estadísticas de ambos niveles de los modelos utilizados para evaluar el desempeño conjunto. Se empezará por unificar las dos matrices de confusión, para poder calcular las estadísticas utilizadas con anterioridad.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix,
                                          confusion.matrix.kmeans.model)
confusion.matrix.two.labels 
```

Se observa que la martriz de confusión quedó muy parecida a la matriz de confusión del modelo de red neuronal, salvo que ahora hay un falso positivo más. Ahora calcularemos las medidas de rendimiento de tasa de acierto, tasa de error, sensitividad, especificidad y precisión.

```{r}
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
```

Las medidas practicamente iguales a las del modelo de red neuronal, esto es porque la aplicación de K-Medias no proporcionó ningún aspecto positivo ni negativo significante.

#### Conclusiones

Individualmente el modelo de red neuronal posee un excelente rendimiento con bajas tasas de falsos positivos y falsos negativos. Al combinarse con K-Medias no se logró ninguna mejora. Tampoco esto deterioró el modelo, aspecto que es positivo. La situación del segundo modelo espera que mejore en escenarios donde el primer nivel tenga una mayor cantidad de falsos negativos, donde quizás los elementos queden con divisiones más obvias y detectables por el algoritmo de K-Medias.

### Máquinas de Soporte Vectorial

En esta sección se describirán las actividades realizadas para el entrenamiento y evaluación de máquinas de vectores de soporte en el ámbito de la deteccción de intrusos en redes de computadoras. Esta sección se subdivide en dos grandes partes, que son entrenamiento del modelo y evaluación del modelo. Esto debido a que los pasos y observaciones para ambos procesos serán realizadas de manera individual.

####Entrenamiento del modelo

Para el entrenamiento de la máquina de vectores de soporte se utilizará el kernel radial, esto debido a que Bhavsar y Waghmare expusieron en su publicación *Intrusion Detection System Using Data Mining Technique:
Support Vector Machine*, que el kernel radial es más preciso y veloz a la hora de entrenar y clasificar que los otros kernels lineal, polinomial o sigmoide. Los parámetros para el algoritmo de SVM con kernel radial tiene los parámetros *cost* y *gamma* que deben ser elegidos. Sin embargo, en esta sección se utilizarán los parámetros por defecto, ya que el objetivo acá es probar el desempeño general del modelo. Más adelante, en la sección de selección de parámetros, se seleccionarán los mejores parámetros para el modelo. Se utilizará el paquete *e1071* debido a que en la documentación en blogs y tutoriales es el más utilizado y es el que posee más documentación. Adicionalmente este paquete es una interfaz para la biblioteca *libSVM*, la cual es la biblioteca de SVM más utilizada.

Se iniciará la descripción de las actividades realizadas eliminando variables parciales, cargando el archivo de funciones y la vista minable del conjunto de entrenamiento.

```{r}
rm(list = ls())
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv",
sep = ",", header = TRUE)
source("../source/functions/functions.R")
```

Es importante mencionar que en esta sección, al igual que en la de redes neuronales se hará el entrenamiento y la evaluación del modelo haciendo uso únicamente del conjunto de entrenamiento, y aplicando la técnica de validación de modelos llamada validación cruzada de 10 conjuntos.

Como se mencionó con anterioridad el paquete a ser utilizado es *e1071*, por consiguiente se procederá a instalarlo y cargarlo en el ambiente de trabajo.

```{r, eval=FALSE}
install.packages("e1071")
library("e1071")
```

Una vez que se tiene nuestro ambiente de trabajo preparado, se eliminarán aquellas etiquetas del conjunto de datos que no van a ser utilizadas a los largo del proceso de entrenamiento del modelo. Este nivel del modelo híbrido poseerá cinco clases objetivos que son DoS, normal, Probing, R2L, y U2R; esto con la finalidad de que la salida para el especialista sea más entendible y pueda identificar las fallas de seguridad acontándolas dentro de estas cuatro clases de ataques. Dicho esto, eliminaremos el resto de las etiquetas.

```{r}
dataset = dataset.training
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Es necesario que todos los tipos de datos de las diferentes columnas sean de tipo numérico para poder entrenar a la máquina de vectores de soporte. Por este motivo, las columnas predictoras serán transformadas a tipo numérico. Adicionalmente, como haremos tareas de clasificación, la columna objetivo la transformaremos a tipo *factor*.

```{r}
for (i in 1 : (ncol(dataset) -1) )
  dataset[,i] = as.numeric(dataset[,i])

dataset[,ncol(dataset)] = as.factor(dataset[,ncol(dataset)])
```

Para reducir el tiempo de entrenamiento y mejorar la precisión de las predicciones es buena práctica escalar el conjunto de datos a valores que posean media cero y desviación estándar uno.

```{r}
dataset = ScaleSet(dataset)
```

La estategia utilizada para la evaluación del modelo será la utilización de validación cruzada de 10 conjuntos. La función *CVSet* toma un conjunto de datos y establece 10 divisiones de igual longitud del conjunto de datos y las retorna en una lista de *dataframes*.

```{r}
cv.sets = CVSet(dataset, k = 10, seed = 22)
length(cv.sets)
```

Se observa que la longitud de la lista es de 10 posiciones, esto debido a que en cada posición se posee un *dataframe* que corresponde a un subconjunto del conjunto de datos original. Todos los registros entre los *dataframes* son diferentes debido a que el muestreo se hizo sin reemplazo. Para seguir con las actividades se procederá a inicializar algunas variables.

```{r, eval=FALSE}
results = vector(mode = "numeric", length = 10)
list.results = list(0, 0, 0, 0)
names(list.results) = c("results", "best_model", "best_testing_set", "best_predictions")
best.accuracy = 0
```

El proceso de entrenamiento y validación del modelo es bastante largo y por eso se almacenarán en una lista los resultados correspondientes a la eficacia del modelo en cada iteación, el mejor modelo, el conjunto de datos de prueba, que originó la predicción, y el mejor conjunto de predicciones. De esta manera, la lista puede ser guardada como un objeto y ser exportada a un archivo que posteriormente puede cargarse y no es necesario esperar a la realización de este paso cada vez que se deseen analizar los resultados. El siguiente fragmento de código es el encargado de realizar las tareas mencionadas con anterioridad.

```{r, eval=FALSE}
for (i in 1:10)
{
  #Extrayendo el conjunto de datos
  testingset = as.data.frame(cv.sets[[i]])
  trainingset = cv.sets
  trainingset[[i]] = NULL
  trainingset = do.call(rbind, trainingset)
  
  #Entrenamiento de SVM
  model = svm(Label ~ .,
              data = trainingset,
              kernel = "radial",
              scale = FALSE,
              probability = TRUE)
  
  #Realizando las predicciones
  predictions = predict(model, testingset[, 1:(ncol(testingset)-1)],
                        type = "class")
  
  
  #Calculando la tasa de aciertos
  accuracy = mean(testingset[, ncol(testingset)] == predictions)
  
  #Almacenando el resultado
  results[i] = accuracy
  
  #Almacenando el mejor resultado
  if(best.accuracy < accuracy)
  {
    list.results$best_model = model
    list.results$best_testing_set = testingset
    list.results$best_predictions = predictions
    best.accuracy = accuracy
  }
}
```

Una vez que se culmina el proceso de entrenamiento y validación, se almacenan en la lista los resultados parciales de cada iteración y se exporta el modelo.

```{r, eval=FALSE}
list.results$results = results
saveRDS(list.results, "../source/default_parameters/original_set/SVM/Tests/list_results.rds")
```

#### Evaluación del modelo
En esta sección se hará la evaluación de los resultados obtenidos en la sección anterior, adicionalmente se tomará el mejor modelo y las mejores predicciones para agregar el segundo nivel de clasificación correspondiente al algoritmo K-Medias.

Se empezará por establecer el ambiente de trabajo eliminando variables parciales, cargando el paquete *e1071*, cargando el archivo de funciones y la lista con la información exportada previamente.

```{r}
rm(list = ls())
library("e1071")
source("../source/functions/functions.R")
list.results = readRDS("../source/default_parameters/original_set/SVM/Tests/list_results.rds")
```

A continuación se visualizará la eficacia obtenida en cada iteración del proceso de validación cruzada y se calculará la media de acierto.

```{r}
list.results$results
mean(list.results$results) * 100
```

La media de aciertos es de 99.19%. Esto demuestra que que las máquinas de soporte vectorial pueden tener un muy buen desempeño en la tarea de detectar anomalías conocidas en redes de computadoras. A continuación se creará una matriz de confusión del mejor modelo obtenido en el proceso para visualizar gráficamente el desempeño del algoritmo.

```{r}
confusion.matrix = table(Real = list.results$best_testing_set[,ncol(list.results$best_testing_set)],
                         Prediction = list.results$best_predictions)
confusion.matrix
```

Se puede ver una matriz bastante ordenada con pocos elementos fuera de la diagonal. A continuación se visualizará la tasa de aciertos para el mejor modelo.

```{r}
accuracy = mean(list.results$best_testing_set[,ncol(list.results$best_testing_set)] == 
                  list.results$best_predictions)

accuracy * 100
ErrorRate(accuracy) * 100
```

La mejor tasa de aciertos fue de 99.37%. Una tasa de aciertos bastante buena, ahora observemos la eficacia por etiquetas del modelo. Recordemos que la salida es un vector numérico ordenado alfabeticamente por el nombre de las etiquetas. Es decir, DoS, normal, Probing, R2L, U2R.

```{r}
AccuracyPerLabel(confusion.matrix, list.results$best_testing_set)
```

Para las clases DoS, normal, Probing y R2L, el desempeño es bastante bueno, en todos los casos por encima del 92.95%. Por otra parte, para la clase U2R es de sólo 33.33%, esto es debido a la poca cantidad de registros presentes en el conjunto de datos para entrenar al modelo con esta clase de ataques que hace que el algoritmo no pueda generalizar de buena manera para esta clase de ataques. Se espera que incrementando la cantidad de registros de esta clase de ataques aumente la eficacia para su detección.

Para poder calcular las medidas de rendimiento binarias correspondientes a la sensitividad, especificidad, precisión y la graficación de la curva ROC es necesario llevar a la matriz de cinco clases a una matriz binaria, es decir, con clases Attack y normal.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(list.results$best_testing_set,
                                                             list.results$best_predictions)
attack.normal.confusion.matrix
```

De esta manera se observa que sólo existen 55 errores en la clasificación, de los cuales 25 pertenecen a falsos negativos y 26 a falsos positivos. Es importante resaltar que el modelo fue hecho para que la clase objetivo sea la detección de ataques, esta información es importante para la correcta interpretación de la matriz de confusión. Ahora que se tiene la matriz de confusión binaria es posible calcular las medidas de rendimiento mencionadas previamente.

```{r}
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

En la tres medidas se obtuvo un excelente desempeño, todas tuvieron un porcentaje superior a 99.32%. Esto nos indica que el modelo es bueno clasificando el tráfico de manera correcta, es decir, acierta de buena manera identificando ataques y tráfico normal.

A continuación se graficará la curva ROC que nos dará una perspectiva gráfica de la certeza de las clasificaciones del modelo. En este gráfico se ilustra la proporción de aciertos contra la proporción de fallos ordenados por la probabilidad de la toma de la decisión del modelo al clasificar cierto registro; por consiguiente, en el inicio del *eje X* se obtendrán aquellas predicciones que fueron hechas con mayor puntaje de certeza; a medida que nos desplazamos hacia la derecha en dicho eje, el puntaje de las certezas va decrementando. Dicho esto, para la creación de la curva ROC se necesita de un vector de probabilidades, un vector de predicciones y un vector real que corresponde al correcto nombramiento del registro.

```{r}
probabilities = attr(predict(list.results$best_model,
                             list.results$best_testing_set[, 1:(ncol(list.results$best_testing_set)-1)],
                             probability = TRUE), "probabilities")

roc.data = DataROC(list.results$best_testing_set, probabilities,
                   list.results$best_predictions)
generate_ROC(scores = roc.data$Prob, real = roc.data$Label,
             pred = roc.data$Prediction)
```

En la curva se muestra como el comportamiento es un tanto errático con respecto a la certeza con la que se toman las decisiones, es decir, hay decisiones erroneas que se toman con alta certeza. El mejor rendimiento se alcanza con valores de certeza intermedios que es donde la función se separa más de la línea de identidad.

#### Segundo nivel de clasificación (K-Medias)
A continuación se añadirá el segundo nivel de clasificación que corresponde al uso de K-Medias para tomar todos aquellos registros clasificados como normal para tratar de corregir los falsos negativos producidos por la máquina de vectores de soporte. El algoritmo de K-Medias será implementado con dos clusters debido a que en la sección de K-Medias se ilustra que con dos clusters se obtuvieron mejores resultados que con cinco.

```{r}
kmeans.set = list.results$best_testing_set[list.results$best_predictions == "normal",]
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

Acá se observa como fueron extraídos los 25 registros que no fueron correctamente clasificados
y los 4407 registros que si fueron correctamente clasificados como tráfico normal. El objetivo es clasificar la mayor cantidad de esos registros que son ataques como ataques.

En la sección de K-Medias se mencionó que utilizando dos centroides en varias iteraciones se obtuvo el mismo resultado, también se mencionó que esto fue debido a que el algoritmo convergió todas esas veces al mismo mínimo local. K-Medias es un algoritmo en el que la preselección de los centroides se hace de manera aleatoria, por esta razón, es posible obtener diferentes resultados si se hacen multiples corridas del algoritmo. Por lo anterior, se precalcularán los centroides ejecutando el algoritmo de K-Medias 100 veces y luego se promediará la posición de los centroides finales. De esta manera, se tendrán mejor posicionados los centroides desde el inicio permitiéndonos acercarnos al mínimo local y obtener mejores resultados.

```{r}
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)

#Promediando los centroides
matrix.centers = matrix.centers/100
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)
```

Una vez que el modelo fue entrenado, veamos sus predicciones.

```{r}
predictions = OrderKmeans(kmeans.model)
confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)
confusion.matrix.kmeans.model
```

Se observa que 0 de los 25 ataques fueron detectados, es decir, volvemos a tener 25 falsos negativos. Dicho esto, aparentemente el uso de K-Medias en esta ocasión no fue eficaz debido a que el desempeño del modelo quedó intacto, esto da indicios a pensar que esos 25 falsos negativos están mezclados dentro lo que es el tráfico normal y no son notablemente separables. Por otra parte hay un aspecto positivo a destacar que es el hecho de que el uso de K-Medias no deterioró de gran manera el trabajo hecho por el modelo de máquina de vectores de soporte. Se espera que este comportamiento mejore conforme haya mayor cantidad de falsos negativos luego de pasar el primer nivel de clasificación. A continuación se calcularán las tasas de acierto y de error del modelo.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)*100
```

Evidentemente la tasa de aciertos es bastante alta debido a que la gran mayoría del tráfico correspondía a tráfico normal y el algoritmo clasificó todos los registros salvo uno como tráfico normal. A continuación veamos la eficacia por etiqueta. Las posiciones del vector de salida corresponden a las clases Attack y normal respectivamente.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

Se obtuvo 0% de acierto en la predicción de ataques, esto no es bueno debido a que el objetivo es la detección de ataques; sin embargo, es bueno que la tasa de aciertos para tráfico normal sea tan alta, ya que esto refleja que no se generaron muchos falsos positivos ni falsos negativos. Ahora veamos las medidas de sensitividad, especificidad y precisión.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

La especificidad es bastante alta, esto quiere decir que el modelo es excelente clasificando el tráfico normal, por otra parte, la sensitividad y la especificidad son cero. En consecuencia, el modelo tiene pobre desempeño clasificando los ataques.

#### Estadísticas totales
A continuación se unificarán las estadísticas de ambos niveles de los modelos utilizados para evaluar el desempeño conjunto. Se empezará por unificar las dos matrices de confusión para poder calcular las estadísticas utilizadas con anterioridad.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix,
                                          confusion.matrix.kmeans.model)
confusion.matrix.two.labels
```

Se observa que la matriz de confusión quedó muy parecida a la matriz de confusión del modelo de máquina de vectores de soporte, salvo que ahora hay un falso positivo más. Ahora calcularemos las medidas de rendimiendo de tasa de acierto, tasa de error, sensitividad, especificidad y precisión.

```{r}
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
```

Las medidas quedaron practicamente invariantes con respecto al modelo de máquina de vectores de soporte, esto debido a que la aplicación de K-Medias fue irrelevante.

####Conclusiones
Individualmente el modelo de SVM posee un muy buen desempeño con bajas tasas de falsos positivos y falsos negativos. Con respecto a la curva ROC, se observa que el modelo comete errores tomando decisiones con un elevado valor de certeza, situación que deteriora un poco su rendimiento.

Al combinarse el modelo de SVM con K-Medias no se logró absolutamente nada, no se mejoró el proceso de detección de intrusos. como aspecto favorable se puede rescatar el hecho de que no se deterioró el rendimiento del primer nivel del modelo híbrido. Por último, la situación con la inclusión de K-Medias se espera que mejore conforme se cometan más fallos de tipo falsos negativos por parte del primer nivel.

### Conclusiones generales

Los modelos de red neuronal y máquina de vectores de soporte de manera individual funcionan de gran manera, con bajas tasas de falsos positivos, falsos negativos, y una gran tasa de acierto. Comparándolos entre ellos, el modelo de red neuronal tiene un mejor desempeño individualmente en cada una de las clases de ataques presentes en el conjunto de datos. Adicionalmente, las decisiones que toma con alta certeza suelen ser acertadas, esta característica se puede observar en la curva ROC.

La inclusión del segundo nivel de K-Medias se comportó en ambos casos de igual manera. Esta no aportó absolutamente nada a la detección de ataques, sin embargo, un aspecto positivo fue que esta no afectó de gran manera el desempeño del primer nivel. Por lo que se espera que con mayor cantidad de ataques no detectados por el primer nivel K-Medias pueda tener un mejor desempeño.

##Análisis sobre el conjunto de prueba
En esta sección se listarán las actividades concernientes al entrenamiento y evaluación del modelo haciendo uso del conjunto de datos para la fase de entrenamiento y haciendo uso del conjunto de prueba para la fase de evaluación. Hasta este punto en el documento se ha probado que ante ataques conocidos los modelos de SVM y NN tienen un muy buen desempeño a la hora de clasificar ataques y el segundo nivel de K-Medias no parece ser útil. En esta oportunidad el conjunto de prueba tendrá ataques no incluidos en el conjunto de entrenamiento, situación que permitirá medir la capacidad de generalización de los modelos de SVM y NN. Adicionalmente, se espera que si hay mayor cantidad de falsos negativos en el primer nivel, entonces K-Medias pueda ser de utilidad. Se iniciará haciendo el análisis sobre el uso de las redes neuronales.

###Redes Neuronales
En esta sección se describirán las actividades realizadas para el entrenamiento y evaluación de las redes neuronales en el ámbito de la detección de intrusos en redes de computadoras. Esta sección se subdivide en dos grandes partes concernientes al entrenamiento del modelo y evaluación del modelo. Esto debido a que los pasos y observaciones se harán de manera individual en cada fase.

####Entrenamiento del modelo
Se aplicará el mismo criterio que se propuso en la sección de análisis sobre el conjunto de entrenamiento; es decir, se usará una arquitectura con 40 neuronas de entrada, una capa intermedia de 20 neuronas y una capa de salida de 5 neuronas. Se usará un modelo de 5 clases donde 4 corresponden a las etiquetas de los ataques y 1 a la etiqueta del tráfico normal. De igual manera, se hará uso del paquete *nnet*. La única diferencia es que ahora se hará uso del conjunto total del conjunto de entrenamiento para el entrenamiento del modelo y se hará la evaluación del modelo haciendo uso del conjunto de datos de prueba, en esta ocasión no se hará uso de validación cruzada de 10 conjuntos como técnica de validación.

Se empezará establecer el ambiente de trabajo eliminando variables parciales, cargando el archivo de funciones y la vista minable del conjunto de entrenamiento.

```{r, eval=FALSE}
rm(list = ls())
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv", sep = ",", header = TRUE)
source("../source/functions/functions.R")
```

El paquete utilizado para el entrenamiento de las redes neuronales es *nnet*, a continuación el paquete se cargará.

```{r, eval=FALSE}
library("nnet")
```

Una vez que tenemos nuestro ambiente de trabajo preparado se eliminarán aquellas etiquetas del conjutno de datos que no van a ser utilizadas a lo largo del proceso de entrenamiento del modelo. El primer nivel de detección del modelo híbrido posee cinco clases obtetivo que son DoS, normal, Probing, R2L y U2R. Esto con la finalidad de que la salida para el especialista sea más entendible y pueda identificar la(s) falla(s) de seguridad acotándola dentro de estas cuatro clases de araques. Dicho esto eliminaremos el resto de las etiquetas.

```{r, eval=FALSE}
dataset = dataset.training
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Es obligatorio que para el uso de las redes neuronales todas las variables predictoras sean de tipo numérico. Por lo tanto, se transformarán cada una de estas a tipo numérico y la columna objetivo se transformará en tipo *factor* debido a que se realizarán labores de clasificación.

```{r, eval=FALSE}
for (i in 1 : (ncol(dataset.training) -1) )
  dataset.training[,i] = as.numeric(dataset.training[,i])

dataset.training[,ncol(dataset.training)] = as.factor(dataset.training[,ncol(dataset.training)])
```

Para acelerar el tiempo de entrenamiento y tener un modelo más preciso es buena práctica escalar el conjunto de datos a rangos similares. En este caso, todas las columnas predictoras tendrán media cero y desviación estándar uno.

```{r, eval=FALSE}
dataset.training = ScaleSet(dataset.training)
```

Ya se tienen el conjunto de datos listo y el ambiente de trabajo preparado, a continuación se iniciará el proceso de entrenamiento. De igual manera que se realizó en la sección análisis sobre el conjunto de prueba el modelo creado será guardado en un objeto debido a que el proceso de entrenamiento es largo y es tedioso tener que esperar a su entrenamiento cada vez que se quiera analizar el modelo. Adicionalmente, en esta oportunidad se calculará el tiempo que tarda el modelo entrenándose. Esto, para poder comparar el tiempo contra la máquina de vectores de soporte y luego contra el tiempo de entrenamiento luego de hacer la selección de características y selección de parámetros.

```{r, eval=FALSE}
start.time = Sys.time()
set.seed(22)

model = nnet(Label ~ .,
                   data = dataset.training,
                   size = 20,
                   maxit = 100)

total.time = Sys.time() - start.time
```

Por último, el tiempo y el modelo creado se guardan en una lista y se exportan como un objeto para su posterior uso.

```{r, eval=FALSE}
list.results = list(total.time, model)
saveRDS(list.results, file = "../source/default_parameters/original_set/NN/Real_Model/list_results.rds")
```

####Evaluación del modelo
En esta sección se hará la evaluación de los resultados obtenidos en la sección anterior, adicionalmente se tomará el mejor modelo y las mejores predicciones obtenidas para agregarle el segundo nivel de clasificación correspondiente al algoritmo K-Medias.

Se empezará por establecer el ambiente de trabajo eliminando variables parciales, cargando el paquete *nnet*,cargando el archivo de funciones, la lista con información exportada previamente y el conjunto de datos de prueba.

```{r}
rm(list = ls())
library("nnet")
source("../source/functions/functions.R")
results = readRDS("../source/default_parameters/original_set/NN/Real_Model/list_results.rds")
testing.set = read.csv("../dataset/NSLKDD_Testing_New.csv",
                       sep = ",", header = TRUE)
```

Se empezará por eliminar las etiquetas innecesarias, transformar las variables predictoras a tipo numérico y la columna objetivo a tipo *factor*, y escalar las variables predictoras dentro de la misma media y desviación estándar.

```{r}
#Eliminando eiquetas
testing.set$Label_Normal_TypeAttack = NULL
testing.set$Label_Num_Classifiers = NULL
testing.set$Label_Normal_or_Attack = NULL

#Cambiando el tipo de dato
for (i in 1 : (ncol(testing.set) -1) )
  testing.set[,i] = as.numeric(testing.set[,i])

testing.set[,ncol(testing.set)] = as.factor(testing.set[,ncol(testing.set)])

#Escalando las variables predictoras
testing.set = ScaleSet(testing.set)
```

Hasta este punto ya se tienen listos el ambiente de trabajo, conjunto de datos y la lista de resultados de la sección anterior. A continuación se extraerá el modelo y el tiempo de entrenamiento del modelo y se visualizará el tiempo correspondiente al entrenamiento del modelo.

```{r}
training.time = results[[1]]
model = results[[2]]
training.time
```

A partir de este punto se empezará con el análisis del modelo. Todos los pasos involucrados con el tiempo de entrenamiento y predicción serán cronometrados y al final serán sumados para tener una perspectiva del tiempo necesario para cada fase. Se iniciará con el cálculo de las predicciones.

```{r}
start.time.predictions = Sys.time()
predictions = predict(model, testing.set[, 1:(ncol(testing.set)-1)], type = "class")
total.time.predictions = Sys.time() - start.time.predictions
total.time.predictions
```

A continuación, se creará una matriz de confusión que nos ayude a ver gráficamente el desempeño del modelo durante el proceso de clasificación.

```{r}
confusion.matrix = table(Real = testing.set[,ncol(testing.set)],
                         Prediction = predictions)

confusion.matrix
```

Si se compara con la matriz de confusión del modelo en la sección de análisis sobre el conjunto de entrenamiento se observa una matriz de confusión mucho más desordenada. Sin embargo, a simple vista se observa que la diagonal acumula la mayoría de los registros, adicionalmente se observa que existen más falsos negativos que falsos positivos, es decir, hubo más errores en los que se clasificó tráfico normal como ataques que ataques que se clasificaron como tráfico normal. A continuación veamos la tasa de aciertos y la tasa de errores.

```{r}
accuracy = mean(testing.set[,ncol(testing.set)] == predictions)
accuracy * 100
ErrorRate(accuracy) * 100
```

Ya no se tiene un desempeño tan alto como se tuvo en el análisis sobre el conjunto de entrenamiento y es entendible, debido a que en el conjunto de prueba hay clases ataques que no estuvieron presentes en conjunto de entrenamiento. Sin embargo, una tasa de aciertos de 76.81% es bastante alta, y se espera que la inclusión de K-Medias incremente la tasa de aciertos. Ahora veamos la precisión por etiquetas, recordemos que la salida corresponde a un vector que corresponde al siguiente orden: DoS, normal, Probing, R2L y U2R.

```{r}
AccuracyPerLabel(confusion.matrix, testing.set)
```

Para las etiquetas de DoS, normal y Probing el rendimiento es bastante bueno, en especial para DoS y normal. Sin embargo, para R2L y U2R es bastante pobre. Esto puede deberse a la poca cantidad de registros usados para el entrenamiento en ambos casos, en particular para la clase U2R.

A continuación crearemos una matriz de confusión binaria para poder calcular las medidas de rendimiento binarias correspondientes a sensitividad, especificidad, precisión y la graficación de la curva ROC.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(testing.set, predictions)
attack.normal.confusion.matrix
```

Se nota una baja cantidad de falsos positivos y una alta cantidad de falsos negativos, y una alta tasa de aciertos con respecto a la clasificación de los registros en la diagonal. Ahora que hay mayor cantidad de falsos negativos el algoritmo de K-Medias puede aportar más al tema de la clasificaicón. Ahora veamos las medidas de rendimiento binarias mencionadas con anterioridad.

```{r}
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

La sensitividad nos dice que el 65.56% de los ataques presentes en el conjunto de datos fueron detectados. Por otra parte, la especificidad nos dice que el 95.16% de los registros pertenecientes al tráfico normal fueron detectados. Por último, la precisión nos dice que el 95.75% de las clasificaciones de ataques fueron correctas.

En general el modelo tiene un buen desempeño en la detección de tráfico normal y la mayoría de las predicciones que hace como ataques son verdaderas. Sin embargo, el gran problema son los falsos negativos que se espera que con la utilización de K-Medias la tasa de aciertos pueda mejorar. A continuación graficaremos la curva ROC.

```{r, fig.align="center"}
probabilities = predict(model, testing.set[, 1:(ncol(testing.set)-1)])
roc.data = DataROC(testing.set, probabilities, predictions)
generate_ROC(roc.data$Prob, roc.data$Label, roc.data$Prediction)
```

En comparación con la sección de análisis sobre el conjunto de entrenamiento, se observa un desempeño notablemente inferior, en esta ocasión la curva no tiene tanta distancia de separación de la función de identidad y se puede observar como ahora comete errores con altos valores de certeza. Esta situación es entendible y esperada, debido a que el conjunto de prueba contiene nuevas clases de ataques.

####Segundo nivel de clasificación (K-Medias)
A continuación se añadirá el segundo nivel de clasificación que corresponde al uso de K-Medias para tomar todos aquellos registros clasificados como normal para tratar de corregir los falsos negativos producidos por la red neuronal. El algoritmo de K-Medias será implementado con dos clusters debido a que en la sección de K-Medias se ilustra que con dos clusters la varianza acumulada es la adecuada, adicionalmente se probó que con dos clusters se obtuvieron mejores resultados que con cinco.

```{r}
kmeans.set = testing.set[predictions == "normal", ]
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

Se observa como se extrayeron los 4420 falsos negativos en conjunto con el resto del tráfico normal y ese será el conjunto de datos para la aplicación de K-Medias. A continuación se precalcularán los centroides. Las actividades relacionadas con el entrenamiento y predicciones serán cronometradas de igual forma que con el primer nivel de clasificación del modelo.

```{r}
start.time.kmeans.training = Sys.time()
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)

#Promediando los centroides
matrix.centers = matrix.centers/100
total.time.kmeans.training = Sys.time() - start.time.kmeans.training
total.time.kmeans.training
```

Ahora se realizarán las predicciones.

```{r}
start.time.kmeans.predictions = Sys.time()
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)

total.time.kmeans.predictions = Sys.time() - start.time.kmeans.predictions
total.time.kmeans.predictions
```

Ahora se creará la matriz de confusión producto de la clasificación de K-Medias.

```{r}
predictions = OrderKmeans(kmeans.model)
confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)
confusion.matrix.kmeans.model
```

Se observa como se detectaron 2334 ataques, sin embargo, ahora hay mayor cantidad de falsos positivos. Veamos la tasa de aciertos y la tasa de errores.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)*100
```

Se obtiene un 72.76% de acierto, es un número bastante bueno, similar al de la detección de intrusos en el primer nivel. Ahora veamos la tasa de aciertos por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

Se detecta alrededor de la mitad de los ataques presentes y se separa con 82% de certeza el tráfico normal. Ahora veamos las medidas binarias de sensitividad, especificidad y precisión.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

El modelo tiene un desempeño decente en la clasificación del tráfico normal y un desempeño intermedio en la detección de ataques. Ahora veamos las estadísticas totales producto de la mezcla de ambos niveles. Empecemos por ver la matriz de confusión.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix, confusion.matrix.kmeans.model)
confusion.matrix.two.labels
```

El resultado total refleja un incremento positivo en la detección de ataques y en la reducción de falsos negativos. Por otra parte, el incremento de los falsos positivosy el decremento de la certeza de clasificación del tráfico normal son aspectos negativos. Veamos la tasa de aciertos y de errores.

```{r}
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
```

La tasa de aciertos mejoró con respecto al primer nivel de clasificación del modelo en un 5%. Una mejora significativa en la detección de intrusos. Ahora veamos como quedaron el resto de las medidas de rendimiento concernientes a la sensitividad, especificidad y precisión.

```{r}
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
```

Se nota un incremento con respecto a la sensitividad del primer nivel del 23%. Por otra parte, hubo un decremento con un promedio de alrededor 15% en la especificidad y en la precisión. En general el modelo híbrido tiene un desempeño bastante bueno, se logra incrementar la cantidad de ataques detectados y se obtiene una combinación balanceada entre los falsos negativos y falsos positivos. Por último el tiempo total para el entrenamiento y las predicciones se imprime a continuación respectivamente.

```{r}
training.time + total.time.kmeans.training
total.time.predictions + total.time.kmeans.predictions
```

####Conclusiones

El rendimiento del primer nivel con red neuronal comparado al redimiento obtenido en la sección de análisis sobre el conjunto de entrenamiento es bastante inferior. Sin embargo, es comprensible debido a que en el conjunto de prueba se agregan nuevos tipos de ataques que no estuvieron presentes en el conjunto de entrenamiento. Más allá de eso, el rendimiento es bueno, con 76% de tasa de aciertos y buenas medidas de rendimiento para la sensitividad, especificidad y precisión. Por otra parte, la curva ROC indica que el modelo no es tan certero con respecto a la toma de decisiones, es decir, comete errores con grandes valores de certeza, situación que en la sección de análisis sobre el conjunto de entrenamiento no se presentó. Adicionalmente el modelo no comete gran cantidad de falsos positivos.

El segundo nivel de K-Medias en esta oportunidad tuvo mayor cantidad de ataques debido a que el primer nivel obtuvo un gran número de falsos negativos. La tasa de aciertos del modelo de K-Medias fue del 72.76%, un número similar a la tasa de aciertos del modelo de red neuronal. K-Medias logró detectar el 50% de los ataques presentes y redujo la cantidad de falsos negativos presentes en la entrada; sin embargo, incrementó la cantidad de falsos positivos notablemente.

En conjunto, con la inclusión de K-Medias se logró un incremento de alrededor del 5% en la tasa de aciertos llegando así al 81%, y un incremento del 23% en la sensitividad. Por otra parte hubo un decremento de alrededor del 15% en la especificidad y precisión. Las comparaciones son realizadas con respecto al desempeño del primer nivel del modelo, que corresponde al clasificador de red neuronal.

En general el desempeño es bastante bueno, la gran mayoría del tráfico fue clasificado de manera satisfactoria y se produjeron alredor de 2000 falsos positivos y 2000 falsos negativos del total de los 22 mil registros presentes en el conjunto de datos. Para un especialista el hecho de que haya mayor cantidad de falsos positivos representará más trabajo desde el punto de vista que tendrá que revisar registros que no son una amenaza. Por otra parte, la presencia de falsos negativos representa un punto más sensible debido a que los ataques están presentes y no fueron detectados.

### Máquina de vectores de soporte
En esta sección se describirán las actividades realizadas para el entrenamiento y evaluación de la máquina de vectores de soporte en el ámbito de la detección de intrusos en redes de computadoras. Esta sección se subdivide en dos grandes partes concernientes al entrenamiento del modelo y evaluación del modelo. Esto debido a que los pasos y observaciones se harán de manera individual en cada fase.

####Entrenamiento del modelo

Se aplicará el mismo criterio que se propuso en la sección de análisis sobre el conjunto de entrenamiento. Es decir, se usará máquina de vectores de soporte con el kernel radial.

Se empezará por establecer el ambiente de trabajo eliminando variables parciales, cargando el archivo de funciones y la vista minable del conjunto de entrenamiento.

```{r, eval=FALSE}
rm(list = ls())
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv", sep = ",", header = TRUE)
source("../source/functions/functions.R")
```

El paquete utilizado para el entrenamiento de las máquinas de soporte vectorial es *e1071*, a continuación será cargado.

```{r, eval=FALSE}
library("e1071")
```

Una vez que tenemos nuestro ambiente de trabajo preparado se eliminarán aquella etiquetas del conjunto de datos que no van a ser utilizadas a lo largo del proceso de entrenamiento del modelo. El primer nivel de detección del modelo híbrido posee cinco clases obtetivo que son DoS, normal, Probing, R2L y U2R. Esto con la finalidad de que la salida para el especialista sea más entendible y pueda identificar la(s) falla(s) de seguridad acotándolas dentro de estas cuatro clases de ataques. Dicho esto eliminaremos el resto de las etiquetas.

```{r, eval=FALSE}
dataset.training$Label_Normal_TypeAttack = NULL
dataset.training$Label_Num_Classifiers = NULL
dataset.training$Label_Normal_or_Attack = NULL
```

Es obligatorio que para el uso de las máquinas de soporte vectorial todas las variables predictoras sean de tipo numérico. Por lo tanto, se transformarán cada una de estas a tipo numérico y la columna objetivo se transformará en tipo factor debido a que se realizarán labores de clasificación.

```{r, eval=FALSE}
for (i in 1 : (ncol(dataset.training) -1) )
  dataset.training[,i] = as.numeric(dataset.training[,i])

dataset.training[,ncol(dataset.training)] = as.factor(dataset.training[,ncol(dataset.training)])
```

Para acelerar el tiempo de entrenamiento y tener un modelo más preciso es buena práctica escalar el conjunto de datos a rangos similares. En este caso, todas las columnas predictoras tendrán media cero y desviación estándar uno.

```{r, eval=FALSE}
dataset.training = ScaleSet(dataset.training)
```

Ya se tienen el conjunto de datos listo y el ambiente de trabajo preparado, a continuación se iniciará el proceso de entrenamiento. De igual manera que se realizó en la sección análisis sobre el conjunto de entrenamiento el modelo creado será guardado en un objeto debido a que el proceso de entrenamiento es largo y es tedioso tener que esperar a su entrenamiento cada vez que se quiera analizar el modelo. Adicionalmente, en esta oportunidad se calculará el tiempo que tarda el modelo entrenándose. Esto, para poder comparar el tiempo contra la red neuronal y luego contra el tiempo de entrenamiento luego de hacer la selección de características y selección de parámetros.

```{r, eval=FALSE}
start.time = Sys.time()

set.seed(22)
model = svm(Label~.,
                data = dataset.training,
                kernel = "radial",
                scale = FALSE,
                probability = TRUE)

total.time = Sys.time() - start.time
```

Por último, el tiempo y el modelo creado se guardan en una lista y se exportan como un objeto para su posterior uso.

```{r, eval=FALSE}
list.results = list(total.time, model)
saveRDS(list.results, file = "../source/default_parameters/original_set/SVM/Real_Model/list_results.rds")
```

####Evaluación del modelo

En esta sección se hará la evaluación de los resultados obtenidos en la sección anterior, adicionalmente se tomará el mejor modelo y las mejores predicciones obtenidas para agregarle el segundo nivel de clasificación correspondiente al algoritmo K-Medias. Se empezará por establecer el ambiente de trabajo eliminando variables parciales, cargando el paquete *e1071*, cargando el archivo de funciones, la lista con información exportada previamente y el conjunto de datos de prueba.

```{r}
rm(list = ls())
library("e1071")
source("../source/functions/functions.R")
results = readRDS("../source/default_parameters/original_set/SVM/Real_Model/list_results.rds")
testing.set = read.csv("../dataset/NSLKDD_Testing_New.csv", sep = ",", header = TRUE)
```

Se empezará por eliminar las etiquetas innecesarias, transformar las variables predictoras a tipo numérico y la columna objetivo a tipo *factor*, y escalar las variables predictoras dentro de la misma media y desviación estándar.

```{r}
#Eliminando eiquetas
testing.set$Label_Normal_TypeAttack = NULL
testing.set$Label_Num_Classifiers = NULL
testing.set$Label_Normal_or_Attack = NULL

#Cambiando el tipo de dato
for (i in 1 : (ncol(testing.set) -1) )
  testing.set[,i] = as.numeric(testing.set[,i])

testing.set[,ncol(testing.set)] = as.factor(testing.set[,ncol(testing.set)])

#Escalando las variables predictoras
testing.set = ScaleSet(testing.set)
```

Hasta este punto ya se tienen listos el ambiente de trabajo, conjunto de datos y la lista de resultados de la sección anterior. A continuación se extraerá el modelo y el tiempo de entrenamiento del modelo y se visualizará el tiempo correspondiente al entrenamiento del modelo.

```{r}
training.time = results[[1]]
model = results[[2]]
training.time
```

A partir de este punto se empezará con el análisis del modelo. Todos los pasos involucrados con el tiempo de entrenamiento y predicción serán cronometrados y al final será sumados para tener una perspectiva del tiempo necesario para cada fase. Se iniciará con el cálculo de las predicciones.

```{r}
start.time.predictions = Sys.time()

predictions = predict(model, testing.set[, 1:(ncol(testing.set)-1)], type = "class")

total.time.predictions = Sys.time() - start.time.predictions
total.time.predictions
```

A continuación se creará una matriz de confusión que nos ayude a ver gráficamente el desempeño del modelo durante el proceso de clasificación.

```{r}
confusion.matrix = table(Real = testing.set[,ncol(testing.set)],
                         Prediction = predictions)
confusion.matrix
```

Si se compara con la matriz de confusión del modelo en la sección análisis sobre el conjunto de entrenamiento, se observa una matriz de confusión mucho más desordenada. Sin embargo, a simple vista se observa que la diagonal acumula la mayoría de los registros, adicionalmente se observa que existen más falsos negativos que falsos positivos; es decir, hubo más errores en los que se clasificó tráfico normal como ataques que ataques que se clasificaron como tráfico normal. A continuación veamos la tasa de aciertos y la tasa de errores.

```{r}
accuracy = mean(testing.set[,ncol(testing.set)] == predictions)
accuracy * 100
ErrorRate(accuracy) * 100
```

Ya no se tiene un desempeño tan alto como se tuvo en el análisis sobre el conjunto de entrenamiento, y es entendible debido a que en el conjunto de prueba hay clases de ataques que no estuvieron presentes en el conjunto de entrenamiento. Sin embargo, una tasa de aciertos de 77.19% es bastante alta para este escenario y se espera que con la inclusión de K-Medias se incremente aún más la tasa de aciertos. Ahora veamos la precisión por etiquetas, recordemos que la salida corresponde a un vector con el siguiente orden: DoS, normal, Probing, R2L y U2R. En comparación con el modelo de red neuronal se tiene un porcentaje de acierto ligeramente mayor, debido a que el modelo de red neuronal tuvo una tasa de aciertos de 76.81%.

```{r}
AccuracyPerLabel(confusion.matrix, testing.set)
```

Para las etiquetas de DoS, normal y Probing el rendimiento es bastante bueno, en especial para DoS y normal. Sin embargo, para R2L y U2R es bastante pobre. Esto puede deberse a la poca cantidad de registros usados para el entrenamiento en ambos casos, en particular para la clase U2R. Con respecto a la red neuronal, este modelo es mejor en la clasificación de las etiquetas DoS y normal, sin embargo, en las demás el modelo de red neuronal tiene un mejor desempeño.

A continuación crearemos una matriz de confusión binaria para poder calcular las medidas de rendimiento binarias correspondientes a sensitividad, especificidad, precisión y la graficación de la curva ROC.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(testing.set, predictions)
attack.normal.confusion.matrix
```

Se nota una baja cantidad de falsos positivos, incluso menos cantidad que en el modelo de red neuronal, y una alta cantidad de falsos negativos, cantidad mayor que en el modelo de red neuronal, y una alta tasa de aciertos con respecto a la clasificación de los registros ubicados en la diagonal. Ahora que hay mayor cantidad de falsos negativos, el algoritmo de K-Medias puede aportar más al tema de la clasificación. Ahora veamos las medidas de rendimiento binarias mencionadas con anterioridad.

```{r}
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

La sensitividad nos dice que el 63.53% de los ataques fueron detectados de forma correcta; así mismo, la especificidad nos dice que el 98.04% del tráfico normal fue clasificado de forma satisfactoria. Por último, la precisión nos dice que el 97%.72% de los registros clasificados como ataques de verdad eran ataques. Dicho esto el modelo es bastante efectivo a la hora de clasificar el tráfico normal y moderadamente bueno a la hora de clasificar los ataques; sin embargo, las decisiones tomadas con respecto a la detección de los ataques es bastante elevada, situación que hace que no tenga tantos falsos positivos. Si se compara con el modelo de red neuronal, la red neuronal detecta mayor cantidad de ataques, mientras que la máquina de vectores de soporte clafica mejor el tráfico normal.

El gran problema del modelo recae en la cantidad de falsos negativos generados. Se espera que con la inclusión de K-Medias esta situación pueda mejorar. A continuación se graficará la curva ROC.

```{r, fig.align="center"}
probabilities = predict(model, testing.set[, 1:(ncol(testing.set)-1)], probability = TRUE)
roc.data = DataROC(testing.set, attr(probabilities, "probabilities"), predictions)
generate_ROC(roc.data$Prob, roc.data$Label, roc.data$Prediction)
```

En comparación con la sección de análisis sobre el conjutno de entrenamiento, se observa un desempeño notablemente inferior, en esta ocasión, el desempeño es bastante errático, teniendo su mejor rendimiento al inicio y luego casi pegándose a la línea del azar. Que el desempeño sea inferior es entendible y esperado dada la naturaleza del conjunto de prueba donde hay nuevos tipos de ataques.

#### Segundo nivel de clasificación (K-Medias)
A continuación se añadirá el segundo nivel de clasificación que corresponde al uso de K-Medias para tomar todos aquellos registros clasificafos como tráfico normal y serán pasados al segundo nivel para corregir los falsos positivos producidos por el modelo del primer nivel correspondiente a la máquina de vectores de soporte. El algoritmo de K-Medias será implementado con dos clusters debido a que en la sección de K-Medias se ilustra que con dos clusters se acumula la mejor cantidad de varianzza, y adicionalmente se probó que con dos clusters se obtuvieron mejores resultados que con cinco clusters.

```{r}
kmeans.set = testing.set[predictions == "normal", ]
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

Se observa como se extrajeron los 4680 falsos negativos en conjunto con el resto del tráfico normal, y ese será el conjunto de datos para la aplicación de K-Medias. A continuación, se precalcularán los centroides. Las actividades relacionadas con el entrenamiento y predicción serán cronometradas de igual forma que se hizo en el primer nivel de clasificación del modelo.

```{r}
start.time.kmeans.training = Sys.time()
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)

#Promediando los centroides
matrix.centers = matrix.centers/100
total.time.kmeans.training = Sys.time() - start.time.kmeans.training
total.time.kmeans.training
```

Ahora se realizarán las predicciones.

```{r}
start.time.kmeans.predictions = Sys.time()
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)
total.time.kmeans.predictions = Sys.time() - start.time.kmeans.predictions
total.time.kmeans.predictions
```

Ahora se creará la matriz de confusión producto de la clasificación de K-Medias.

```{r}
predictions = OrderKmeans(kmeans.model)
confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)
confusion.matrix.kmeans.model
```

Se observa como se separaron 542 ataques de los 4680 iniciales, la cantidad de falsos negativos se redujo y la cantidad de falsos positivos aumentó en 80. Es una mejora bastante conservadora que tiene un incremento bastante favorable en la detección de los ataques sin desordenar de gran manera la clasificación lograda para el tráfico normal. Veamos la tasa de aciertos y la tasa de errores.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)*100
```

Se obtuvo una tasa de aciertos de 70.30%, es un número bastante bueno, similar al obtenido en el primer nivel. Si se compara con el rendimiento obtenido por el modelo de red neuronal, entonces se obtiene 2% menos, pero en la red neuronal se cometen mayor cantidad de falsos positivos. Ahora veamos la tasa de aciertos por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

Se detecta solo el 11.58% de los ataques presentes, y se clasifica de buena manera el 99.16% del tráfico normal. Si se compara con el el modelo de red neuronal, la red neuronal es mejor detectando ataques pero la máquina de vectores de soporte clasifica mejor el tráfico normal. Ahora veamos las medidas binarias de sensitividad, especificidad y precisión.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

El modelo tiene un desempeño excelente en la detección del tráfico normal, por otra parte, el desempeño a la hora de clasificar los ataques es bastante pobre, pero acierta con alta probabildiad los ataques detectados, por lo tanto no genera muchos falsos positivos. Ahora veamos las estadísticas totales producto de la mezcla de ambos niveles. Empecemos por ver la matriz de confusión.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix, confusion.matrix.kmeans.model)
confusion.matrix.two.labels
```

El resultado total refleja un incremento positivo en la detección de ataques aunque no muy grande. La cantidad de falsos negativos sigue siendo bastante alta, mientras que la cantidad de falsos positivos es bastante baja. Por último, la fortaleza de este modelo es la correcta identificación del tráfico normal, motivo por el cuál existe una gran cantidad de falsos negativos. Ahora veamos la tasa de aciertos y la tasa de errores.

```{r}
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
```

Se logró un incremento del 3% en la tasa de aciertos con respecto al primer nivel de clasificación, una mejora significativa en el área de la detección de intrusos en redes de computadoras. Ahora veamos como quedaron el resto de las medidas de rendimiento concernientes a la sensitividad, especificidad y precisión.

```{r}
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
```

Se nota un incremento con respecto a la sensitividad del 4%, es decir, se detectaron 4% más de los ataques presentes con la inclusión de K-Medias. Por otra parte hubo un decremento de alrededor del 1% con respecto a la especificidad y precisión. En general el modelo híbrido tiene un buen desempeño, los puntos altos son la baja generación de falsos positivos y la alta eficacia en la clasificación del tráfico normal. Los puntos bajos corresponden a la gran cantidad de falsos negativos presentes en las predicciones. Por último, el tiempo total para el entrenamiento y las predicciones se mostrará a continuación respectivamente.

```{r}
training.time + total.time.kmeans.training
total.time.predictions + total.time.kmeans.predictions
```

####Conclusiones
El rendimiento del primer nivel con máquina de vectores de soporte comparado con el rendimiento obtenido en la sección análisis sobre el conjunto de entrenamiento es bastante inferior; sin embargo, es comprensible debido a que en el conjunto de prueba se agregan nuevos tipos de ataques que no estuvieron presentes en el conjunto de entrenamiento. Más allá de eso el rendimiento es bastante bueno con 77% de tasa de aciertos y buenas medidas de rendimiento para la sensitividad, especificidad y precisión. Por otra parte la curva ROC indica que el modelo es bastante variante con respecto a la certeza con la que toma las decisiones, llegando en un punto a pegarse bastante a la línea del azar, situación bastante deteriorada con respecto a la sección de análisis sobre el conjunto de entrenamiento. Como aspecto positivo, el modelo no comete gran cantidad de falsos positivos, pero si una gran cantidad de falsos negativos.

El segundo nivel del modelo, con K-Medias tuvo un mejor desempeño comparado con la sección análisis sobre el conjunto de entrenamiento, esto debido a que el primer nivel correspondiente a la máquina de vectores de soporte tuvo gran cantidad de falsos negativos. La tasa de aciertos del modelo K-Medias fue del 70.30%, un número similar al del primer nivel. Sin embargo, K-Medias solo logró detectar el 11% de los ataques presentes y redujo escasamente la cantidad de falsos negativos. Por otra parte, un aspecto positivo fue la no generación excesiva de falsos positivos.

En conjunto, la inclusión de K-Medias logró un incremento de alrededor del 3% con respecto a los resultados obtenidos en el primer nivel en la tasa de aciertos, llegando así a un 80%. Por otra parte el incremento en la sensitividad fue de sólo el 4% que corresponde a la proporción de los ataques detectados por K-Medias. La especificidad y la precisión se vieron invariantes, decrementando ambas alrededor de 1%. Estas comparaciones fueron realizadas con respecto al desempeño obtenido por el primer nivel, que corresponde al clasificador de máquina de vectores de soporte.

En general el desempeño es bastante bueno, la gran mayoría del tráfico fue clasificado de manera satisfactoria y se produjeron alrededor de 4400 errores en la clasificación, donde 4138 corresponden a falsos negativos de los 22 mil registros presentes en el conjunto de prueba. Para un especialista el hecho de que no haya gran cantidad de falsos positivos es positivo debido a que no tendrá que invertir tiempo revisando registros que no son una amenaza. Sin embargo, la gran cantidad de falsos negativos representan una gran amenaza debido a que los ataques no fueron detectados y además elimina la posibilidad de poder retroalimentar el modelo tomando los falsos positivos y colocándolos como pertenecientes al tráfico normal, para que de esta manera el modelo pueda aumentar su base de conocimientos.

###Conclusiones generales
El modelo de red neuronal es más efectivo a la hora de detectar ataques, adicionalmente se observa mediante la curva ROC que las decisiones tomadas tienen mayor certeza y son más precisas. Por otra parte, el modelo de máquina de vectores de soporte es mejor clasificando el tráfico normal, e incluso individualmente tiene mayor cantidad de tasa de aciertos. También se pudo observar que la máquina de vectores de soporte es más efectiva detectando las clases DoS y normal, mientras que la red neuronal es mejor detectando el resto de las clases concernientes a Probing. R2L y U2R.

La inclusión de K-Medias en los modelos repercutó de manera diferente en ambos modelos. Para la red neuronal logró un incremento notable en la cantidad de ataques detectados, pero incrementó notablemente la cantidad de falsos positivos generados. Por el contrario, para la máquina de vectores de soporte la inclusión de K-Medias fue más conservadora, detectando menor cantidad de ataques pero sin generar exceso de falsos positivos.

Para poder determinar cual modelo es mejor que otro hay que irse por el tema de prioridades. Es decir, ¿Es más importante tener más cantidad de ataques detectados con un mayor número de falsos positivos presentes o es mejor un enfoque más conservador con menor cantidad de ataques detectados pero con menor cantidad de falsos positivos presentes? Particularmente me parece que la red neuronal es mejor debido a que el objetivo es la detección de ataques. Adicionalmente, en este caso los falsos positivos incrementan el trabajo del especialista para examinar los posibles ataques, y en caso de que una no sea correcta, esta puede ser etiquetada y ser usada para la retroalimentación del modelo, es decir, hay una curva de aprendizaje mucho más rápida que en el modelo híbrido de la máquina de vectores de soporte.

Hasta este punto se han usado los parámetros por defecto, queda como tarea pendiente aún realizar la selección de características y la selección de parámetros y analizar el impacto sobre ambos enfoques.

#Selección de características
En esta sección se realizán las actividades concernientes a la selección de características. La idea principal detrás de la reducción de características es la de quitar aquellas variables predictoras que puedan introducir ruido al modelo, adicionalmente al haber menor cantidad de dimensiones el modelo es entrenado de forma más rápida y las predicciones también son hechas con mayor velocidad. El conjunto de datos NSL-KDD quedó con 40 variables predictoras luego de realizar el pre-procesamiento, y en esta sección se reducirá su número y se analizará su impacto para los modelos híbridos basados en red neuronal y en máquina de vectores de soporte. 

Se aplicarán dos métodos para la selección de características. El primer método, que es uno de los más populares y ampliamente usados en el área de aprendizaje automático, que es el Análisis de Componentes Principales (PCA). Con la técnica de PCA se crea un nuevo espacio de variables predictoras basándose en combinaciones lineales entre las mismas. Como ventaja para este enfoque se tiene una manera efectiva de visualizar y obtener aquellas nuevas variables predictoras que acumulan mayor cantidad de varianza. Por otra parte, se pierde interpretabilidad de los datos, debido a que ya no hay variables predictoras con un nombre que se pueda asociar a un evento producido en el ámbito del problema.

Como segunda técnica se usará la Reducción Gradual de Características (GFR) que es una técnica propuesta por Li en su trabajo *An Efficient Intrusion Detection System Based on Support Vector Machines and Gradually Feature Removal Method*. Esta técnica tuvo buenos resultados en dicha publicación. Adicionalmente es sencilla de implementar y de esta manera se puede visualizar cuales son las variables predictoras más importantes ya que en esta se mantiene la interpretabilidad de los datos. Por otra parte, habrá que compararla con PCA para saber cuál de estas tiene mejor desempeño.

Comenzaremos por la implementacion de PCA y posteriormente con GFR.

##PCA
En esta sección se describirán las actividades concernientes a la implementación y análisis de la aplicación de PCA sobre el conjunto de datos NSL-KDD para la reducción de características. Estas actividades corresponden al análisis exploratorio y posteriormente se calculará el error producido por cada uno de los modelos basados en red neuronal y máquina de vectores de soporte.

###Análisis exploratorio
Acá se aplicará PCA sobre el conjunto de datos y se verá con cuántas variales predictoras se acumula una cantidad suficiente de varianza acumulada. También se verá si este número de variables corresponde a una reducción significativa.

Empezaremos las actividades limpiando el ambiente de trabajo, cargando el conjunto de datos de entrenamiento y el archivo de funciones.

```{r}
rm(list = ls())
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)
source("../source/functions/functions.R")
```

Para probar el error de las características se usarán los clasificadores red neuronal y máquina de vectores de soporte. Debido a esto se usaŕan 5 clases objetivo y motivado por esto es necesario eliminar aquellas etiquetas innecesarias, transformar las columnas predictoras a tipo numérico, la columna objetivo a tipo factor y escalar el conjunto de datos para que estos tengan media cero y desviación estándar uno.

```{r}
#Eliminando columnas innecesarias
dataset = dataset.training
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL

#Cambiando el tipo de dato de las columnas
for (i in 1:(ncol(dataset)-1))
  dataset[,i] = as.numeric(dataset[,i])

dataset[,ncol(dataset)] = as.factor(dataset[,ncol(dataset)])

#Escalando las variables predictoras
dataset = ScaleSet(dataset)
```

Ya se tiene el ambiente de trabajo listo, y ahora podemos aplicar PCA.

```{r}
pca = prcomp(dataset[,-41], scale. = TRUE)
```

Se utilizó la función *prcomp* perteneciente a la biblioteca *stats*. se observa que como pará metros se pasaron todas las variables predictoras (se dejó por fuera la variable objetivo), y se pidió que se escalara el conjunto de datos. El esacalamiento de los datos juega un rol fundamental en PCA debido a que las combinaciones lineales ameritan que los valores estén unificados con respecto a su rango para poder tener éxito. De otra manera, las combinaciones lineales podrían no tener sentido. A continuación veamos un resumen del objeto *pca*.

```{r}
summary(pca)
```

Se observa que las componente fueron enumeradas en las columnas de la forma *PCX* donde la *X* corresponde a un número en el rango [1,40] debido a que teníamos 40 variables predictoras inicialmente. Adicionalmente las filas corresponden a tres medidas que son: desviación estándar, que mide la desviación estándar que se logra sin dicha componente. La proporción de varianza dice cual es la varianza lograda por dicha componente individualmente. Por último, la proporción acumulada tiene la sumatoria de todas las proporciones de varianza hasta cierto punto. Es decir, la proporción acumulada hasta la componente principal 3 es la sumatoria de la proporción de varianza desde PC1 hasta PC3.

Las primeras componentes al ser las que mayor cantidad de varianza acumulan son las más relevantes. A continuación colocaremos en un *dataframe* las siguientes medidas: desviación estándar, varianza por componente, porcentaje de varianza acumulada y varianza acumulada.

```{r}
std.deviation = pca$sdev
PC.variance = std.deviation^2
PR.variance = PC.variance/sum(PC.variance)
cum.variance = cumsum(PR.variance) * 100
summary.pca = data.frame(std_deviation = std.deviation,
                         PC_variance = PC.variance,
                         PR_variance = PR.variance,
                         cum_variance = cum.variance)
summary.pca
```

De esta manera podemos ver por lo menos en la primera fila que la componente número 1 tiene una desviación estándar de 2.78, una varianza de 7.75, un *orcentaje de varianza acumulada de 19.38% y una varianza acumulada de 19.38%. Anteriormente se mencionó que la selección de las componente principales debería tener una varianza acumulada de alredor 95%, este número se alcanza con 24 componentes. Lo que nos dice que teóricamente con 24 variables predictoras de nuestras componentes principales se puede tener una buena selección de características. Ahora grafiquemos la varianza acumulada en función del número de componentes, de esta manera, se puede tener una vista gráfica de a partir de cual cantidad de componentes principales la varianza se estabiliza.

```{r, fig.align="center"}
plot(summary.pca$cum_variance,
     ylab = "Proporción Acumulada",
     xlab = "Número de Componentes Principales",
     type = "b", col = "blue")
```

Se observa que a partir de aproximadamente 24 componentes, la varianza acumulada crece de manera bastante lenta y aparentemente se estabiliza en ese punto. Dicho esto, 24 debería ser un buen número de variables predictoras a usar, reduciendo así en 16 variable predictoras la dimensionalidad del conjunto de datos.

Se unificará el nuevo espacio de variables predictoras con las etiquetas correspondientes a cada registro.

```{r}
dataset.pca = as.data.frame(pca$x)
dataset.pca = data.frame(dataset.pca,
                         Label = dataset$Label)
```

Para terminar con el análisis exploratorio se graficarán las primeras dos componentes principales para visualizar si existe una separación notable entre los registros clasificados como ataques y los clasificados como tráfico normal.


```{r, fig.align= "center"}
colors = as.character(dataset.pca[,ncol(dataset.pca)])
colors[colors == "normal"] = "black"
colors[colors == "DoS"] = "red"
colors[colors == "Probing"] = "green"
colors[colors == "R2L"] = "blue"
colors[colors == "U2R"] = "magenta"

plot(x = dataset.pca[,1],  y = dataset.pca[,2], col = colors,
     main = "Gráfico de las Dos Componentes Principales",
     xlab = "Componente 1", ylab = "Componente 2", pch = 19)

legend("bottomleft", legend = c("Normal", "DoS", "Probing", "R2L", "U2R"),
       col = c("black","red", "green", "blue", "magenta"), pch = 19)

```

En la gráfica se observa como los ataques DoS y Probing en su mayoría poseen una separación bastante marcada con respecto al tráfico normal, adicionalmente se observa que estas fronteras poseen una forma no lineal. Los ataques R2L y U2R están escondidos dentro del conglomerado de puntos del tráfico normal, situación que posiblemente conlleve a que esta clase de ataques sea más difícil de detectar que las clases DoS y Probing.

Adicionalmente, como se usará validación cruzada de 10 conjuntos, se dividirá el conjunto de datos en 10 subconjuntos de manera estratificada.

```{r, eval=FALSE}
cv.sets = CVSet(dataset.pca, k = 10, seed = 22)
```

###Efecto de PCA sobre SVM
En esta sección se evaluará el efecto de la aplicación de PCA sobre SVM. Para esto se entrenarán modelos haciendo uso desde [1,40] variables predictoras y se calculará la tasa de aciertos en cada iteración. Para validar el modelo se hará uso de la técnica de validación de modelos de validación cruzada de 10 conjuntos. Esta sección se dividirá en dos partes: entrenamiento y análisis.

####Entrenamiento
A continuación se describen todas las tareas realizadas en el proceso de entrenamiento de los modelos en el rango de [1,40] variables predictoras. Cabe destacar que se hará uso de las varibles parciales utilizadas en la sección de PCA, ya que son necesarias para la elaboración de los diferentes modelos.

Empezaremos por crear una matriz para almacenar los resultados de cada iteración. La matriz será de 40x10, donde las 40 filas corresponden al número de componentes y las 10 columnas a cada iteración corresponden a una iteración en el proceso de validación cruzada. Con esta matriz luego se pueden calcular medidas como la media por componente y la desviación estándar o varianza dentro de cada componente.

```{r, eval=FALSE}
results = matrix(nrow = 40, ncol = 10)
```

El siguiente segmento de código es el encargado de ejecutar las 400 iteraciones correspondientes al entrenamiendo de los modelos de SVM utilizando validación cruzada de 10 conjuntos.

```{r, eval=FALSE}
for (i in 1:40)
{
  results.cv = vector(mode = "numeric", length = 10)
  
  for (j in 1:10)
  {
    data.cv.testing = cv.sets[[j]]
    data.cv.training = cv.sets
    data.cv.training[[j]] = NULL
    data.cv.testing = as.data.frame(data.cv.testing)
    data.cv.training = do.call(rbind, data.cv.training)
    
    data.training.pca = as.data.frame(data.cv.training[,1:i])
    colnames(data.training.pca) = names(data.cv.training)[1:i]
    data.training.pca = data.frame(data.training.pca,
                                   Label = data.cv.training$Label)
    
    data.testing.pca = as.data.frame(data.cv.testing[,1:i])
    colnames(data.testing.pca) = names(data.cv.testing)[1:i]
    data.testing.pca = data.frame(data.testing.pca,
                                     Label = data.cv.testing$Label)
    
    model = svm(Label ~ .,
                           data = data.training.pca,
                           kernel = "radial",
                           scale = FALSE)
    
    if(i==1)
      prediction = predict(model, data.frame(PC1 = data.testing.pca[,1]), type = "class")
    else
      prediction = predict(model, data.testing.pca[,1:i], type = "class")
    
    results.cv[j] = mean(prediction == data.testing.pca[,ncol(data.testing.pca)])
  }
  
  results[i,] = results.cv
  cat(i, " ")
}
```

Una vez que se acaba el proceso, la matriz es exportada como un objeto para su posterior análisis. Esto es debido a que el proceso para el entrenamiento de los 400 modelos llevó alrededor de 16 horas, y es tedioso tener que esperar todo ese tiempo cada vez que se quiera analizar los resultados.

```{r, eval=FALSE}
saveRDS(results, file = "../source/feature_selection/SVM/results_PCA.rds")
```

####Análisis
En esta sección se realizará el análisis de los resultados obtenidos en la fase de entrenamiento. Se cargarán los resultados, se calcularán la desviación estándar y la media de los resultados por cada componente y se graficarán para poder decidir un buen número de componentes a elegir para nuestro modelo definitivo.

Se empezarán las tareas preparando el ambiente de trabajo, esto incluye la eliminación de variables parciales y la carga del objeto de resultados de la sección anterior.

```{r}
rm(list = ls())
results  = readRDS("../source/feature_selection/SVM/results_PCA.rds")
```

En la variable *results* se tiene una matriz con los resultados de la eficacia producto de la validación cruzada sobre la combinación de componentes principales en el intérvalo [1,40]. A continuación se crearán dos vectores en los cuales se almacenarán los resultados producto del cálculo de la desviación estándar y la media de la eficacia por cada una de las componentes.

```{r}
sd.results = apply(results, 1, sd)
mean.results = apply(results, 1, mean)
```

Para seleccionar el número de componentes se usará un criterio similar al del codo de jambu. Es decir, se buscará el punto donde la eficacia empieza a suavizarse conforme el número de componentes principales son agregadas, adicionalmente, se verificará que la desviación estándar sea poca para dicho número de componentes.

```{r, fig.align = "center"}
#Dividiendo la pantalla en dos columnas
par(mfrow = c(1,2))

#Graficando Desviación Estándar vs Número de Componentes
plot(sd.results, col = "blue", type = "b",
     main = "D. Estándar vs # Componentes",
     xlab = "Número de Componentes", ylab = "Desviación Estándar",
     log = "y")

#Graficando Media de Eficacia vs Número de Componentes
plot(mean.results, col = "blue", type = "b",
     main = "Media vs # Componentes",
     xlab = "Número de Componentes", ylab = "Media Acierto")
```

En las gráficas se observa que con 7 componentes principales que logra una tasa de aciertos de alrededor de 99%. A partir de ese punto la mejora obtenida es mínima. Adicionalmente, se observa que la desviación estándar para dicho número de componentes principales es bastante bajo.

Por lo anterior, se puede pensar que con 7 componentes principales se lograría una buena tasa de aciertos en la detección de intrusos y se reduciría la dimensionalidad del conjunto de datos en un 82.5%.

###Efecto de PCA sobre NN
En esta sección se evaluará el efecto de la aplicación de PCA sobre NN. Para esto se entrenarán modelos haciendo uso desde [1,40] variables predictoras y se calculará la tasa de aciertos en cada iteración. Para validar el modelo se hará uso de la técnica de validación modelos de validación cruzada de 10 conjuntos. Esta sección se dividirá en dos partes: entrenamiento y análisis.

####Entrenamiento
A continuación se describren todas las tareas realizadas en el proceso de entrenamiento de los modelos en el rango [1,40] variables predictoras. Cabe destacar que se hará uso de las variables parciales utilizadas en la sección de PCA, ya que son necesarias para la elaboración de los diferentes modelos.

Empezaremos por crear una matriz para almacenar los resultados de cada iteración. La matriz será de 40x10, donde las 40 filas corresponden al número de componentes y las 10 columnas a cada iteración durante el proceso de validación cruzada. Con esta matriz luego se pueden calcular medidas como la media, la desviación estándar o varianza por número de componentes.

```{r, eval = FALSE}
results = matrix(nrow = 40, ncol = 10)
```

El siguiente segmento de código es el encargado de ejecutar las 400 iteraciones correspondientes al entrenamiento de los modelos de NN utilizando validación cruzada de 10 conjuntos.

```{r, eval=FALSE}
for (i in 1:40)
{
  results.cv = vector(mode = "numeric", length = 10)
  
  for (j in 1:10)
  {
    data.cv.testing = cv.sets[[j]]
    data.cv.training = cv.sets
    data.cv.training[[j]] = NULL
    data.cv.testing = as.data.frame(data.cv.testing)
    data.cv.training = do.call(rbind, data.cv.training)
    
    data.training.pca = as.data.frame(data.cv.training[,1:i])
    colnames(data.training.pca) = names(data.cv.training)[1:i]
    data.training.pca = data.frame(data.training.pca,
                                   Label = data.cv.training$Label)
    
    data.testing.pca = as.data.frame(data.cv.testing[,1:i])
    colnames(data.testing.pca) = names(data.cv.testing)[1:i]
    data.testing.pca = data.frame(data.testing.pca,
                                  Label = data.cv.testing$Label)
    
    model = nnet(Label ~ .,
                 data = data.training.pca,
                 size = 20,
                 maxit = 100)
    
    if(i==1)
      prediction = predict(model, data.frame(PC1 = data.testing.pca[,1]), type = "class")
    else
      prediction = predict(model, data.testing.pca[,1:i], type = "class")
    
    results.cv[j] = mean(prediction == data.testing.pca[,ncol(data.testing.pca)])
  }
  
  results[i,] = results.cv
  cat(i, " ")
}
```

Una vez que se acaba el proceso, la matriz es exportada como un objeto para su posterior análisis. Esto es debiado a que el proceso de entrenamiento para los 400 modelos llevó alrededor de 13 horas, y es tedioso tener que esperar todo ese tiempo cada vez que se quieran analizar los resultados.

```{r, eval=FALSE}
saveRDS(results, file = "../source/feature_selection/NN/results_PCA.rds")
```

####Análisis
En esta sección se realizará el análisis de los resultados obtenidos en la fase de entrenamiento. Se cargarán los resultados, se calculará la desviación estándar y la media de los resultados por cada número de componentes y se graficarán para poder decidir un buen número de componentes a elegir para nuestro modelo definitivo.

Se empezarán las tareas preparando el ambiente de trabajo, esto incluye la eliminación de variables parciales y la carga del objeto de los resultados de la sección anterior.

```{r}
rm(list = ls())
results = readRDS("../source/feature_selection/NN/results_PCA.rds")
```

En la variable *results* se tiene una matriz con los resultados de la eficacia producto de la aplicación de validación cruzada sobre la combinación de componentes principales en el intérvalo [1,40]. A continuación se crearán dos vectores en los cuales se almacenarán los resultados producto del cálculo de la desviación estándar y la media de la eficacia por cada una de las componentes.

```{r}
sd.results = apply(results, 1, sd)
mean.results = apply(results, 1, mean)
```

Para seleccionar el número de componentes se usará un criterio similar al del codo de jambu. Es decir, se buscará el punto donde la eficacia empieza a suavizarse conforme el número de componentes principales son agregadas. Adicionalmente, se verificará que la desviación estándar sea poca para dicho número de componentes.

```{r, fig.align="center"}
#Dividiendo la pantalla en dos columnas
par(mfrow = c(1,2))
#Graficando Desviación Estándar vs Número de Componentes
plot(sd.results, col = "blue", type = "b",
main = "D. Estándar vs # Componentes",
xlab = "Número de Componentes", ylab = "Desviación Estándar",
log = "y")
#Graficando Media de Eficacia vs Número de Componentes
plot(mean.results, col = "blue", type = "b",
main = "Media vs # Componentes",
xlab = "Número de Componentes", ylab = "Media Acierto")
```

En las gráficas se observa que con 7 componentes principales se logra una tasa de aciertos de alrededor de 99%. A partir de ese punto, la mejora obtenida es mínima. Adicionalmente, se observa que la desviación estándar para dicho número de componentes principales es bastante bajo.

Por lo anterior se puede pensar que con 7 componentes principales se lograría una buena tasa de aciertos en la detección de intrusos, y se reduciría la dimensionalidad del conjunto de datos en un 82.5%.

###Conclusión
Una buena medida para seleccionar el número de componentes principales según Andrew Ng experto en el área de aprendizaje automático es elegir el número de componentes principales que logren capturar varianza en el rango [95%, 99%]. En el análisis exploratorio se observó que la medida de 95% es alcanzada con el uso de 24 componentes principales. Sin embargo, en la las secciones donde se realizó el análisis de PCA sobre SVM y NN, se puede notar que con 7 componentes principales se logra un excelente rendimiento reduciendo en 82.5% la dimensionalidad del conjunto de datos. La fase de análisis fue realizada haciendo uso de la técnica de validación de modelos de validación cruzada de 10 conjuntos y quedaría por ver el rendimiento de estos algoritmos utilizando el conjunto de pruebas para medir la eficacia de los mismos.

##GFR
En esta sección se describen las actividades concernientes a la implementación y análisis de GFR sobre el conjunto de datos NSL-KDD para la reduccción de características. Estas actividades corresponden al análisis de los resultados obtenidos para los modelos de red neuronal y máquina de vectores de soporte.

###Efecto de GFR sobre SVM
En esta sección se evaluará el efecto de la aplicación de GFR sobre SVM. Para esto se entrenarán modelos haciendo uso desde [1,40] variable predictoras y se calculará la tasa de aciertos en cada iteración. Para validar el modelo se hará uso de la técnica de validación de modelos de validación cruzada de 10 conjuntos. Esta sección se divide en dos partes: entrenamiento y análisis.

####Entrenamiento
Acá se describen todas las tareas realizadas en el proceso de entrenamiento de los modelos en el rango [1,40] variables predictoras. Se iniciará preparando el ambiente de trabajo: limpiar variables parciales, cargar paquetes necesarios y archivo de funciones.

```{r, eval=FALSE}
rm(list = ls())
require("e1071")
source("functions/functions.R")
```

Se eliminan las etiquetas a no ser usadas.

```{r, eval=FALSE}
dataset = dataset.training
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Se transforman las variables predictoras a tipo *numérico*.

```{r, eval=FALSE}
for (i in 1:(ncol(dataset)-1))
  dataset[,i] = as.numeric(dataset[,i])
```

Se transforma la variable objetivo a tipo *factor* esto es debido a que se realizarán labores de clasificación. Es importarnte recordar que la variable objetivo posee cinco clases: DoS, normal, Probing, R2L y U2R.

```{r, eval= FALSE}
dataset[,ncol(dataset)] = as.factor(dataset[,ncol(dataset)])
```

Se escala el conjunto de datos para que todas las variables predictoras tengan media cero y desviación estándar uno.

```{r, eval=FALSE}
dataset = ScaleSet(dataset)
```

En este punto se tiene todo listo para aplicar el algoritmo GFR, este fue comprimido en la función GFR que recibe como parámetros un *dataframe* y el tipo de algoritmo que se desea usar SVM o NN.

```{r, eval=FALSE}
results = GFR(dataset, "NN")
```

*GFR* retorna una matriz de dimensiones 41x10 donde 41 son la cantidad de resultados por característica eliminada y 10 son los resultados obtenidos por iteración durante el proceso de validación cruzada de 10 conjuntos. Por último, dicha matriz será almacenada en un objeto para su posterior análisis. El tiempo de entrenamiento de este método fue de 12 días, un tiempo bastante elevado y más si se compara con el método de reducción de características PCA.

```{r, eval=FALSE}
saveRDS(results, "../source/feature_selection/SVM/results_GFR.rds")
```


####Análisis
En esta sección se describen las actividades realizadas para la fase de análisis. Esta empezará limpiando el ambiente de trabajo de variables parciales y cargando el archivo de funciones.

```{r}
rm(list = ls())
source("../source/functions/functions.R")
```

A continuacióm se cargará el objeto con los resultados obtenidos en la sección anterior.

```{r}
svm.gfr = readRDS("../source/feature_selection/SVM/results_GFR.rds")
```

Las características más importantes extraídas en el proceso anterior se ilustran a continuación en orden descendente de importancia. Es decir, la primera representa la más importante y las siguientes implican menor importancia.

```{r}
rownames(svm.gfr)[-nrow(svm.gfr)]
```

Veamos si con las primeras dos variables se puede determinar gráficamente alguna separación notable con respecto al tráfico normal o a los ataques. Para ello primero debemos cargar el conjutno de datos de entrenamietno y eliminar las columnas de etiquetas que no se utilizarán.

```{r}
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)

#Eliminando características innecesarias
dataset = dataset.training
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Adicionalmente se asegurará que las columnas variables predictoras sean de tipo numérico.

```{r}
for (i in 1:(ncol(dataset)-1))
  dataset[,i] = as.numeric(dataset[,i])
```

Luego es necesario crear un vector de colores para poder diferenciar entre las diferentes clases en el gráfico.

```{r}
colors = as.character(dataset[,ncol(dataset)])
colors[colors == "normal"] = "black"
colors[colors == "DoS"] = "red"
colors[colors == "Probing"] = "green"
colors[colors == "R2L"] = "blue"
colors[colors == "U2R"] = "magenta"
```

Donde negro corresponde a la clase normal, rojo a ataques DoS, verde a ataques Probing, azúl a ataques R2L y magenta a ataques U2R. El gráfico de las dos primeras características más importantes se muestra a continuación.

```{r, fig.align="center"}
par(mfrow = c(1,1))
plot(dataset[, rownames(svm.gfr)[1]], dataset[, rownames(svm.gfr)[2]],
     col = dataset$Label, pch = 19,
     xlab = "Flag", ylab = "Count",
     main = "Principales Características GFR - SVM")
```

En la gráfica anterior se pueden observar patrones con respecto a los valores de las características que pueden ayudar a la separación de las clases, por ejemplo, si *Flag* = 2 y *Count* >= 300, se puede decir que dicho registro pertenece a un a ataque Probing. Sin embargo, SVM no funciona de esta manera (estableciendo reglas), a su vez, funciona delimitando fronteras. Esto puede presentar una situación a analizar para la decisión del uso de SVM o de NN. Por ejemplo, ya que NN si es capaz de establecer reglas como las mencionadas anteriormente.

Ahora se procede a realizar los pasos para el análisis de las características a elegir de forma definitiva. Inicialmente, se calculan y la media y desviación estándar.

```{r}
mean.values = apply(svm.gfr, 1, mean)
sdeviation.values = apply(svm.gfr, 1, sd)
```

Y se grafican las medidas calculadas previamente.

```{r, fig.align="center"}
par(mfrow = c(1,2))
plot(sdeviation.values[2:length(mean.values)],
     type = "b", col = "blue",
     main = "D. Estándar vs # Componentes",
     xlab = "Número de Componentes", ylab = "Desviación Estándar",
     log = "y")
plot(mean.values[2:length(mean.values)],
     type = "b", col = "blue",
     main = "Medias vs # Componentes",
     xlab = "Número de Componentes", ylab = "Media Acierto")
```

Para la selección de características en esta sección se usa un criterio similar al de codo de jambu. Acá se observá que la articulación se logra con 9 ó 10 características. Como se observa en el gráfico de la desviación estándar, los resultados con 9 variables son más estables que con 10 variables y por dicho motivo se seleccionará dicho número como cantidad de variables ideal para el modelo de SVM. Las mismas se listan a continuación.

```{r}
rownames(svm.gfr)[1:9]
```

Luego, se puede observar con con la selección de 9 variables se lograría reducir la dimensionalidad del conjunto de datos en un 77.5%.

###Efecto de GFR sobre NN
En esta sección se evalua el efecto de la aplicación de GFR sobre SVM. Para esto se entrenan modelos haciendo uso de [1,40] variables predictoras y se calculan las tasa se aciertos en cada iteración. Para validar el modelo se hará uso de la técnica de validación cruzada de 10 conjuntos. Esta sección está dividida en dos partes: entrenamiento y análisis.

####Entrenamiento
Acá se describen todas las tareas realizadas en el proceso de entrenamiento de los modelos en el rango [1,40] variables predictoras. Se iniciará prepandado el ambiente de trabajo: limpiar variables parciales, cargar paquetes necesarios y archivos de funciones.

```{r, eval=FALSE}
rm(list = ls())
require("nnet")
source("functions/functions.R")
```

Se eliminan las etiquetas que no serán usadas.

```{r, eval=FALSE}
dataset = dataset.training
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Se transforman las variables predictoras a tipo numérico.

```{r, eval=FALSE}
for (i in 1:(ncol(dataset)-1))
  dataset[,i] = as.numeric(dataset[,i])
```

Se transforma la variabe objetivo a tipo *factor*, esto debido a que se realizarán labores de clasificación. Es importante recordar que la variable objetivo posee cinco clases: DoS, normal, Probing, R2L y U2R.

```{r, eval=FALSE}
dataset[,ncol(dataset)] = as.factor(dataset[,ncol(dataset)])
```

Se escala el conjunto de datos para que todas las variables predictoras tengan media cero y desviación estándar uno.

```{r, eval=FALSE}
dataset = ScaleSet(dataset)
```

En este punto se tiene todo listo para aplicar el algoritmo GFR, este fue comprimido en la función *GFR* que recibe como parámetros un *dataframe* y el tipo de algoritmo que se desea usar SVM o NN.

```{r, eval=FALSE}
results = GFR(dataset, "SVM")
```

GFR retorna una matriz de dimensiones 41x10, donde 41 corresponde a la cantidad de resultados por característica eliminada y 10 son los resultados obtenidos por iteración durante el proceso de validación cruzada de 10 conjuntos. Por último, la matriz será almacenada en un objeto para su posterior análisis. Esto debido a que este proceso duró 12 días para su culminación.

```{r, eval=FALSE}
saveRDS(results, "../source/feature_selection/NN/results_GFR.rds")
```

####Análisis
Esta sección describe las actividades realizadas para la fase de análisis. Estas iniciarán limpiando el ambiente de trabajo de variables parciales y cargando el archivo de funciones.

```{r}
rm(list = ls())
source("../source/functions/functions.R")
```

A continuación se cargará el objeto con los resultados obtenidos en la sección anterior.

```{r}
nn.gfr = readRDS("../source/feature_selection/NN/results_GFR.rds")
```

A continuación se ilustran los resultados de las características obtenidas en la sección anterior. Las mismas están ordenadas de manera descendente. Es decir, las primeras posiciones representan las más importantes.

```{r}
rownames(nn.gfr)[-nrow(nn.gfr)]
```

Veamos si con las primeras dos variables má simportantes se puede determinar gráficamente alguna separación notable con respecto al trafico normal o a los ataques. Para ello primero debemos cargar el conjunto de datos de entrenamiento y eliminar las columnas de etiquetas que no se utilizarán.

```{r}
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)
#Eliminando características innecesarias
dataset = dataset.training
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Adicionalmente se asegurará que las columnas de variables predictoras sean de tipo numérico.

```{r}
for (i in 1:(ncol(dataset)-1))
  dataset[,i] = as.numeric(dataset[,i])
```

Luego, es necesario crear un vector de colores para poder diferencias las diferentes clases en el gráfico.

```{r}
colors = as.character(dataset[,ncol(dataset)])
colors[colors == "normal"] = "black"
colors[colors == "DoS"] = "red"
colors[colors == "Probing"] = "green"
colors[colors == "R2L"] = "blue"
colors[colors == "U2R"] = "magenta"
```

Donde negro corresponde a la clase normal, rojo a ataques DoS, verde a ataques Probing, azúl a ataques R2L y magenta a ataques U2R. El gráfico de las dos primeras características más importantes se muestra a continuación.

```{r, fig.align="center"}
par(mfrow = c(1,1))
plot(dataset[, rownames(nn.gfr)[1]], dataset[, rownames(nn.gfr)[2]],
     col = dataset$Label, pch = 19,
     xlab = "Count", ylab = "Protocol Type",
     main = "Principales Características GFR - NN")
```

En el gráfico se observan patrones que pueden separar notablemente a las diferentes clases. Por ejemplo, si Count > 50 y Protocol Type = 1, entonces el tráfico es normal. El gráfico muestra separaciones bastante ordenadas y esto ayuda mucho a la forma en la que se comportan las redes neuronales, ya que este algoritmo busca patrones analíticos en los datos que ayuden a la clasificación.

Ahora se procede a realizar los pasos para el análisis de las características a elegir de forma definitiva. inicialmente, se calculan la media y desviación estándar.

```{r}
mean.values = apply(nn.gfr, 1, mean)
sdeviation.values = apply(nn.gfr, 1, sd)
```

Y se grafican las medidas calculadas previamente.

```{r, fig.align="center"}
par(mfrow = c(1,2))
plot(sdeviation.values[2:length(mean.values)],
     type = "b", col = "blue",
     main = "D. Estándar vs # Componentes",
     xlab = "Número de Componentes", ylab = "Desviación Estándar",
     log = "y")
plot(mean.values[2:length(mean.values)],
     type = "b", col = "blue",
     main = "Media vs # Componentes",
     xlab = "Número de Componentes", ylab = "Media Acierto")
```

Para la selección de características en esta sección se usa un criterio similar al de codo de jambu. Acá se observa que la articulación se logar con 9 ó 10 características. En el gráfico de desviación estándar, los resultados con 9 variables son más estables que con 10 variables y por dicho motivo se seleccionará dicho número como cantidad de variables ideal para el modelo de NN. Las mismas se listan a continuación.

```{r}
rownames(nn.gfr)[1:9]
```

Luego, se observa que con la selección de 9 variables se logra reducir la dimensionalidad del conjunto de datos en un 77.5%.

###Conclusión

Con GFR se muestra que se puede obtener bueno resultados para ambos modelos (SVM y NN) usando el 22.5% de las variables totales. Se observó también que la desviación estándar de los resultados en ambos casos es pequeña, por no decir mínima, y esto es un hecho que respalda positivamente a los resultados obtenidos, indicando que la reducción de dimensionalidad para este escenario es válido y efectivo. Como aspecto a destacar está el orden de las características seleccionadas es diferente con respecto a los modelos de SVM y NN y esto tiene su naturaleza en que el algoritmo de SVM tiene una orientación descriptiva y busca aquellas características que posicionalmente dividan mejor a las clases en un espacio geométrico N-Dimensional. Por otra parte, el algoritmo de NN tiene un enfoque analítico que busca patrones en las diferentes características que sirvan como premisa para poder separar las diferentes clases. Es por esto, que se obtuvieron diferentes resultados en los diferentes modelos. Sin embargo, se puede destacar de acá que la unión de las primeras 9 características seleccionadas para ambos modelos son fuertes candidatas a variables predictoras a tomar en cuenta para la detección de intrusos en redes de computadoras.

Se puede hacer una unión entre las características seleccionadas por SVM y NN para así tener las características más importantes.

Eliminaremos las variables parciales por defecto y cargaremos los resultados de la selección de características para GFR.

```{r}
#Eliminando variables parciales
rm(list = ls())

#Cargando características de SVM y NN
svm.gfr = readRDS("../source/feature_selection/SVM/results_GFR.rds")
nn.gfr = readRDS("../source/feature_selection/NN/results_GFR.rds")
```

Previamente se mencionó que se usarían las primeras 9 variables de mayor importancia.

```{r}
svm.gfr = rownames(svm.gfr)[1:9]
nn.gfr = rownames(nn.gfr)[1:9]
```

Al hacer unión entre estos dos vectores se puede observar como existen 12 variables que los modelos sugieren que deben estar presentes para el análisis de anomalías en redes de computadoras.

```{r}
union(nn.gfr, svm.gfr)
length(union(nn.gfr, svm.gfr))
```

##Análisis entre PCA y GFR

Una vez realizadas las pruebas concernientes se puede observar que desde un punto de vista de rendimiento, PCA alcanza el 99% de precisión con 7 variables mientras que GFR alcanza 99% con 9 variables. Adicionalmente, PCA tardó a penas 13 horas para poder realizar la selección de componente principales, mientras que con GFR el proceso duró 12 días. Sin embargo, con GFR se pudo mantener el nombre de las variables importantes y no se perdió interpretación como si sucedió con la aplicación de PCA.

#Selección de parámetros
En esta sección se detallan las actividades realizadas para la selección de parámetros y características. Esta sección abarca la selección de parámetros para los conjuntos de datos producto de PCA, GFR y el conjunto de datos original. Por lo mismo, esta sección será divida en conjunto de datos original y conjunto de datos reducido. La mención del conjunto de datos en esta sección hace referencia al conjunto de datos de entrenamiento NSL-KDD.

Para la selección de los parámetros se hará uso de la técnica de validación cruzada de 10 conjuntos para de esta manera poder comparar el rendimiento de los modelos de la manera correcta.

##Conjunto de datos original
Acá se utilizará el conjunto de datos original. Es decir, se utilizarán todas las características presentes en el conjunto de datos de entrenamiento NSL-KDD.

###SVM
En esta sección se hará la selección de parámetros para el algoritmo SVM con kernel radial. La biblioteca *e1071* es la interfaz de R para la biblioteca *LibSVM*, que corresponde a la biblioteca más utilizada para la utilización de SVM en todos los lenguajes de programación.

El kernel radial de SVM consta de dos parámetros ajustables. Estos son: *gamma* y *cost*. El parámetro *gamma* determinará lo amplio de las zonas de los vectores de soporte y el parámetro *cost* determinará la tolerancia o margen de error que se permite para la clasificación. Los parámetros por defecto de estos valores son: *gamma* = $\frac{1}{\# Características}$ y cost = 1.

El conjunto de datos NSL-KDD luego del pre-procesamiento realizado quedó con 40 variables predictoras, lo que hace que el parámetro por defecto de *gamma* sea $\frac{1}{40} = 0.025$. Thaseen en una de sus publicaciones realizó el tunning de los parámetros y dio con que 0.07 es el valor óptimo de *gamma* y 10 el valor para el parámetro *cost*.

A continuación se realizará el ajuste d elos parámetros de SVM para el conjunto de entrenamiento original colocando los valores de *gamma*: {0.01, 0.025, 0.03, 0.04, 0.05, 0.07, 0.08} y de *cost*: {1, 2, 3, 4, 5, 6}. Se hará uso de la función *tune* quien por defecto hace uso de la técnica de validación cruzada de 10 conjuntos.

Empezaremos por preparar el ambiente de trabajo eliminando variables parciales, cargando el archivo de funciones y la biblioteca a utilizar *e1071*.

```{r, eval=FALSE}
rm(list = ls())
source("../source/functions/functions.R")
require(e1071)
```

Posteriormente, cargaremos el conjunto original de entrenamiento y eliminaremos la etiquetas que no serán utilizadas. Recordando que la variable objetivo corresponde a cinco niveles: DoS, normal, Probing, R2L y U2R.

```{r, eval=FALSE}
dataset = read.csv("../dataset/NSLKDD_Training_New.csv")

#Eliminando etiquetas innecesarias
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

A continuación se transformarán las variables predictoras a tipo numérico y se escalará el conjunto de datos para que cada una de las variables predictoras tenga media cero y desviación estándar uno.

```{r, eval = FALSE}
#Extrayendo las etiquetas
names = colnames(dataset)
Label = dataset$Label_Normal_ClassAttack

#Transformando las variables predictoras a tipo numérico
dataset = as.data.frame(apply(dataset[,-ncol(dataset)], 2, as.numeric))
dataset[,ncol(dataset)+1] = Label
colnames(dataset) = names

#Elminando variables parciales
remove(list = c("names", "Label"))

#Escalando el conjunto de datos
dataset = ScaleSet(dataset)
```

El siguiente paso pasa por establecer una semilla y hacer el ajuste de los parámetros en los rangos mencionados. Es importante destacar que este proceso es bastante largo y los resultados serán exportados a un objeto para su posterior análisis.

```{r, eval=FALSE}
set.seed(22)
tuned.model = tune(svm,
                   Label ~.,
                   data = dataset,
                   scale = F,
                   kernel = "radial",
                   ranges = list(cost = c(1, 2, 3, 4, 5, 6),
                                 gamma = c(0.01, 0.025, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08)
                                 )
                   )
#Guardando los resultados
saveRDS(tuned.model, "../source/parameter_selection/SVM/original_set/tuned_model.rds")
```

Una vez que el proceso terminó podemos ver el resumen de los resultados obtenidos cargando el objeto y visualizar los resultados y el mejor modelo obtenido.

```{r}
tuned.model = readRDS("../source/parameter_selection/SVM/original_set/tuned_model.rds")
tuned.model$performances
tuned.model$best.parameters
```

Se puede observar que los mejores parámetros fueron *cost* = 6 y *gamma* = 0.08. Estos resultados dieron una tasa de aciertos de 99.61975%.

```{r}
100 - (0.003802480 * 100)
```

Adicionalmente con estos parámetros se obtuvieron medidas de dispersión de resultados durante el proceso de validación cruzada de 10 conjuntos bastante bajas, lo que es señal de que ese resultado está bastante bien aproximado. Por último, se puede observar como Thaseen tenía razón al decir que los parámetros por defecto no eran los idóneos.

###NN
En esta sección se hará la selección de parámetros para el algoritmo de NN haciendo uso del conjunto de entrenamiento original de NSL-KDD. Se utilizará el paquete *nnet* que permite crear arquitecturas de NN con una sola capa intermedia. Este aspecto no es una limitación debido a que se pudo observar en secciones previas como el modelo de NN se comporta de muy buena manera con una arquitectura 40-20-5. Como las neuronas de entrada y se salida son determinadas por la cantidad de variables predictoras y clases objetivo respectivamente, el único nivel de la NN que se puede ajustar es la cantidad de neuronas de la capa intrmedia.

El rango seleccionado para el ajuste de neuronas en la capa intermedia será [17, 21]. Esto es debido a que el paquete *nnet* tiene un límite de pesos por defecto debido a que con ese número se alcanza un tiempo de ejecución bastante alto  y ya superar esa cantidad de pesos en el modelo tardaría una cantidad de tiempo excesiva. Para hacer el ajuste de parámetros se hará uso de la función *tune.nnet* del paquete *e1071*. Este paquete hace uso de la *nnet* para hacer el entrenamiento de los parámetros y selecciona el mejor modelo haciendo uso de validación cruzada de 10 conjuntos por defecto.

Se empezará con la preparación del ambiente de trabajo eliminando variables parciales, cargando los paquetes correspondientes y el archivo de funciones.

```{r, eval=FALSE}
rm(list = ls())
source("source/functions/functions.R")
require(e1071)
require(nnet)
```

Como se mencionó previamente, se utilizará el conjunto de datos de entrenamiento NSL-KDD preprocesado y que contiene 40 variables predictoras. El mismo será cargado y se le serán eliminadas aquellas etiquetas que no serán utilizadas puesto que se utilizarán 5 clases objetivo, las cuales son: DoS, normal, Probing, R2L y U2R.

```{r, eval=FALSE}
dataset = read.csv("../dataset/NSLKDD_Training_New.csv")

#Eliminando etiquetas innecesarias
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Se deben transformas las variables predictoras a tipo númerico y escalar el conjunto de datos para que todas las variables predictoras tengan media 0 y desviación estándar uno.

```{r, eval=FALSE}
#Extrayendo información
names = colnames(dataset)
Label = dataset$Label_Normal_ClassAttack

#Transformando variables predictoras a tipo numérico
dataset = as.data.frame(apply(dataset[,-ncol(dataset)], 2, as.numeric))
dataset[,ncol(dataset)+1] = Label
colnames(dataset) = names

#Eliminando variables parciales
remove(list = c("names", "Label"))

#Escalando el conjunto de datos
dataset = ScaleSet(dataset)
```

Por último se hará el ajuste de los parámetros estableciendo el rango de las neuronas de la capa intermedia en el rango [17,21]. Se establecerá una semilla para que esta corrida sea reproducible. Adicionalmente, este proceso dura algunas horas y es por ello que el resultado será exportado a un objeto para su posterior carga y análisis.

```{r, eval=FALSE}
set.seed(22)
tuned.model = tune.nnet(Label ~.,
                        data = dataset,
                        size = 17:21,
                        maxit = 100)

#Guardando los resultados
saveRDS(tuned.model, "source/parameter_selection/NN/original_set/tuned_model.rds")
```

Una vez que se tienen los resultados es posible cargar el objeto almacena y visualizar los resultados obtenidos.

```{r}
tuned.model = readRDS("../source/parameter_selection/NN/original_set/tuned_model.rds")
tuned.model$performances
tuned.model$best.parameters
```

Se puede observar que los mejores resultados se obtuvieron con 21 neuronas con un rendimiento de 99.54562% de acierto.

```{r}
100 - (0.004543848 * 100)
```

Adicionalmente, se puedo obserar que la dispersión de los resultados mediante el proceso de validación cruzada de 10 conjuntos es bastante baja. Este aspecto refleja que los resultados obtenidos no están sesgados.

##Conjunto de datos reducido
En esta sección se realiza la selección de parámetros sobre los conjuntos de datos reducidos producto de la aplicación de la selección de características haciendo uso de las técnicas PCA y GFR.

La selección de los parámetros cambia en conjunto con la reducción de características, puesto que el problema cambió. Es por ello que las características seleccionadas para el conjunto de datos original ya no son de fiar y se deben ajustar los parámetros para los nuevos conjuntos de datos. La reducción de características, como se mencionó previamente ayuda a seleccionar las características realmente útiles y elimina aquellas que puedan aportar ruido. Así mismo, con un conjunto de características reducido es una posibilidad para poder utilizar más neuronas en la capa intermedia del modelo de NN. Puesto que se mencionó previamente que el paquete *nnet* tiene un número máximo de pesos por defecto para proteger al programador o al analista a la hora de entrenar el modelo.

Se poseen dos conjuntos de datos reducidos, el de PCA y el de GFR. Primero se trabajará con el conjunto de PCA y posteriormente con el conjunto de GFR.

###PCA
En la sección de selección de características usando PCA se seleccionaron 7 componentes principales como número de características relevantes. Por ello, en esta sección se trabajará haciendo uso de dichas componentes principales para el ajuste de los parámetros de los modelos NN y SVM.

####SVM
Previamente se mencionó que para el algoritmo SVM con kernel radial hay dos parámetros ajustables que hacen referencia a *gamma* y *cost*. En este caso el parámetro *gamma* por defecto es $\frac{1}{7}$ = 0.14; esto debido a que ahora hay 7 variables predictoras. Por otra parte, *cost* = 1. Dicho esto, el espacio de valores para a probar para cada el ajuste del modelo será: *cost* = {1, 2, 3, 4, 5, 6} y *gamma* = {0.06, 0.07, 0.08, 0.14, 0.2, 0.3, 0.4}. De esta manera se están probando combinaciones usando parámetros por defecto, los parámetros seleccionados haciendo uso del conjunto original y nuevos parámetros cercanos a los parámetros por defecto.

Se empezarán las labores preparando el ambiente de trabajo. Se eliminarán las variables parciales, se cargarán los parámetros por defecto y se cargarppan las bibliotecas a utilizar.

```{r, eval=FALSE}
rm(list = ls())
source("../source/functions/functions.R")
require(e1071)
require(nnet)
```

Se debe cargar el conjunto de datos original y remover las etiquetas que no se utilizarán.

```{r, eval=FALSE}
dataset = read.csv("../dataset/NSLKDD_Training_New.csv")

#Eliminando etiquetas innecesarias
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Al igual que en otras ocasiones, se deben transformar las variables predictoras a tipo numérico. Y escalar el conjunto de datos para que tenga media cero y desviación estándar uno.

```{r, eval= FALSE}
#Transformando variables predictoras a tipo numérico
dataset = as.data.frame(apply(dataset[,-ncol(dataset)], 2, as.numeric))
dataset[,ncol(dataset)+1] = Label
colnames(dataset) = names

#Eliminando variables parciales
remove(list = c("names", "Label"))

#Escalando el conjunto de datos
dataset = ScaleSet(dataset)
```

El siguiente paso pasa por aplicar PCA al conjunto de datos y seleccionar las primeras 7 componentes principales.

```{r, eval=FALSE}
#Aplicando PCA
pca = prcomp(dataset[, -41], scale. = TRUE)
#Seleccionando las primeras 7 componentes
dataset = cbind(as.data.frame(pca$x[,1:7]), Label = dataset$Label)
```

Una vez que se tiene el conjunto de datos deseado, se procede a hacer uso de la función *tune* para hacer la selección de los parámetros. Recordemos que esta función hace uso de validaicón cruzada de 10 conjuntos. Adicionalmente, se colocará una semilla para hacer este paos reproducible.

Este proceso dura algunas horas y es por eso que los resultados se exportarán a un objeto para su posterior análisis.

```{r, eval=FALSE}
set.seed(22)
tuned.model = tune(svm,
                   Label ~.,
                   data = dataset,
                   scale = F,
                   kernel = "radial",
                   ranges = list(cost = c(1, 2, 3, 4, 5, 6),
                                 gamma = c(0.06, 0.07, 0.08, 0.14, 0.2, 0.3, 0.4)
                                 )
                   )

#Guardando los resultados
saveRDS(tuned.model, "../source/parameter_selection/SVM/PCA/tuned_model.rds")
```

Una vez que el proceso es completado se puede cargar el objeto y visualizar los resultados.

```{r}
tuned.model = readRDS("../source/parameter_selection/SVM/PCA/tuned_model.rds")
tuned.model$performances
tuned.model$best.parameters
```

Se observa el resultado con el mejor rendimiento es haciendo uso de los parámetros *cost* = 6 y *gamma* = 0.4. Adicionalmente, se puede apreciar como la dispersión de los resultados es bastante baja. El mejor desempeño obtenido fue de 99.45239% de acierto.

```{r}
100 - (0.005476092 * 100)
```

De esta manera, se espera que con dichos parámetros se pueda alcanzar un mejor desempeño que el alcanzado por el modelo de SVM con parámetros por defecto.

####NN
Esta sección corresponde a la selección de parámetros haciendo uso del conjunto de datos reducido producto de la aplicación de PCA para el modelo de NN. Como se mencionó previamente, se hará uso de las primeras 7 componentes principales. El parámetro a ajustar corresponde a la cantidad de neuronas en la capa intermedia. Parámetro que haciendo uso del conjunto de datos original no podía exceder de 21 neuronas debido a la cantidad de pesos que generaba. Esta es una de las ventajas de la reducción de características reflejada en el modelo de redes neuronales. El rango neuronas intermedias será [17,30].

Se iniciará con el proceso preparando el ambiente de trabajo. Se eliminarán variables parciales, se cargará el archivo de funciones y se cargarán los paquetes a utilizar.

```{r, eval=FALSE}
rm(list = ls())
source("../source/functions/functions.R")
require(e1071)
require(nnet)
```

Una vez listo el ambiente de trabajo es necesario cargar el conjunto de datos original y eliminar aquellas etiquetas que no serán utilizadas.

```{r, eval=FALSE}
#Cargando el conjutno de datos
dataset = read.csv("../dataset/NSLKDD_Training_New.csv")

#Eliminando etiquetas innecesarias
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Se deben transformar las variables predictoras a tipo numérico y escalar el conjunto de datos para que todas las variables predictoras tengan media cero y desviación estándar uno.

```{r, eval=FALSE}
#Extrayendo información
Label = dataset$Label_Normal_ClassAttack
names = colnames(dataset)

#Tranformando variables predictoras a tipo numérico
dataset = as.data.frame(apply(dataset[,-ncol(dataset)], 2, as.numeric))
dataset[,ncol(dataset)+1] = Label
colnames(dataset) = names

#Eliminando variables parciales
remove(list = c("names", "Label"))

#Escalando el conjunto de datos
dataset = ScaleSet(dataset)
```

Una vez que se tiene el conjunto de datos listo, se le deba aplicar PCA y seleccionar las primeras 7 componentes principales.

```{r, eval=FALSE}
#Aplicando PCA
pca = prcomp(dataset[, -41], scale. = TRUE)
#Seleccionando las primeras 7 componentes
dataset = cbind(as.data.frame(pca$x[,1:7]), Label = dataset$Label)
```

Ya se tiene el conjunto de datos reducido que será utilizado. Se establece una semilla para hacer este proceso reproducible y se hace uso de la funcion *nnet*. Recordemos que esta función hace uso de la técnica de validación cruzada de 10 conjuntos para el proceso de selección de parámetros. Adicionalmente, al proceso ser bastante largo, los resultados serán exportados a una objeto para su posterior análisis.

```{r, eval= FALSE}
set.seed(22)
tuned.model = tune.nnet(Label ~.,
                        data = dataset,
                        size = 17:30,
                        maxit = 100)

#Guardando los resultados
saveRDS(tuned.model, "../source/parameter_selection/NN/PCA/tuned_model.rds")
```

Una vez que el proceso culmina, se puede cargar el objeto y visualizar los resultados obtenidos.

```{r}
tuned.model = readRDS("../source/parameter_selection/NN/PCA/tuned_model.rds")
tuned.model$performances
tuned.model$best.parameters
```

Se observa que con 30 neuronas el desempeño es de 99.00276%.

```{r}
100 - (0.009972417 * 100)
```

Adicionalmente, este resultado está respaldado con una dispersión bastante baja en los resultados. De tal manera, se espera que con una mayor cantidad de neuronas que las presentadas en el conjunto de datos por defecto se pueda alcanzar un mejor desempeño a la hora de clasificar las anomalías.

###GFR
En esta sección se hará el ajuste de los parámetros para los modelos de NN y de SVM haciendo uso del conjunto de datos reducido producto de la aplicación de GFR. En la sección de GFR se seleccionaron 9 características como número ideal de características. Estas características difieren en sus nombres entre los modelos de SVM y NN. Sin embargo, para cada modelo en particular se seleccionarán aquellas características seleccionadas idealmente para cada caso.

####SVM
En esta sección se seleccionan los parámetros para el modelo de SVM. en este caso los valores por defecto son: *gamma* = $\frac{1}{9}$ = 0.11 y *cost* = 1. De igual manera que con PCA pasa que al hacer reducción de características el problema cambia y es necesario hacer un ajuste nuevo de los parámetros, diferente al del conjunto original de  NSL-KDD.

Empezaremos la labores preparando el ambiente de trabajo eliminando variables parciales, cargando los paquetes a utilizar y cargando el archivos de funciones a utilizar.

```{r, eval=FALSE}
rm(list = ls())
source("../source/functions/functions.R")
require(e1071)
require(nnet)
```

A continuación, se cargará el conjunto de datos de entrenamiento y se eliminarán aquellas etiquetas que no se utilizarán.

```{r, eval=FALSE}
dataset = read.csv("../dataset/NSLKDD_Training_New.csv")

#Eliminando etiquetas innecesarias
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Se deben pasar todas las variables predictoras a tipo numérico y escalar el conjunto de datos para que las mismas tenga media cero y desviación estándar uno.

```{r, eval=FALSE}
#Extrayendo características
svm.gfr = readRDS("../source/feature_selection/SVM/results_GFR.rds")
svm.gfr = rownames(svm.gfr)[1:9]

#Extrayendo información
Label = dataset$Label_Normal_ClassAttack
dataset = dataset[, svm.gfr]
names = colnames(dataset)


#Transformando variables predictoras en tipo numérico
dataset = as.data.frame(apply(dataset, 2, as.numeric))
dataset[,ncol(dataset)+1] = Label
colnames(dataset) = names

#Eliminando variables parciales
remove(list = c("names", "Label"))

#Escalando el conjunto de datos
dataset = ScaleSet(dataset)
```

En este punto se tiene todo listo para seleccionar los parámetros. Se seleccionará una semilla para la reproducibilidad de la prueba y se almacenará el resultado del proceso en un objeto para su posterior análisis debido al tiempo que tarda este paso en ejecutarse.

```{r, eval=FALSE}
set.seed(22)
tuned.model = tune(svm,
                   Label ~.,
                   data = dataset,
                   scale = F,
                   kernel = "radial",
                   ranges = list(cost = c(1, 2, 3, 4, 5, 6),
                                 gamma = c(0.06, 0.07, 0.08, 0.11, 0.2, 0.3, 0.4)
                                 )
                   )

#Guardando los resultados
saveRDS(tuned.model, "source/parameter_selection/SVM/GFR/tuned_model.rds")
```

Una vez que el proceso culmina se puede cargar el objeto almacenado y visualizar los resultados obtenidos.

```{r}
tuned.model = readRDS("../source/parameter_selection/SVM/GFR/tuned_model.rds")
tuned.model$performances
tuned.model$best.parameters
```

Se observa que los mejores parámetros fueron *cost* = 6 y *gamma* = 0.4. Estos resultados respaldados por la poca dispersión obtenida con dicho resultado. El mejor resultado fue de 99.35383%.

```{r}
100 - (0.006461745 * 100)
```

####NN
En esta sección se realizará la selección de parámetros para NN haciendo uso del conjunto de datos reducido de GFR. En la sección de reducción de características se seleccionaron 9 variables predictoras para NN que serán utilizadas para la selección de parámetros. Adicionalmente, al igual que se explicó en PCA, al haber menor cantidad de neuronas en la capa de salida es posible incrementar la complejidad de la capa intermedia, esperando de esta manera tener mejores resultados.

Se empezarán las labores preparando el ambiente de trabajo eliminando variables parciales, cargando el archivo de funciones y los paquetes a utilizar.

```{r, eval=FALSE}
rm(list = ls())
source("../source/functions/functions.R")
require(e1071)
require(nnet)
```

A continuación se cargará el conjunto de datos de entrenamiento de NSL-KDD original y se eliminarán las etiquetas que no se utilizarán.

```{r, eval=FALSE}
dataset = read.csv("../dataset/NSLKDD_Training_New.csv")

#Eliminando etiquetas innecesarias
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

A continuación se deben extraer las características, transformas las variables predictoras a tipo numérico y escalar el conjunto de datos a media cero y desviación estándar uno.

```{r, eval=FALSE}
#Cargando características
nn.gfr = readRDS("../source/feature_selection/NN/results_GFR.rds")
nn.gfr = rownames(nn.gfr)[1:9]

#Extrayendo información
Label = dataset$Label_Normal_ClassAttack
dataset = dataset[, nn.gfr]
names = colnames(dataset)

#Transformando predictores a tipo numérico
dataset = as.data.frame(apply(dataset, 2, as.numeric))
dataset[,ncol(dataset)+1] = Label
colnames(dataset) = names

#Eliminando variables parciales
remove(list = c("names", "Label"))

#Escalando el conjunto de datos
dataset = ScaleSet(dataset)
```

En este punto ya se tiene el conjunto de datos listo para hacer el proceso de selección de características. Se seleccionará una semilla aleatoria para reproducir los resultados posterioremente y se guardará el resultado en un objeto para su posterior evaluación. Esto, debido a que el proceso dura algunas horas en culiminar.

```{r, eval=FALSE}
set.seed(22)
tuned.model = tune.nnet(Label ~.,
                        data = dataset,
                        size = 17:30,
                        maxit = 100)

#Guardando los resultados
saveRDS(tuned.model, "../source/parameter_selection/NN/GFR/tuned_model.rds")
```

Una vez terminado el proceso es posible cargar los resultados obtenidos y examinar los mismos.

```{r}
tuned.model = readRDS("../source/parameter_selection/NN/GFR/tuned_model.rds")
tuned.model$performances
tuned.model$best.parameters
```

Se puede observar que con 28 neuronas el modelo alcanza el mejor rendimiento. El mismo correspondiente a 99.02691% de acierto.

```{r}
100 - (0.009730928 * 100)
```

Este resultado respaldado por la baja dispersión en los resultados en la validación cruzada de 10 conjuntos.

# Implementación de modelos híbridos sobre el conjunto de datos original, PCA y GFR haciendo uso de parámetros seleccionados

En esta sección se implementarán y analizarán los modelos (III) PCA - NN - K-Medias, (IV) PCA - SVM - K-Medias, (V) GFR - NN - K-Medias y (VI) GFR - SVM - K-Medias. Los mismos se realizarán utilizando parámetros seleccionados. Adicionalmente se probará la repercusión de la selección de parámetros para los modelos (I) NN - K-Medias y (II) SVM - K-Medias. 

Los análisis sobre los modelos descritos previamente serán ejecutados haciendo uso del conjunto de entrenamiento para entrenamiento y prueba, y haciendo uso del conjunto de entrenamiento para el entrenamiento y del conjunto de prueba para la prueba. Este doble enfoque se realizará para observar el desempeño de los modelos ante registros conocidos y sobre registros no conocidos. Recordemos que el conjunto de prueba tiene 14 nuevos ataques que no están presentes en el conjunto de entrenamiento. Y es por esto que se tendrán dos apartaos concernientes al análisis sobre el conjunto de entrenamiento y al análisis sobre el conjunto de prueba.

##Análisis sobre el conjunto de entrenamiento
En esta sección se listan las actividades concernientes al entrenamiento y evaluación de los modelos híbridos haciendo uso exclusivo del conjunto de entrenamiento y de la técnica de validación cruzada de 10 conjunto para la validación del modelos. 

###Conjunto original
En este apartado se utilizará el conjunto de entrenamiento original y se analizará el impacto de la selección de características para los modelos (I) SVM - K-Medias y (II) NN - K-Medias. La sección de análisis de K-Medias será omitida debido a que ya fue realizada anteriormente, es por ello que sólo se entrenará y analizarán los diferentes modelos.

#### (I) NN - K-Medias
Se utilizarán los parámetros seleccionados para el conjunto de datos de entrenamiento original. De esta manera la arquitectura usada será 40-X-5. Donde X corresponde al número de neuronas seleccionadas en el proceso de selección de parámetros.

#### Entrenamiento del modelo

Se iniciarán las tareas limpiando el ambiente de trabajp, cagrando el paquete a hacer utilizado y cargando el archivo de funciones.

```{r, eval=FALSE}
rm(list = ls())
#Cargando paquete
library("nnet")
#Cargando funciones
source("../source/functions/functions.R")
```

Posteriormente se cargará el conjunto de datos de entrenamiento y se eliminarán las etiquetas que no se utilizarán. Recordando que se utilizarán cinco clases objetivo: DoS, normal, Probing, R2L y U2R.

```{r, eval=FALSE}
#Cargando conjunto de datos
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
dataset = dataset.training
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Es importante que las variables predictoras sean de tipo numérico y la variable objetivo de tipo *factor*. Adicionalmente, las variables predictoras deben ser escaladas para que tengan media cero y desviación estándar uno.

```{r, eval=FALSE}
#Asignando los tipos de dato
for (i in 1 : (ncol(dataset) -1) )
  dataset[,i] = as.numeric(dataset[,i])

dataset[,ncol(dataset)] = as.factor(dataset[,ncol(dataset)])

#Escalando el conjunto de datos
dataset = ScaleSet(dataset)
```

Una vez listo el conjunto de datos se inicia con el proceso de validaicón cruzada de 10 conjuntos diviendo el conjunto de datos en 10 partes iguales para su posterior uso.

```{r, eval=FALSE}
#Iniciando el proceso de validación cruzada de 10 conjuntos
cv.sets = CVSet(dataset, k = 10, seed = 22)
length(cv.sets)
```

Se crearán algunas variables parciales que llevarán el registro de los resultados obtenidos durante el proceso.

```{r, eval=FALSE}
#Inicializando algunas variables
results = vector(mode = "numeric", length = 10)
list.results = list(0, 0, 0, 0)
names(list.results) = c("results", "best_model", "best_testing_set", "best_predictions")
best.accuracy = 0
```

En este punto se cargarán los mejores parámetros seleccionados durante el proceso de selección de parámetros para el conjunto de datos de entrenamiento original.

```{r}
hidden.neurons = readRDS("../source/parameter_selection/NN/original_set/tuned_model.rds")
hidden.neurons = hidden.neurons$best.parameters$size
hidden.neurons
```

El número de neuronas seleccionado fue de 21. Así que ese será el número utilizado para el entrenamiento de la red neuronal. Dicho esto, inicia el proceso de validaicón cruzada de 10 conjuntos.

```{r, eval=FALSE}
for (i in 1:10)
{
  #Extracting sets
  testingset = as.data.frame(cv.sets[[i]])
  trainingset = cv.sets
  trainingset[[i]] = NULL
  trainingset = do.call(rbind, trainingset)
  
  #NN Model
  model = nnet(Label ~ .,
               data = trainingset,
               size = hidden.neurons,
               maxit = 100)
  
  #Making predictions
  predictions = predict(model, testingset[, 1:(ncol(testingset)-1)], type = "class")
  
  
  #Calculating accuracy
  accuracy = mean(testingset[, ncol(testingset)] == predictions)
  #Storing results
  results[i] = accuracy
  
  #Storing best results
  if(best.accuracy < accuracy)
  {
    list.results$best_model = model
    list.results$best_testing_set = testingset
    list.results$best_predictions = predictions
    best.accuracy = accuracy
  }
}
```

Como el proceso es bastante largo, los resultados serán almacenados en un objeto para su posterior carga y análisis.

```{r, eval=FALSE}
#Storing results
list.results$results = results

#Saving list of objects
saveRDS(list.results, "../source/tuned_model/original_set/NN/training_set/list_results.rds")
```

####Evaluación del modelo

En esta sección se analizan los resulatdos obtenidos en la sección anterior y adicionalmente agrega el segundo nivel de clasificación correspondiente a K-Medias con dos centroides.

Se empezará por establecer el ambiente de trabajo eliminando variables parciales, cargando el paquete a utilizar y cargando el archivo de funciones correspondiente.

```{r}
rm(list = ls())
#Cargando paquete
library("nnet")
#Cargando funciones
source("../source/functions/functions.R")
```

A continuación se carga la lista de resultados obtenidos la sección anterior y visualizaremos los resultados obtenidos.

```{r}
#Cargando la lista de resultados
list.results = readRDS("../source/tuned_model/original_set/NN/training_set/list_results.rds")
#Mostrando los resultados
list.results$results * 100
```

Se puede observar un resultado excelente con un porcentaje de acierto superior al 99% en todas las iteraciones. A continuación veamos la media de lso resultados obtenidos.

```{r}
mean(list.results$results) * 100
```

Una media de 99.51%. Un resultado excelente para la clasificación. Ahora empezaremos a hacer uso de la matriz de confusión para evaluar el desempeño del mejor modelo obtenido.

```{r}
#Creando la matriz de confusión
confusion.matrix = table(Real = list.results$best_testing_set[,ncol(list.results$best_testing_set)],
                         Prediction = list.results$best_predictions)
#Visualizando la matriz de confusión
confusion.matrix
```

Una matriz de confusión excelente, presentando solo 17 fallos. Ahora veamos la tasa de acierto y la tasa de error.

```{r}
accuracy = mean(list.results$best_testing_set[,ncol(list.results$best_testing_set)] == 
                  list.results$best_predictions)

accuracy * 100
ErrorRate(accuracy) * 100
```

Con 99.69% se refleja el resultado obtenido en la matriz de confusión. Veamos la eficacia por clase.

```{r}
AccuracyPerLabel(confusion.matrix, list.results$best_testing_set)
```

La salida del vector corresponde a las posiciones DoS, normal, Probing, R2L y U2R. Se puede observar como todos los ataques DoS son detectados. Y se tiene un excelente desempeño en la clasificación de las clases normal, Probing y R2L. U2R fue la clase con menor porcentaje de acierto, pero también hay que considerar la escaza ocurrencia de este tipo de clase en el conjunto de entrenamiento.

A continuación se pasará la matriz de confusión de cinco clases a una matriz de confusión binaria para poder sacar otro tipo de medidas.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(list.results$best_testing_set,
                                                             list.results$best_predictions)
attack.normal.confusion.matrix
```

Se obserba que hubo 9 falsos negativos y 8 falsos positivos. Con K-Medias se tratarán de extraer esos 9 ataques que fueorn clasificados como normal. Adicionalmente veamos la sotras medidas de rendimiento.

```{r}
Accuracy(attack.normal.confusion.matrix) * 100
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

Todas están por encima del 99% de acierto. Esto quiere decir que el modelo es muy bueno clasificando el tráfico anómalo y el normal. Para visualizar la certeza del modelo a la hora de tomar decisiones se utilizará la curva ROC.

```{r, fig.align="center"}
#Extrayendo probabilidades
probabilities = predict(list.results$best_model,
                        list.results$best_testing_set[, 1:(ncol(list.results$best_testing_set)-1)])

#Generando la curva ROC
roc.data = DataROC(list.results$best_testing_set, probabilities,
                   list.results$best_predictions)
generate_ROC(scores = roc.data$Prob, real = roc.data$Label,
             pred = roc.data$Prediction)
```

Se puede ver una curva ROC bastante buena que muestra que el modelo toma las decisiones correctar con una alta certeza en las mismas. Ahora se añadirá el segundo nivel de clasificación de K-Medias extrayendo las etiquetas que fueron etiquetadas como normal para intentar extraer los 9 ataques presentes en dicho sub-conjunto.

####Segundo nivel de clasificación (K-Medias)

```{r}
kmeans.set = list.results$best_testing_set[list.results$best_predictions == "normal",]
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

Una vez que se tiene el conjunto de datos a utilizar se pre-calcularán los centroides para maximizar la eficacia en los resultados obtenidos.

```{r}
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)
```

Y una vez precalculados los centroides se procede a la clasificación de K-Medias usando dichos centroides.

```{r}
matrix.centers = matrix.centers/100
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)
```

Posteriormente las predicciones son ordenadas y se crea la matriz de confusión de los resultados.

```{r}
predictions = OrderKmeans(kmeans.model)
confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)
confusion.matrix.kmeans.model
```

En este caso de observa que se detectaron 3 de los ataques, se mantuvieron 6 falsos negativos y se incrementó considerablemente el número de falsos positivos. Un desempeño bastante malo que ensucia y deteriora al primer nivel. Veamos la eficacia del modelo.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)*100
```

El resultado de 76.56% refleja lo visto en la matriz de confusión. Veamos el acierto por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

Solo el 33.33% de los ataques que estaban presentes fueron detectados y se hubieron muchos falsos positivos.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

La baja precisión ilustra lo malo que fue el modelo a la hora de acertar en los ataques que clasificó como ataques.

####Estadísticas totales
Se completará uniendo los resultados obtenidos de ambos niveles para explorar el rendimiento conjunto.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix,
                                          confusion.matrix.kmeans.model)

confusion.matrix.two.labels
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
```

Evidentemente la tasa de aciertos total bajó debido al pobre rendimiento de K-Medias, sin embargo sigue siendo bastante alta, pero por el excelente rendimiento que obtuvo el primer nivel.

```{r}
ErrorRate(accuracy.total) * 100
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
```

El resto de las medidas se ven deterioradas con respecto a las obtenidas en el primer nivel debido a que K-Medias solo empeoró los resultados.

#####Conclusión
Con la inclusión de una neurona más, se obtuvieron mejores resultados con respecto a la tasa de aciertos por parte de K-Medias. Sin embargo, al quitar ataques, K-Medias deterioró su desempeño comparado al previo análisis con parámetros por defecto. Adicionalmente se puede observar que la curva ROC se comporta de manera similar en ambos casos.

###(II) SVM - K-Medias
En este apartado se utilizarán los parámetros seleccionados para el conjunto de datos original.

####Entrenamiento del modelo
Se iniciarán las labores eliminando las variables parciales, cargando los paquetes a utilizar y el archivo de funciones correspondiente.

```{r, eval=FALSE}
rm(list = ls())

#Cargando paquete
library("e1071")

#Cargando archivo de funciones
source("../source/functions/functions.R")
```

Se cargará el conjunto de entrenamiento y se eliminarán las etiquetas que no se utilizarán. Teniendo en cuenta que las cinco variables objetivo son: DoS, normal, Probing, R2L y U2R.
```{r, eval=FALSE}
#Cargando el conjunto de datos
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
dataset = dataset.training
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Las variables predictoras deben ser de tipo numérico y las variables objetivo de tipo factor. Adicionalmente, las variables predictoras deben ser escaladas para que todas tengan media cero y desviación estándar uno.

```{r, eval=FALSE}
#Asignando el tipo de dato correcto
for (i in 1 : (ncol(dataset) -1) )
  dataset[,i] = as.numeric(dataset[,i])

dataset[,ncol(dataset)] = as.factor(dataset[,ncol(dataset)])

#Escalando el conjunto de datos
dataset = ScaleSet(dataset)
```

Una vez que se tiene el conjunto de datos preparado, es posible empezar con las labores de validación cruzada de 10 conjuntos. Así se empezará con al división del conjunto de datos en 10 sub-conjuntos.

```{r, eval=FALSE}
cv.sets = CVSet(dataset, k = 10, seed = 22)
length(cv.sets)
```

Porteriormente, se inicializarán algunas variables que recolectarán los resultados obtenidos durante dicho proceso.

```{r, eval=FALSE}
results = vector(mode = "numeric", length = 10)
list.results = list(0, 0, 0, 0)
names(list.results) = c("results", "best_model", "best_testing_set", "best_predictions")
best.accuracy = 0
```

Se cargarán los parámetros obtenidos durante el proceso de selección de parámetros.

```{r}
tuned.parameters = readRDS("../source/parameter_selection/SVM/original_set/tuned_model.rds")
tuned.cost = tuned.parameters$best.parameters$cost
tuned.gamma = tuned.parameters$best.parameters$gamma
tuned.cost
tuned.gamma
```

Donde se puede observar que *cost* = 6 y *gamma* = 0.08. Es decir, que el modelo opta por permitir mayor error y una región de separación mayor. Ahora, estos parámetros serán utilizados para el proceso de validación cruzada de 10 conjuntos.

```{r, eval=FALSE}
for (i in 1:10)
{
  #Extracting sets
  testingset = as.data.frame(cv.sets[[i]])
  trainingset = cv.sets
  trainingset[[i]] = NULL
  trainingset = do.call(rbind, trainingset)
  
  #SVM Model
  model = svm(Label ~ .,
              data = trainingset,
              kernel = "radial",
              cost = tuned.cost,
              gamma = tuned.gamma,
              scale = FALSE,
              probability = TRUE)
  
  #Making predictions
  predictions = predict(model, testingset[, 1:(ncol(testingset)-1)], type = "class")
  
  
  #Calculating accuracy
  accuracy = mean(testingset[, ncol(testingset)] == predictions)
  #Storing results
  results[i] = accuracy
  
  #Storing best results
  if(best.accuracy < accuracy)
  {
    list.results$best_model = model
    list.results$best_testing_set = testingset
    list.results$best_predictions = predictions
    best.accuracy = accuracy
  }
}
```

Al este proceso ser bastante largo en tiempo, los reusltados serán guardados en un objeto para su posterior evaluación.

```{r, eval=FALSE}
list.results$results = results
saveRDS(list.results, "source/tuned_model/original_set/SVM/training_set/list_results.rds")
```

####Evaluación del modelo
En esta sección se evaluará el desempeño del modelo creado en la sección previa. Se iniciará por preparar el ambiente de trabajo eliminando variables parciales, cargando el paquete a utilizar y el archivo de funciones.

```{r}
rm(list = ls())

#Cargando el paquete
library("e1071")

#Cargando funciones
source("../source/functions/functions.R")
```

Posteriormente se cargarán los resultados obtenidos previamente.

```{r}
list.results = readRDS("../source/tuned_model/original_set/SVM/training_set/list_results.rds")
```

Veamos los resultados obtenidos y la media de aciertos correspondiente.

```{r}
#Mostrando los resultados
list.results$results * 100
#Calculando la media de los resultados
mean(list.results$results) * 100
```

Se puede observar un rendimiento superior a 99.48%. Siendo 99.579% la tasa de acierto promedio. Esta tasa de acierto bastante buena. Veamos graficamente el desempeño del mejor modelo obtenido haciendo uso de la matriz de confusión.

```{r}
#Creando la matriz de confusión
confusion.matrix = table(Real = list.results$best_testing_set[,ncol(list.results$best_testing_set)],
                         Prediction = list.results$best_predictions)
#Mostrando la matriz de confusión
confusion.matrix
```

Solo se presentaron 26 errores en la clasificación. De esta manera se mejora el desempeño obtenido haciendo uso de los parámetros por defecto obtenidos en secciones previas. Ahora veamos algunas medidas de rendimiento como lo son la tasa de aciertos y la tasa de aciertos por etiqueta.

```{r}
accuracy = mean(list.results$best_testing_set[,ncol(list.results$best_testing_set)] == 
                  list.results$best_predictions)

accuracy * 100
ErrorRate(accuracy) * 100
AccuracyPerLabel(confusion.matrix, list.results$best_testing_set)
```

Se puede observar un desempeño muy alto a la hora de detectar las clases DoS, normal, Probing y R2L. Por otra parte, la clase U2R, es la que peor desempeño tiene con solo un 33% de acierto.

A continuación veamos las medidas de rendimiento binarias. Empezando por unificar la matriz de confusión en dos clases: Attack o normal.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(list.results$best_testing_set,
                                                             list.results$best_predictions)
attack.normal.confusion.matrix
```

Se pueden observar que solo hay 26 mala clasificaciones, de las cuales 15 son falsos negativos y 11 falsos positivos. Calculemos algunas medidas de rendimiento.

```{r}
Accuracy(attack.normal.confusion.matrix) * 100
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

Estas medidas de rendimiento nos dicen que el modelo funciona de muy buena manera clasificando el tráfico normal y anómalo. Ahora veamos gráficamente el comportamiento del modelo haciendo uso de la curva ROC.

```{r, fig.align="center"}
#Extrayendo probabilidades
probabilities = attr(predict(list.results$best_model,
                             list.results$best_testing_set[, 1:(ncol(list.results$best_testing_set)-1)],
                             probability = TRUE), "probabilities")

#Generando la curva ROC
roc.data = DataROC(list.results$best_testing_set, probabilities,
                   list.results$best_predictions)
generate_ROC(scores = roc.data$Prob, real = roc.data$Label,
             pred = roc.data$Prediction)
```

Se puede observar un mejor comportamiento que el visto en al curva ROC con parámetros por defecto, esto quiere decir, que con el ajuste de los parámetros se ayudó al modelo a mejorar la confianza con respecto a la toma de decisiones. Sin embargo, el comportamiento sigue siendo más errático que el presentado por NN. 

####Segundo nivel de clasificación (K-Medias)
Se añadirá el segundo nivel de clasificaicón correspondiente a K-Medias con dos grupos. Se empezará por crear el conjunto de datos a utilizar.

```{r}
kmeans.set = list.results$best_testing_set[list.results$best_predictions == "normal",]
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

En este punto se extrajeron todos aquellos registros clasificados como normal, de los cuales se espera poder separar de la mejor manera posible los 15 registros que corresponden a ataques. Se precalcularán los centroides para aumentar la tasa de aciertos del proceso.

```{r}
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)
```

Y una vez finalizado dicho paso, entonces se clasificarán los registros teniendo como punto de partida dichos centroides pre-calculados.

```{r}
matrix.centers = matrix.centers/100
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)
```

Las predicciones son ordenas y se crea la matriz de confusión.

```{r}
predictions = OrderKmeans(kmeans.model)

#Creando la matriz de confusión
confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)
#Imprimiendo la matriz de confusión
confusion.matrix.kmeans.model
```

Veamos la tasa de aciertos y la tasa de error.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)
```

Se puede observar como la inclusión de K-Medias se comportó de manera similar a la que se comportó con parámetros por defecto. No aportó absolutamente nada al primer nivel, sin embargo, no deterioró tampoco el desempeño. Ahora veamos el desempeño por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

Evidentemente al no detectar ningún ataque el rendimiento por ataque es 0%, sin y el de normal es 99.98% debido a que mantuvo los mismos resultados que los arrojados en el primer nivel.

Veamos el restos de las estadísticas que no aportarán mayor información.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

Se observa una alta efectividad al clasificar el tráfico normal y una efectividad nula para detectar los ataques.

####Estadísticas totales
Ahora veamos cual es el desempeño en conjunto de ambos modelos. Empezaremos por unir las dos matrices de confusión.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix,
                                          confusion.matrix.kmeans.model)
confusion.matrix.two.labels
```

La matriz de confusión es idéntica a la presentada en el primer nivel. Ahora veamos la tasa de aciertos y de error totales.

```{r}
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
```

No modificó con respecto a la presentada en el primer nivel de SVM. Por último veamos las estadísticas binarias.


```{r}
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
```

Los resultados muestran que el modelo se comporta de muy buena manera a la hora de clasificar el tráfico normal y anómalo.

####Conclusión
Se mejoró el desempeño de SVM con respecto a los parámetros por defecto. Hubo menor cantidad de errores y se mejoró el comportamiento errático presentado en la curva ROC. Por otra parte, K-Medias no aportó nada al modleo ni lo deterioró, situación que si sucedió con la inclusión de una nueva neurona el el modelo presentado previamente. Por último este último modelo mejora el rendimiento de NN con 21 neuronas en la capa intermedia. Siendo su único punto débil el comportamiento errático de la curva ROC.

##Conjunto reducido PCA
En esta sección se hará uso del conjunto de datos derivado de la aplicación de PCA, el cual corresponde al uso de las 7 primeras componentes principales seleccionadas en la sección de selección de características.

###K-Medias
Se empezará por realizar el análisis sobre K-Medias haciendo uso del nuevo conjunto de datos. El análisis realizado para este algoritmo haceindo uso del conjunto de datos por defecto ya no es válido debido a la reducción del mismo.

Se empezará por establecer el ambiente de trabajo eliminando variables parciales y el archivo de funciones correspondiente.

```{r, eval=FALSE}
rm(list = ls())
source("../source/functions/functions.R")
```

Seguidamente se cargará el conjunto de datos de entrenamiento y se eliminarán las etiquetas que no se utilizarán, en esta ocasión se dejarán las etiquetas de cinco clases: Dos, nomrla, Probing, R2L y U2R. Y la de dos clases: Attack y normal.

```{r, eval=FALSE}
dataset = read.csv("../dataset/NSLKDD_Training_New.csv")

#eliminando etiquetas innecesarias
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
```

El siguiente paso pasa por asignarle el tipo numérico a las variables predictoras, escalar el conjunto de datos, aplicar PCA y seleccionar las primeras 7 componentes principales.

```{r, eval=FALSE}
#Extrayendo información
Labels = dataset[, (ncol(dataset)-1):ncol(dataset)]

#Transformando predictores en tipo numérico
dataset = as.data.frame(apply(dataset[, c(-41, -42)], 2, as.numeric))
dataset = cbind(dataset, Label = Labels[,1])

#Escalando el conjunto de datos
dataset = ScaleSet(dataset)

#Aplicando PCA
pca = prcomp(dataset[, -41], scale. = TRUE)
dataset = cbind(as.data.frame(pca$x[,1:7]), Label = Labels[,1])
dataset.five = cbind(as.data.frame(pca$x[,1:7]), Label = Labels[,1])
dataset.two = cbind(as.data.frame(pca$x[,1:7]), Label = Labels[,2])

#Eliminando variables parciales
remove(list = c("pca", "Labels"))
```

En este punto se tienen los conjuntos de datos que se utilizarán listos y se puede aplicar el codo de Jambu para ver cual es la cantidad de centroides óptima a utilizar.

####Codo de Jambu
Al igual que en secciones anteriores, se utilizará el método de codo de Jambú para ver con cual configuración de centroides y con cual tipo de algoritmo se obtiene un mejor desempeño.

```{r, eval=FALSE}
IIC.Hartigan = vector(mode = "numeric", length = 30)
IIC.Lloyd = vector(mode = "numeric", length = 30)
IIC.Forgy = vector(mode = "numeric", length = 30)
IIC.MacQueen = vector(mode = "numeric", length = 30)

for (k in 1:30)
{
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "Hartigan-Wong")
  IIC.Hartigan[k] = groups$tot.withinss
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "Lloyd")
  IIC.Lloyd[k] = groups$tot.withinss
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "Forgy")
  IIC.Forgy[k] = groups$tot.withinss
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "MacQueen")
  IIC.MacQueen[k] = groups$tot.withinss
}
```

Los resultados obtenidos son guardados en un objeto para su posterior análisis.

```{r, eval=FALSE}
#Creando una lista para guardar los resultados
jambu.results = list(IIC.Hartigan = IIC.Hartigan, IIC.Lloyd = IIC.Lloyd,
                     IIC.Forgy = IIC.Forgy, IIC.MacQueen = IIC.MacQueen)
#Guardando los resultados
saveRDS(object = jambu.results, file = "../source/tuned_model/PCA/KMEANS/jambu_results_7_features.rds")
```

Adicionalmente, se seleccionará el mejor algoritmo haciendo 50 corridas de los mismos con 2 y 5 centroides y luego promediando los resultados.

```{r, eval = FALSE}
measure.two = lapply(MeasuareKMeans(dataset, 2), max)
measure.five = lapply(MeasuareKMeans(dataset, 5), max)
```

De igual manera, los resultados derivados serán almacenados en un objeto para su posterior análisis.

#####Análisis codo de Jambu
Una vez finalizado el proceso, entonces se pueden observar los resultados obtenidos para el análisis del codo de Jambu. Empezaremos por eliminar variables parciales y cargar el archivo de funciones correspondiente.

```{r}
rm(list = ls())
source("../source/functions/functions.R")
```

Se hará el mismo trabajo de cargar el conjunto de datos, eliminar etiquetas innecesarias, transformar los predicores a tipo numérico, escalar el conjunto de datos y aplicar PCA para crear los conjuntos de datos a utilizar.

```{r}
#Cargando el conjunto de datos
dataset = read.csv("../dataset/NSLKDD_Training_New.csv")

#Eliminando etiquetas innecesarias
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL

#Extrayendo información
Labels = dataset[, (ncol(dataset)-1):ncol(dataset)]

#Transformando predictores en tipo numérico
dataset = as.data.frame(apply(dataset[, c(-41, -42)], 2, as.numeric))
dataset = cbind(dataset, Label = Labels[,1])

#Escalando el conjunto de datos
dataset = ScaleSet(dataset)

#Aplicando PCA
pca = prcomp(dataset[, -41], scale. = TRUE)
dataset = cbind(as.data.frame(pca$x[,1:7]), Label = Labels[,1])
dataset.five = cbind(as.data.frame(pca$x[,1:7]), Label = Labels[,1])
dataset.two = cbind(as.data.frame(pca$x[,1:7]), Label = Labels[,2])

#Eliminando variables parciales
remove(list = c("pca", "Labels"))
```

Una vez realizado este procedimiento se puden cargar los resultados obtenidos del codo de Jambu y graficarlos.

```{r, fig.align="center"}
jambu.results = readRDS("../source/tuned_model/PCA/KMEANS/jambu_results_7_features.rds")
plot(jambu.results$IIC.Hartigan, col = "blue", type = "b", pch = 19, main = "Codo de Jambu",
     xlab = "Número de Centroides", ylab = "Varianza", log = "y")
points(jambu.results$IIC.Lloyd, col = "red", type = "b", pch = 19)
points(jambu.results$IIC.Forgy, col = "green", type = "b", pch = 19)
points(jambu.results$IIC.MacQueen, col = "magenta", type = "b", pch= 19)
legend("topright", legend = c("Hartigan", "Lloyd", "Forgy", "MacQueen"),
       col = c("blue","red", "green", "magenta"), pch = 19)
```

Se puede observar como el gráfico es mucho más estable que el presentando con el conjunto de datos original. aparentemente con 3 centroides debería ser un buen número, sin embargo, se sabe de antemano que se debe hacer uso de 2 o 5 grupos. Así que de nuevo este gráfico no nos ayuda de mucho. Veamos cual de los algoritmos es el mejor, aunque todos para 2 y 5 clases presentan el mismo rendimiento, así que es indiferente cual se utilice.

```{r}
measures.results = readRDS("../source/tuned_model/PCA/KMEANS/measures_results_7_features.rds")
measures.results$measure.two
measures.results$measure.two[1]
measures.results$measure.five
measures.results$measure.five[1]
```

Se observa que Hartigan es el algoritmo seleccionado para dos y cinco grupos.

####K-Medias (cinco grupos)
Ahora comenzaremos con el análisis con la utilización de cinco grupos. Para ello entrenaremos 10 modelos de K-Medias

```{r}
results.five = vector(mode = "numeric", length = 10)
best.accuracy.five = 0
for (i in 1:length(results.five))
{
  set.seed(i)
  model.kmeans.five = kmeans(dataset.five[,-ncol(dataset.five)],
                             5, iter.max = 100)
  
  prediction.five = OrderKmeans(model.kmeans.five)
  accuracy.five = mean(prediction.five == dataset.five$Label)
  
  results.five[i] = accuracy.five
  
  if(best.accuracy.five < accuracy.five)
  {
    best.prediction.five = prediction.five
    best.accuracy.five = accuracy.five
  }
}
```

Una vez finalizado el proceso veamos los resultados y la media de aciertos.

```{r}
results.five * 100
mean(results.five) * 100
```

Se observa un desempeño promedio de 74%.

Ahora vamos a analizar el mejor modelo obtenido visualizando la matriz de confusión.

```{r}
confusion.matrix.five = table(Real = dataset.five$Label,
                              Prediction = best.prediction.five)
confusion.matrix.five
```

Se observa que presenta muchos registros mal clasificados que quedan fuera de la diagonal. Ahora veamos la tasa de acierto y de error alcanzada por el modelo.

```{r}
best.accuracy.five*100
ErrorRate(best.accuracy.five)*100
```

Una tasa de aciertos de 79% bastante alta. Ahora veamos la eficacia por clase.

```{r}
AccuracyPerLabel(confusion.matrix.five, dataset.five)
```

Se observa que el modelo es bastante bueno detectando las clases DoS y normal. Regular para la clase *Probing* y muy malo para el restos de las clases correspondientes a R2L y U2R. Veamos la matriz binaria para calcular otras medidas de rendimiento.

```{r}
attack.normal.confusion.matrix.five = AttackNormalConfusionMatrix(dataset.five,
                                                                  best.prediction.five)
attack.normal.confusion.matrix.five
```

A pesar de que hay bastantes falsos postivos y negativos, estadísticamente se puede apreciar una alta tasa de aciertos.

```{r}
Accuracy(attack.normal.confusion.matrix.five) * 100
AccuracyPerLabel(attack.normal.confusion.matrix.five, dataset.two)
Sensitivity(attack.normal.confusion.matrix.five) * 100
Especificity(attack.normal.confusion.matrix.five) * 100
Precision(attack.normal.confusion.matrix.five) * 100
```

Se puede observar que  se obtuvo un 90% de acierto con 91% de los ataques detectados, el resto de las medidas respaldan los resultados de un modelo que separando tráfico anómalo del normal parace hacerlo bastante bien.

####K-Medias (dos grupos)
Veamos el desempeño a la hora de utilizar dos grupos en vez de cinco. Empezaremos poe ejecutar 10 veces el algoritmo.


```{r}
results.two = vector(mode = "numeric", length = 10)
best.accuracy.two = 0

for (i in 1:length(results.two))
{
  set.seed(i)
  model.kmeans.two = kmeans(dataset.two[,-ncol(dataset.two)],
                            2, iter.max = 100)
  
  prediction.two = OrderKmeans(model.kmeans.two)
  accuracy.two = mean(prediction.two == dataset.two$Label)
  
  results.two[i] = accuracy.two
  
  if(best.accuracy.two < accuracy.two)
  {
    best.prediction.two = prediction.two
    best.accuracy.two = accuracy.two
  }
}
```

Veamos los resultados obtenidos y el promedio.

```{r}
results.two * 100
mean(results.two) * 100
```

Se obtiene una media de 80.6% que supera a la obtenida con cinco clases y se observa que hay resultados que se repiten en diferentes iteraciones, es decir, se alcanza el mismo mínimo local varias veces, esto da la sensación de pensar que con dos conjuntos el modelo alcanza su mejor desempeño. Ahora veamos como queda ordenada la matriz de confusión del mejor modelo obtenido.

```{r}
confusion.matrix.two = table(Real = dataset.two$Label,
                             Prediction = best.prediction.two)
confusion.matrix.two
```

Se obtienen muchos falsos negativos en comparación a los falsos positivos generados. De esta manera se reducen el número de fallos presentados con cinco grupos. Ahora veamos la mejor tasa de aciertos presentada.

```{r}
best.accuracy.two*100
ErrorRate(best.accuracy.two)*100
```

El mejor modelo alcanzó una tasa de aciertos del 90.45%. Mejorando la obtenida por los cinco grupos. Terminemos el análisis viendo las medidas de rendimiento restantes.

```{r}
AccuracyPerLabel(confusion.matrix.two, dataset.two)
Sensitivity(confusion.matrix.two) * 100
Especificity(confusion.matrix.two) * 100
Precision(confusion.matrix.two) * 100
```

Se presenta un 80% de acierto a la hora de detectar ataques, 10% menor que el número de ataques presentados en el modelo con cinco clases. Luego el resto de las estadísticas muestran un modelo cuya fortaleza es la de clasificar el tráfico normal, con un alto acierto en la detección de ataques.

#### Conclusión
Con la reducción de características se quitó ruido al codo de Jambu, se obtienen muy buenos resultados a la hora de clasificar y crear los grupos, que sugiere que la reducción de características funciona de buena manera. Por otra parte, con dos grupos se alcanza el mejor desempeño por parte de K-Medias, ya que es el algoritmo que presenta mejor porcentaje de acierto, mejor promedio y acierta contables veces a el mismo mínimo local.

###(III) PCA - NN - K-Medias
Se iniciará el proceso para el modelo que combina PCA - NN - K-Medias. Se hará uso de lso parámetros seleccionados con anterioridad en la sección correspondiente a la selección de parámetros.

####Entrenamiento del modelo

Se empezarán las labores preparando el ambiente de trabajo eliminando variables parciales, cargando el archivo de funciones y el paquete correspondiente.

```{r, eval=FALSE}
rm(list = ls())

#Cargando paquete
library("e1071")

#Cargando funciones
source("../source/functions/functions.R")
```

El siguiente paso consta de la carga del conjunto de datos de entrenamiento y la eliminación de las etiquetas que no se utilizarán. Ya que se utilizará la etiqueta de cinco clases correspondientes a: DoS, normal, Probing, R2L y U2R.

```{r, eval=FALSE}
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
dataset = dataset.training
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Las variables predictoras deben ser de tipo numérico y la objetivo de tipo *factor*. Adicionalmente el conjunto de datos será escalado para que la svariables predictoras tenga media cero y desviación estándar uno.

```{r, eval=FALSE}
#asignando el tipo de dato correspondiente a las variables
for (i in 1 : (ncol(dataset) -1) )
  dataset[,i] = as.numeric(dataset[,i])

dataset[,ncol(dataset)] = as.factor(dataset[,ncol(dataset)])

#Escalando el conjunto de datos
dataset = ScaleSet(dataset)
```

En este punto se tiene todo listo para aplicar PCA y seleccionar las primeras 7 componentes.

```{r, eval=FALSE}
pca = prcomp(dataset[, -41], scale. = TRUE)
dataset = cbind(as.data.frame(pca$x[,1:7]), Label = dataset$Label)
```

Luego, se puede empezar con el proceso de validación cruzada dividiendo el conjunto de datos en 10 sub-conjuntos.

```{r, eval=FALSE}
cv.sets = CVSet(dataset, k = 10, seed = 22)
length(cv.sets)
```

Y luego inicializar algunas variables para almacenar los resultados de dicho proceso.

```{r, eval=FALSE}
results = vector(mode = "numeric", length = 10)
list.results = list(0, 0, 0, 0)
names(list.results) = c("results", "best_model", "best_testing_set", "best_predictions")
best.accuracy = 0
```

El número de neuronas seleccionado se carga para luego ser pasado como paŕametro.

```{r}
hidden.neurons = readRDS("../source/parameter_selection/NN/PCA/tuned_model.rds")
hidden.neurons = hidden.neurons$best.parameters$size
hidden.neurons
```

30 fue el número de neuronas de la capa intermedia que se seleccionaron. Ahora si se puede ejecutar la validacipon cruzada de 10 conjuntos.

```{r, eval=FALSE}
for (i in 1:10)
{
  #Extracting sets
  testingset = as.data.frame(cv.sets[[i]])
  trainingset = cv.sets
  trainingset[[i]] = NULL
  trainingset = do.call(rbind, trainingset)
  
  #NN Model
  model = nnet(Label ~ .,
               data = trainingset,
               size = hidden.neurons,
               maxit = 100)
  
  #Making predictions
  predictions = predict(model, testingset[, 1:(ncol(testingset)-1)], type = "class")
  
  
  #Calculating accuracy
  accuracy = mean(testingset[, ncol(testingset)] == predictions)
  #Storing results
  results[i] = accuracy
  
  #Storing best results
  if(best.accuracy < accuracy)
  {
    list.results$best_model = model
    list.results$best_testing_set = testingset
    list.results$best_predictions = predictions
    best.accuracy = accuracy
  }
}
```

Como el proceso es extenso, los resultados son guardados en una lista y posteriormente en un objeto para sus posterior análisis.

```{r, eval=FALSE}
list.results$results = results
saveRDS(list.results, "../source/tuned_model/PCA/NN/training_set/list_results.rds")
```

####Evaluación del modelo
En esta sección se evaluará el modelo creado en la sección anterior. Emepezaremos eliminando variables parciales, cargando el paquete a utilizar y el archivo de funciones correspondiente.

```{r}
rm(list = ls())

#Cargando paquetes
library("nnet")

#Cargando el archivo de funciones
source("../source/functions/functions.R")
```

Carguemos los resultados obtenidos, y veamos los resultados.

```{r}
list.results = readRDS("../source/tuned_model/PCA/NN/training_set/list_results.rds")
list.results$results
mean(list.results$results) * 100
```

Se puede observar una media de aciertos de 99.08%. Veamos como quedó organizada la matriz de confusión del mejor modelo obtenido.
```{r}
confusion.matrix = table(Real = list.results$best_testing_set[,ncol(list.results$best_testing_set)],
                         Prediction = list.results$best_predictions)
confusion.matrix
```

Una matriz de confusión bastante ordenada, ahora veamos la tasa de acierto y de error.

```{r}
accuracy = mean(list.results$best_testing_set[,ncol(list.results$best_testing_set)] == 
                  list.results$best_predictions)
accuracy * 100
ErrorRate(accuracy) * 100
```

Con 99.3%, la tasa de aciertos es excelente. Por último veamos la tasa de aciertos por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix, list.results$best_testing_set)
```

Se obtiene un excelente desempeño para detectar las clases DoS, normal y *Probing*, para la clase *R2L* es bastante bueno también y un desempeño intermedio para la clase U2R, que es la clase que menor cantidad de registros presenta. Ahora se calcularán las medidas de rendimiento binarias, para ello se creará una matriz de confusión binaria.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(list.results$best_testing_set,
                                                             list.results$best_predictions)
attack.normal.confusion.matrix
```

Se presentan solo 36 errores donde 16 son falsos negativos y 20 falsos positivos. La cantidad de ataques detectado es bastante grande. Veamos el resto de las medidas de rendimiento,}}
```{r}
Accuracy(attack.normal.confusion.matrix) * 100
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

Las mismas reflejan el desempeño de un modelo muy bueno. Por último veamos la curva ROC.

```{r, fig.align="center"}
#Extrayendo probabilidades
probabilities = predict(list.results$best_model,
                        list.results$best_testing_set[, 1:(ncol(list.results$best_testing_set)-1)])

#Generating curve ROC
roc.data = DataROC(list.results$best_testing_set, probabilities,
                   list.results$best_predictions)
generate_ROC(scores = roc.data$Prob, real = roc.data$Label,
             pred = roc.data$Prediction)
```

El rendimiento de la curva ROC es muy bueno que indica que el modelo toma las decisiones con una alta confianza y probabilidad de acierto sobre las mismas, de hecho, se puede observar ua mejor curva ROC que con el conjunto de datos entero.

####Segundo nivel de clasificación (K-Medias)
Ahora agregaremos el segundo nivel de clasificación. Empezaremos por tomar los regitros clasificados como normal para crear un nuevo conjunto d edatos.

```{r}
kmeans.set = list.results$best_testing_set[list.results$best_predictions == "normal",]
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

Se reflejan los 16 falsos negativos presentados con anterioridad. A continuación se buscarán los mejores centroides.

```{r}
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)
```

Al pre-calcular los centroides se maximiza la convergencia al mejor mínimo local. Ahora los centroides pre-calculados son pasados como parámetros para la clasificaicón definitiva.

```{r}
matrix.centers = matrix.centers/100
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)

```

Veamos la matriz de confusión.

```{r}
predictions = OrderKmeans(kmeans.model)
confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)
confusion.matrix.kmeans.model
```

De los 16 ataques solo se detectaron 6, y se agregaron 327 falsos positivos. La alta precisión del primer nivel hace que de nuevo el segudno nivel de K-Medias sea inútil. Ahora veamos la tasa de error y de aciertos alcanzada y la precisión por etiqueta.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)*100
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

Se muestra una alta tasa de aciertos que pasa por la alta clasificación del tráfico normal. Pero un pobre desempeño a la hora de clasificar los ataques presentes. Veamos el resto de las medidas.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

Estas reflejan lo bueno del modelo para la clasificación del tráfico normal y lo malo que es el mismo a la hora de clasificar los ataques y lo poco confiable que son las respectivas predicciones de los ataques. Para terminar veamos las estadísticas totales.

####Estadísticas totales

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix,
                                          confusion.matrix.kmeans.model)

confusion.matrix.two.labels
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
```

Se observa que en conjunto las estadísticas son bastantes buenas, sin embargo, se pudo observar previamente que el segundo nivel de clasificación de K-Medias deteriora lo realizado por el primer modelo.

####Conclusiones
La reducción de carcaterísticas fue efectiva para NN, presentando una alta tasa de aciertos en todos las clases, y una curva ROC que respalda la buena toma de decisiones tomadas por el modelo. De nuevo, el segundo nivel de K-MEdias no fue d emucha ayuda debido al buen rendimiento del primer nivel. Se espera que el segundo nivel funcione mejor si el primer nivel presenta mayor cantidad de falsos negativos.

###(IV) PCA - SVM - K-Medias
En esta sección se utilizarán los parámetros seleccionados para el agoritmo de SVM.

####Entrenamiento del modelo
Se empezarán las labores eliminando variables parciales, cargando elpaquete a utilizar y el archivo de funciones correspondiente.

```{r, eval=FALSE}
rm(list = ls())

#Cargando paquetes
library("e1071")

#Cargando funciones
source("../source/functions/functions.R")
```

A continuación se carga el conjunto de entrenamiento y se eliminan las equitas innecesarias.

```{r, eval=FALSE}
dataset = dataset.training
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Las variables predictoras deben ser de tipo numérico y la variable objetivo debe ser de tipo *factor*, de igual manera, el conjunto de datos debe ser escalado para que la svariables predictoras tengan emdia cero y desviación estándar uno.

```{r, eval=FALSE}
for (i in 1 : (ncol(dataset) -1) )
  dataset[,i] = as.numeric(dataset[,i])

dataset[,ncol(dataset)] = as.factor(dataset[,ncol(dataset)])

#Escalando el conjunto de datos
dataset = ScaleSet(dataset)
```

En este punto se puede aplicar PCA y seleccionar las 7 componentes principales.

```{r, eval=FALSE}
pca = prcomp(dataset[, -41], scale. = TRUE)
dataset = cbind(as.data.frame(pca$x[,1:7]), Label = dataset$Label)
```

Ahora se puede empezar con la fase de validación cruzada de 10 conjuntos dividiendo el conjunto de datos en 10 sub-conjuntos.

```{r, eval=FALSE}
cv.sets = CVSet(dataset, k = 10, seed = 22)
length(cv.sets)
```

Luego se inicializan algunas variables para llevar el control de los resultados obtenidos durante el proceso.

```{r, eval=FALSE}
results = vector(mode = "numeric", length = 10)
list.results = list(0, 0, 0, 0)
names(list.results) = c("results", "best_model", "best_testing_set", "best_predictions")
best.accuracy = 0
```

Ahora se cargan los parámetros seleccionados. y se guardan en una variable para luego ser pasados al modelo.

```{r}
tuned.parameters = readRDS("../source/parameter_selection/SVM/PCA/tuned_model.rds")
tuned.cost = tuned.parameters$best.parameters$cost
tuned.gamma = tuned.parameters$best.parameters$gamma
tuned.cost
tuned.gamma
```

Se puede observar que el valor de *cost* = 6 y de *gamma* = 0.4, esta configuración sugiere un espectro de las regiones clasificadoras más amplio y permitiendo mayor error que con los parámetros por defecto. A continuación se inicia con la validación cruzada de 10 conjuntos.

```{r, eval=FALSE}
for (i in 1:10)
{
  #Extracting sets
  testingset = as.data.frame(cv.sets[[i]])
  trainingset = cv.sets
  trainingset[[i]] = NULL
  trainingset = do.call(rbind, trainingset)
  
  #SVM Model
  model = svm(Label ~ .,
              data = trainingset,
              kernel = "radial",
              cost = tuned.cost,
              gamma = tuned.gamma,
              scale = FALSE,
              probability = TRUE)
  
  #Making predictions
  predictions = predict(model, testingset[, 1:(ncol(testingset)-1)], type = "class")
  
  
  #Calculating accuracy
  accuracy = mean(testingset[, ncol(testingset)] == predictions)
  #Storing results
  results[i] = accuracy
  
  #Storing best results
  if(best.accuracy < accuracy)
  {
    list.results$best_model = model
    list.results$best_testing_set = testingset
    list.results$best_predictions = predictions
    best.accuracy = accuracy
  }
}
```

Este proceso es bastante duradero en tiempo, es por elloq ue lso resultaods se almacenarán en un a lista y posteriormente en un objeto para su posterior evaluación.

```{r, eval=FALSE}
list.results$results = results
saveRDS(list.results, "source/tuned_model/PCA/SVM/training_set/list_results.rds")
```

####Evaluación del modelo
Para la evaluación del modelo empezaremos por eliminar variables parciales, cargar el paquete a utilizar y el archivo de funciones correspondiente.

```{r}
rm(list = ls())

#Cargando paquetes
library("e1071")

#Cargando funciones
source("../source/functions/functions.R")
```

Se cargan los resulatdos obtenidos en la sección anterior.

```{r}
list.results = readRDS("../source/tuned_model/PCA/SVM/training_set/list_results.rds")
```

Veamos los resultados obtenidos y la media de aciertos.

```{r}
list.results$results
#Calculando la media de los resultados
mean(list.results$results) * 100
```

Se obtuvo una tasa de aciertos promedio de 99.4%. Esta tasa es bastante alta y buena cosideranod que se redujo a 7 el número de variables predictoras. Veamos la matriz de confusión obtenida del mejor modelo obtenido.

```{r}
confusion.matrix = table(Real = list.results$best_testing_set[,ncol(list.results$best_testing_set)],
                         Prediction = list.results$best_predictions)
confusion.matrix
```

La matriz de confusión está bastante ordenada, sin emabrgo presenta un peor desempeño que el modelo de NN. Ahora veamos las tasas de acierto y de error del modelo.

```{r}
accuracy = mean(list.results$best_testing_set[,ncol(list.results$best_testing_set)] == 
                  list.results$best_predictions)

accuracy * 100
ErrorRate(accuracy) * 100
```

El mejor modelo obtuvo una tasa de aciertos de 99.53%. Ahora veamos el rendimiento por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix, list.results$best_testing_set)
```

Se observa un muy buen rendimeinto para todas las clases salvo para la clase U2R, cuyo rendimiento fue nulo. Hay que recordar que esta clase es la que menor cantidad de regitros presenta en el conjunto de datos. Ahora calculemos las menidas binarias de rendimiento, para ello empezaremos por crear una matriz de confusión binaria.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(list.results$best_testing_set,
                                                             list.results$best_predictions)
attack.normal.confusion.matrix
```

Se puede observar que solo hubo 41 fallos, donde 11 fueron falsos positivos y 30 falsos negativos. De resto, el resultado es excelente. Veamos el resto de estadísticas.

```{r}
Accuracy(attack.normal.confusion.matrix) * 100
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

Las estadísticas reflejan un modelo muy bueno a la hora de clasificar tráfico anómalo y normal. Por último veamos la curva ROC.

```{r, fig.align="center"}
probabilities = attr(predict(list.results$best_model,
                             list.results$best_testing_set[, 1:(ncol(list.results$best_testing_set)-1)],
                             probability = TRUE), "probabilities")

roc.data = DataROC(list.results$best_testing_set, probabilities,
                   list.results$best_predictions)
generate_ROC(scores = roc.data$Prob, real = roc.data$Label,
             pred = roc.data$Prediction)
```

El desempño de la curva ROC no se ve nada bien, se ve que el modelo comete bastantes fallos con altos niveles de probabilidad. En contraste con la curva ROC generada por el modelo de redes neuronales que se veía mucho más estable.

####Segundo nivel de clasificación (K-Medias)
Llegó el momento de introducir el segundo nivel de clasificación correspondiente a K-Medias. Se empezará por extraer los registros clasificados como normal para crear el conjunto de datos a ser utilziado por K-Medias.

```{r}
kmeans.set = list.results$best_testing_set[list.results$best_predictions == "normal",]
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

Se observa que los 30 falsos negativos estpan presentes, y son estos los que se tratarán de extraer. Para ello primero se pre-calcularán los centroides de K-Medias para posicionar los centroides incialmente de la mejor manera posible.

```{r}
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)
```

Luego, la posición de los centroides es utilizada para el proceso de clasificación definitivo.

```{r}
matrix.centers = matrix.centers/100
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)
```

Veamos la matriz de confusión.

```{r}
predictions = OrderKmeans(kmeans.model)

confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)
confusion.matrix.kmeans.model
```

Se detectaron a penas 10 ataques, y se agregaron 591 nuevos falsos positivos. De nuevo, se espera que en un escenario donde el primer nivel falle más, se pueda obtener un mejor desempeño por parte de K-Medias. Veamos la tasa de acierto y de error.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)
```

Con 87% de eficacia al clasificar la mayoría del tráfico normal, sin embargo, no es tan efectivo a la hora de detectar ataques. Veamos la eficacia por etiquetas.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

Solo el 33% de los ataques son detectados. Terminemos la evaluación de K-Medias viendo el resto de las estadísticas binarias.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

Estas estadísticas reflejan con la sensitiviidad y la precisión lo malo que es es el modelo para detectar los ataques.

####Estadísticas totales
Ahora combinemos los resultados obtenidos. EMpezando por combinar las matrices de confusión.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix,
                                          confusion.matrix.kmeans.model)

confusion.matrix.two.labels
```

Se puede observar que el desempeño comparado al obtenido únicamente por el primer nivel se deterioró. Ahora veamos el resto de las estadísticas.

```{r}
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
```

Se redujo la especificidad y la presición con respecto al desempeño obtenido en el primer nivel, ya que la inclusión de K-Medias deterioró considerablemente el desempeño.

####Conclusiones
La reducción de características funcionó de buena manera para SVM, donde se pudo observar hubo un muy buen rendimiento. Sin embargo, al haber menos información, el algoritmo de K-Medias ahora deteriora el desempeño del modelo más que lo que hacía usando las características orginales.

###Conclusión
La reducción de características de PCA funciona de buena manera para el primer nivel de clasificación. Sin embargo, para el segundo nivel correspondiente a K-Medias los resultados empeoraron con respecto a los obtenidos haciendo uso de los parámetros por defecto, esto debido a que ahora se introducen mayor cantidad de falsos positivos.

##Conjunto Reducido GFR
En esta sección se hará el análisis sobre el conjunto de entrenamiento reducido luego de aplicar la selección de características GFR. En esta ocasión todas las actividades deben hacerse al doble, debido a que la selección de características por parte de NN fueron diferentes a las de SVM.

###K-Medias
Empezaremos por el entrenamiento y análisis de K-Medias para NN y para SVM de manera independiente.

####NN
En esta sección se hará uso de las 9 características seleccionadas por GFR para NN.Empezaremos por preparar el ambiente de trabajo eliminando variables parciales y cargando el archivo de funciones correspondiente.

```{r, eval=FALSE}
rm(list = ls())
source("../source/functions/functions.R")
```

A continuación se carga el conjunto de datos de entrenamiento y se eliminan las etiquetas innecesarias.

```{r, eval=FALSE}
dataset = read.csv("../dataset/NSLKDD_Training_New.csv", sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
```

Posteriormente, se cargan las características seleccionadas por GFR.

```{r, eval=FALSE}
nn.gfr = readRDS("../source/feature_selection/NN/results_GFR.rds")
nn.gfr = rownames(nn.gfr)[1:9]
```

Se debe crear el nuevo conjunto de datos. Para ello se extrae información importante, se transforman las variables predictoras a tipo numérico y se eliminan variables parciales.

```{r, eval=FALSE}
#Extrayendo información
Labels = dataset[, (ncol(dataset)-1):ncol(dataset)]
dataset = dataset[, nn.gfr]

#Transformando variables predictoras a tipo numérico
dataset = as.data.frame(apply(dataset, 2, as.numeric))
dataset.five = cbind(dataset, Label = Labels[,1])
dataset.two = cbind(dataset, Label = Labels[,2])
dataset = cbind(dataset, Label = Labels[,1])

#Eliminando variables parciales
remove(list = c("Labels"))
```

Posteriormente se escalan los conjuntos de datos generados para que tengan media cero y desviación estándar uno.

```{r, eval=FALSE}
dataset = ScaleSet(dataset)
dataset.two = ScaleSet(dataset.two)
dataset.five = ScaleSet(dataset.five)
```

En este punto se tienen los conjuntos de datos preparados para ser evaluados en el codo de Jambu.

#####Codo de Jambu
Con el codo de Jambu s eve la distancia intra-grupos para seleccionar la configuración de centroides que arroje mejor desempeño.

```{r, eval=FALSE}
IIC.Hartigan = vector(mode = "numeric", length = 30)
IIC.Lloyd = vector(mode = "numeric", length = 30)
IIC.Forgy = vector(mode = "numeric", length = 30)
IIC.MacQueen = vector(mode = "numeric", length = 30)

for (k in 1:30)
{
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "Hartigan-Wong")
  IIC.Hartigan[k] = groups$tot.withinss
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "Lloyd")
  IIC.Lloyd[k] = groups$tot.withinss
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "Forgy")
  IIC.Forgy[k] = groups$tot.withinss
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "MacQueen")
  IIC.MacQueen[k] = groups$tot.withinss
}
```

Este proceso es algo duradero con respecto al tiempo así que los resultados serán guardados en un objet para su posterior análisis.

```{r, eval=FALSE}
jambu.results = list(IIC.Hartigan = IIC.Hartigan, IIC.Lloyd = IIC.Lloyd,
                     IIC.Forgy = IIC.Forgy, IIC.MacQueen = IIC.MacQueen)
saveRDS(object = jambu.results, file = "../source/tuned_model/GFR/NN/KMEANS/jambu_results_9_features.rds")
```

Adicionalmente, se selecciona el mejor algoritmo de distancias para dos grupos y cinco grupos.

```{r, eval=FALSE}
measure.two = lapply(MeasuareKMeans(dataset, 2), max)
measure.five = lapply(MeasuareKMeans(dataset, 5), max)
```

De igual manera, a este proceso ser bastante largo, los resultados son exportados a un objeto para su posterior análisis.

```{r, eval=FALSE}
measures.results = list(measure.two = measure.two, measure.five = measure.five)
saveRDS(object = measures.results, file = "source/tuned_model/GFR/NN/KMEANS/measures_results_9_features.rds")
```

#####Análisis codo de Jambu
Empezaremos con el análisis del codo de Jambu preparando el ambiente de trabajo eliminando variables parciales y cargando el archivo de funciones.

```{r}
rm(list = ls())

#Cargando funciones
source("../source/functions/functions.R")
```

Luego, se carga el conjunto de enrenamiento y se eliminan etiquetas innecesarias.

```{r}
dataset = read.csv("../dataset/NSLKDD_Training_New.csv",
                   sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
```

Posteriormente, se cargan las características seleccionadas por GFR.

```{r}
nn.gfr = readRDS("../source/feature_selection/NN/results_GFR.rds")
nn.gfr = rownames(nn.gfr)[1:9]
```

Se debe crear el nuevo conjunto de datos. Para ello se extrae información importante, se transforman las variables predictoras a tipo numérico y se eliminan variables parciales.

```{r}
#Extrayendo información
Labels = dataset[, (ncol(dataset)-1):ncol(dataset)]
dataset = dataset[, nn.gfr]

#Transformando variables predictoras a tipo numérico
dataset = as.data.frame(apply(dataset, 2, as.numeric))
dataset.five = cbind(dataset, Label = Labels[,1])
dataset.two = cbind(dataset, Label = Labels[,2])
dataset = cbind(dataset, Label = Labels[,1])

#Eliminando variables parciales
remove(list = c("Labels"))
```

Posteriormente se escalan los conjuntos de datos generados para que tengan media cero y desviación estándar uno.

```{r}
dataset = ScaleSet(dataset)
dataset.two = ScaleSet(dataset.two)
dataset.five = ScaleSet(dataset.five)
```

Ahora que tenemos los conjuntos de datos listos, podemos anaizar los resulatdos del codo de Jambu. Iniciaremos por cargar los resultados obtenidos en la sección anterior y graficarlos.

```{r, fig.align="center"}
jambu.results = readRDS("../source/tuned_model/GFR/NN/KMEANS/jambu_results_9_features.rds")
plot(jambu.results$IIC.Hartigan, col = "blue", type = "b", pch = 19, main = "Codo de Jambu",
     xlab = "Número de Centroides", ylab = "Varianza", log = "y")
points(jambu.results$IIC.Lloyd, col = "red", type = "b", pch = 19)
points(jambu.results$IIC.Forgy, col = "green", type = "b", pch = 19)
points(jambu.results$IIC.MacQueen, col = "magenta", type = "b", pch= 19)
legend("topright", legend = c("Hartigan", "Lloyd", "Forgy", "MacQueen"),
       col = c("blue","red", "green", "magenta"), pch = 19)
```

Se puede apreciar que el codo de Jambu comparado con el obtendio haciendo uso total de las características se ve mucho más estable y este sugiere que con 4 grupos o centroides es el número ideal. Esto tiene sentido considerando que el número de registros de la clase U2R son muy escazos. Adicionalmente, con cualqueira de los algoritmos los resultados hasta los 5 grupos serán iguales, así que aparentemente nod ebemos preocuparnos por ese detalle. Sin embargo, veamos cual es el algoritmo que mejor desempeño presenta.

```{r}
measures.results = readRDS("../source/tuned_model/GFR/NN/KMEANS/measures_results_9_features.rds")
measures.results$measure.two
measures.results$measure.two[1]
measures.results$measure.five
measures.results$measure.five[1]
```

Tanto para el uso de dos centroides y de 5 centroides el mejor desempeño lo presenta el algoritmo Hartigan. Ahora comenzaremos con el análisis para K-Medias usando cinco grupos y dos grupos respectivamente.

##### K-Medias (cinco grupos)
Se empezará por ejecutar el algorimo 10 veces y ver cuales son los resulatdos obtenidos en cada iteración.

```{r}
results.five = vector(mode = "numeric", length = 10)
best.accuracy.five = 0
for (i in 1:length(results.five))
{
  set.seed(i)
  model.kmeans.five = kmeans(dataset.five[,-ncol(dataset.five)],
                             5, iter.max = 100)
  
  prediction.five = OrderKmeans(model.kmeans.five)
  accuracy.five = mean(prediction.five == dataset.five$Label)
  
  results.five[i] = accuracy.five
  
  if(best.accuracy.five < accuracy.five)
  {
    best.prediction.five = prediction.five
    best.accuracy.five = accuracy.five
  }
}
results.five * 100
mean(results.five) * 100
```

Se puede observar que la media es de 40% de acierto, y que la mayor tasa de aciertos estuvo alrededor del 77%. Estos resultados se ven deteriorados en comparación con el conjunto de datis completo

```{r}
confusion.matrix.five = table(Real = dataset.five$Label,
                              Prediction = best.prediction.five)
confusion.matrix.five
```

Esta queda muy desordenada, e incluso no presenta ningún acierto para la clase U2R. Adicionalmente se pueden observar muchos fallos en la clasificación entre los diferentes tipos de ataque. Veamos la tasa de acierto y de error.

```{r}
best.accuracy.five*100
ErrorRate(best.accuracy.five)*100
```

La tasa de acierto alcanzada es de 77.63%. Ahora veamos la tasa de acierto por etiqueta

```{r}
AccuracyPerLabel(confusion.matrix.five, dataset.five)
```

El rendimiento para las clases DoS y normal es bastane bueno. Para las clases Probing y R2L es regular y para U2R es nulo. Ahora veamos la matriz de confusión binaria

```{r}
attack.normal.confusion.matrix.five = AttackNormalConfusionMatrix(dataset.five,
                                                                  best.prediction.five)
attack.normal.confusion.matrix.five
```

Se ve la generación de muchos falsos positivos y poca generación de falsos negativos. Ahora veamos la tasa de acierto por etiqueta.

```{r}
AccuracyPerLabel(attack.normal.confusion.matrix.five, dataset.two)
```

Se detecta del 96% de los ataques. Esto respaldado por la alta genración de falsos positivos, deteriorando la clasificación del tráfico normal. Veamos el resto de las medidas de rendimiento.

```{r}
Accuracy(attack.normal.confusion.matrix.five) * 100
Sensitivity(attack.normal.confusion.matrix.five) * 100
Especificity(attack.normal.confusion.matrix.five) * 100
Precision(attack.normal.confusion.matrix.five) * 100
```


Con una tasa de 90.15% el modelo presenta muy buen acierto y unas medidas que indican un muy desempeño para la correcta clasificación de las diferentes clases presentes.

##### K-Medias (dos grupos)
Ahora hagamos el análisis para dos grupos. De igual manera ejecutaremos el algoritmo 10 veces para evaluar los resultados obtenidos en cada iteración.

```{r}
results.two = vector(mode = "numeric", length = 10)
best.accuracy.two = 0

for (i in 1:length(results.two))
{
  set.seed(i)
  model.kmeans.two = kmeans(dataset.two[,-ncol(dataset.two)],
                            2, iter.max = 100)
  
  prediction.two = OrderKmeans(model.kmeans.two)
  accuracy.two = mean(prediction.two == dataset.two$Label)
  
  results.two[i] = accuracy.two
  
  if(best.accuracy.two < accuracy.two)
  {
    best.prediction.two = prediction.two
    best.accuracy.two = accuracy.two
  }
}
results.two * 100
mean(results.two) * 100
```

Se observa una tasa promedio de 71.84% De igual manera se puede observar claramente como hay resultados que se repiten en varias ocasiones, es decir, que se alcanza el mismo mínimo local varias veces, situación que nos indica que con dos grupos se pueden alcanzar resultados sin mucha varianza. Veamos como queda la matriz de confusión para el mejor modelo obtenido. 

```{r}
confusion.matrix.two = table(Real = dataset.two$Label,
                             Prediction = best.prediction.two)
confusion.matrix.two
```

Se observa que se generan bastantes falsos negativos, muy pocos falsos negativos y que se mejoran los resultados obtenidos con cinco grupos. Ahora veamos la tasa de acierto y de error.

```{r}
best.accuracy.two*100
ErrorRate(best.accuracy.two)*100
```

La mejor tasa de aciertos es de 88.57%, mejora ampliamente a la obtenida con cinco conjuntos. Veamos la eficacia por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix.two, dataset.two)
```

Se observa un modelo que clasifica muy bien el tráfico normal y tiene una baja notable con respecto a la detección de los ataques, sin embargo es bastante alto el acierto con los mismos y mejora los resultados obtenidos con cinco grupos. Terminemos la evaluación calculando el resto de las métricas de rendimiento.

```{r}
Sensitivity(confusion.matrix.two) * 100
Especificity(confusion.matrix.two) * 100
Precision(confusion.matrix.two) * 100
```

Se observa un modelo con una alta tasa de aciertos sobre el tráfico normal, y con una lata confianza con respecto a los ataques detectados.

#####Conclusión
El codo de Jambu sigue siendo errático, sin embargo, se puede apreciar como de nuevo con dos centroides se logra la mayor tasa de aciertos. A su vez, Hartigan fue de nuevo el algoritmo seleccionado para el cálculo de las distancias.

#### SVM
En esta sección se hará uso de las 9 características seleccionadad por GFR para SVM. Empezaremos por preparar el ambiente de trabajo eliminando variables parciales y cargando el archivo de funciones correspondiente.

```{r, eval=FALSE}
rm(list = ls())
source("../source/functions/functions.R")
```

Ahora cargaremos el conjunto de datos de entrenamiento y eliminaremos las etiquetas que no se utilizarán.

```{r, eval=FALSE}
dataset = read.csv("dataset/NSLKDD_Training_New.csv", sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
```

A continuación se cargan las características seleccionadas por GFR para SVM y s ecrean los conjuntos de datos a utilizar.

```{r, eval=FALSE}
#Cargando características
svm.gfr = readRDS("source/feature_selection/SVM/results_GFR.rds")
svm.gfr = rownames(svm.gfr)[1:9]

#Extrayendo información
Labels = dataset[, (ncol(dataset)-1):ncol(dataset)]
dataset = dataset[, svm.gfr]

#Tranformando variables predictoras a tipo numérico
dataset = as.data.frame(apply(dataset, 2, as.numeric))
dataset.five = cbind(dataset, Label = Labels[,1])
dataset.two = cbind(dataset, Label = Labels[,2])
dataset = cbind(dataset, Label = Labels[,1])

#Eliminando variables parciales
remove(list = c("Labels"))
```

Por último, las variables predictoras son escaladas para que tengan media cero y desviación estándar uno.

```{r,eval=FALSE}
dataset = ScaleSet(dataset)
dataset.two = ScaleSet(dataset.two)
dataset.five = ScaleSet(dataset.five)
```

##### Codo de Jambu
Con el codo de Jambu se calcula la distancia intra-grupos para seleccionar la configuraicón de centroides que tenga mejor desempeño.

```{r, eval=FALSE}
IIC.Hartigan = vector(mode = "numeric", length = 30)
IIC.Lloyd = vector(mode = "numeric", length = 30)
IIC.Forgy = vector(mode = "numeric", length = 30)
IIC.MacQueen = vector(mode = "numeric", length = 30)

for (k in 1:30)
{
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "Hartigan-Wong")
  IIC.Hartigan[k] = groups$tot.withinss
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "Lloyd")
  IIC.Lloyd[k] = groups$tot.withinss
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "Forgy")
  IIC.Forgy[k] = groups$tot.withinss
  set.seed(k)
  groups = kmeans(dataset[,-ncol(dataset)], k, iter.max = 100, algorithm = "MacQueen")
  IIC.MacQueen[k] = groups$tot.withinss
}
```

Los resultados serán almacenados en una lista y posteriormente en un objeto para su posterior evaluación.

```{r, eval=FALSE}
jambu.results = list(IIC.Hartigan = IIC.Hartigan, IIC.Lloyd = IIC.Lloyd,
                     IIC.Forgy = IIC.Forgy, IIC.MacQueen = IIC.MacQueen)
saveRDS(object = jambu.results, file = "../source/tuned_model/GFR/SVM/KMEANS/jambu_results_9_features.rds")
```

Adicionalmente, se selecciona el mejor algoritmo de distancias para dos grupos y cinco grupos.

```{r, eval=FALSE}
measure.two = lapply(MeasuareKMeans(dataset, 2), max)
measure.five = lapply(MeasuareKMeans(dataset, 5), max)
```

Los resultados serán almacenados en una lista y posteriormente en un objeto para su análisis.

```{r, eval= FALSE}
measures.results = list(measure.two = measure.two, measure.five = measure.five)
saveRDS(object = measures.results, file = "../source/tuned_model/GFR/SVM/KMEANS/measures_results_9_features.rds")
```

##### Análisis codo de Jambu

Empezaremos con el análisis del codo de Jambu preparando el ambiente de trabajo eliminando variables parciales y cargando el archivo de funciones.

```{r}
rm(list = ls())

#Cargando archivo de funciones
source("../source/functions/functions.R")
```

A continuación se cargará el conjunto de datos de entrenamiento y se eliminan las etiquetas inncesarias.

```{r}
dataset = read.csv("../dataset/NSLKDD_Training_New.csv",
                   sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
```

Ahora se cargaran las características seleccionadas por GFR para SVM y se crearán los respectivos conjuntos de datos.

```{r}
#Cargando características
svm.gfr = readRDS("../source/feature_selection/SVM/results_GFR.rds")
svm.gfr = rownames(svm.gfr)[1:9]

#Extrayendo información
Labels = dataset[, (ncol(dataset)-1):ncol(dataset)]
dataset = dataset[, svm.gfr]

#Creando conjuntos de datos
dataset = as.data.frame(apply(dataset, 2, as.numeric))
dataset.five = cbind(dataset, Label = Labels[,1])
dataset.two = cbind(dataset, Label = Labels[,2])
dataset = cbind(dataset, Label = Labels[,1])

#Eliminando variables parciales
remove(list = c("Labels"))
```

Las variables predictoras de los diferentes conjuntos de datos serán escalados para que tengan media cero y desviación estándar uno.

```{r}
dataset = ScaleSet(dataset)
dataset.two = ScaleSet(dataset.two)
dataset.five = ScaleSet(dataset.five)
```

En este punto se tiene todo listo para empezar con el análisis del codo de Jambu.

```{r, fig.align="center"}
jambu.results = readRDS("../source/tuned_model/GFR/SVM/KMEANS/jambu_results_9_features.rds")
plot(jambu.results$IIC.Hartigan, col = "blue", type = "b", pch = 19, main = "Codo de Jambu",
     xlab = "Número de Centroides", ylab = "Varianza", log = "y")
points(jambu.results$IIC.Lloyd, col = "red", type = "b", pch = 19)
points(jambu.results$IIC.Forgy, col = "green", type = "b", pch = 19)
points(jambu.results$IIC.MacQueen, col = "magenta", type = "b", pch= 19)
legend("topright", legend = c("Hartigan", "Lloyd", "Forgy", "MacQueen"),
       col = c("blue","red", "green", "magenta"), pch = 19)
```

El codo de Jambu no aporta mucha información ya que sigue mostrando un comportamiento bastante errático. Sin embargo, con cuatro grupos pareciera que se alcanza la mejor medida, quizás por los pocos registros presentes de U2R.

Ahora veamos la mejor medida de rendimiento para dos y conco grupos.

```{r}
measures.results = readRDS("../source/tuned_model/GFR/SVM/KMEANS/measures_results_9_features.rds")
measures.results$measure.two
measures.results$measure.two[1]
measures.results$measure.five
measures.results$measure.five[1]
```

De nuevo Hartigan fue el seleccionado para ambos casos.

##### K-Medias (cinco grupos)
Para la evaluación con cinco grupos se correrá el algoritmos 10 veces.

```{r}
results.five = vector(mode = "numeric", length = 10)
best.accuracy.five = 0
for (i in 1:length(results.five))
{
  set.seed(i)
  model.kmeans.five = kmeans(dataset.five[,-ncol(dataset.five)],
                             5, iter.max = 100)
  
  prediction.five = OrderKmeans(model.kmeans.five)
  accuracy.five = mean(prediction.five == dataset.five$Label)
  
  results.five[i] = accuracy.five
  
  if(best.accuracy.five < accuracy.five)
  {
    best.prediction.five = prediction.five
    best.accuracy.five = accuracy.five
  }
}
```

Veamos los resultados y la media de la tasa de aciertos.

```{r}
results.five * 100
mean(results.five) * 100
```

Se observa un resultado bastante mediocre con una media de aciertos de apenas 29.84%. Veamos la matriz de confusión del mejor modelo obtenido.

```{r}
confusion.matrix.five = table(Real = dataset.five$Label,
                              Prediction = best.prediction.five)
confusion.matrix.five
```

Se observa una matriz de confusión bastante desordenada. Veamos la tasa de aciertos y la tasa de error.

```{r}
best.accuracy.five*100
ErrorRate(best.accuracy.five)*100
```

La mejor tasa de aciertos fue de 74.29%. Este resultado solo fue alcanzado en una iteración. Ahora veamos la tasa de acierto por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix.five, dataset.five)
```

Se observan buenas tasas de aciertos para las clases DoS, normal y R2L. Por otra parte, las otras clases tienen un desempeño bastante pobre. Ahora crearemos la matriz de confusión binaria para calcular otras medidas de rendimiento más interesantes.

```{r}
attack.normal.confusion.matrix.five = AttackNormalConfusionMatrix(dataset.five,
                                                                  best.prediction.five)
attack.normal.confusion.matrix.five
```

Se observa una alta generación de falsos positivos y una pequeña generación de falsos negativos. Veamos la tasa de acierto por etiqueta.

```{r}
AccuracyPerLabel(attack.normal.confusion.matrix.five, dataset.two)
```

97.86% de acierto para los ataques y 74.94% para la clase normal. De esta manera se justifica la alta generación de falsos positivos. Veamos ahora el resto de las medidas binarias.

```{r}
Accuracy(attack.normal.confusion.matrix.five) * 100
Sensitivity(attack.normal.confusion.matrix.five) * 100
Especificity(attack.normal.confusion.matrix.five) * 100
Precision(attack.normal.confusion.matrix.five) * 100
```

Con dos grupos se observa un desempeño bastante mayor donde la mayoría de los ataques son generados con una alta tasa de falsos positivos que deterioran el desempeño a la hora de clasificar el tráfico normal.

##### K-Medias (dos grupos)
El algoritmo se correrá 10 veces y se analizarán los resultados obtenidos.

```{r}
results.two = vector(mode = "numeric", length = 10)
best.accuracy.two = 0

for (i in 1:length(results.two))
{
  set.seed(i)
  model.kmeans.two = kmeans(dataset.two[,-ncol(dataset.two)],
                            2, iter.max = 100)
  
  prediction.two = OrderKmeans(model.kmeans.two)
  accuracy.two = mean(prediction.two == dataset.two$Label)
  
  results.two[i] = accuracy.two
  
  if(best.accuracy.two < accuracy.two)
  {
    best.prediction.two = prediction.two
    best.accuracy.two = accuracy.two
  }
}
```

Veamos los resultados y la media de la tasa de aciertos.

```{r}
results.two * 100
mean(results.two) * 100
```

Se observa una media bastante superior a la presentada con 5 clases. Adicionalmente, se observa que el resultado de 89.20% se repite en varias ocasiones. Es decir, el algoritmo converge a ese míinimo local multiples veces, lo que da una pista de que con dos grupos se pueden obtener resulatdos más constantes y mejores. Ahroa veamos la matriz de confusión.

```{r}
confusion.matrix.two = table(Real = dataset.two$Label,
                             Prediction = best.prediction.two)
confusion.matrix.two
```

Se observa un mejor desempeño con respecto a la correcta clasificación del tráfico normal. Sin emabrgo, se generan mayor cantidad de falsos negativos. Veamos la mejor tasa de aciertos y la tasa de error respectiva.

```{r}
best.accuracy.two*100
ErrorRate(best.accuracy.two)*100
```

La mejor tasa de aciertos supera claramente a la de 5 grupos. Ahora veamos la tasa de acierto por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix.two, dataset.two)
```

Se observa un muy buen desempeño para la correcta clasificación de los ataques y del tráfico normal. Veamos el resto de las medidas de rendimiento.

```{r}
Sensitivity(confusion.matrix.two) * 100
Especificity(confusion.matrix.two) * 100
Precision(confusion.matrix.two) * 100
```

Estas sugieren un modelo con muy buena eficacia a la hora de detectar ataques y tráfico normal.

##### Conclusión
La reducción de características afectó de manera drástica al algoritmo de K-Medias para 5 grupos. De esta manera se puede observar como dos conjuntos es de nuevo el número óptimo de centroides para el conjunto de datos reducido.

###(V) GFR - NN -K-Medias
En esta sección se hará uso del conjunto de datos reducido GFR para NN.

####Entrenamiento del modelo
Empezaremos con el entrenamiento del modelo. Primero se preparará el ambiente de trabajo eliminando variables parciales, cargando el paquete y el archivos de funciones correspondientes.

```{r, eval=FALSE}
rm(list = ls())

#Cargando paquete
library("nnet")

#Cargando archivo de funciones
source("../source/functions/functions.R")
```

A continuación se carga el conjunto de entrenamiento y se eliminan las etiquetas innecesarias.

```{r, eval=FALSE}
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
dataset = dataset.training
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Todas las variables predictoras deben ser de tipo numérico, y la variable objetivo debe ser de tipo *factor*. Adicionalmente, las variables predictoras debben ser escaladas para tener media cero y desviación estándar uno.

```{r, eval=FALSE}
for (i in 1 : (ncol(dataset) -1) )
  dataset[,i] = as.numeric(dataset[,i])

dataset[,ncol(dataset)] = as.factor(dataset[,ncol(dataset)])
dataset = ScaleSet(dataset)
```

Seleccionaremos las 9 características más relevantes seleccionadas por GFR para NN y crearemos el nuevo conjunto de datos.

```{r, eval=FALSE}
nn.gfr = readRDS("../source/feature_selection/NN/results_GFR.rds")
nn.gfr = rownames(nn.gfr)[1:9]

#Extrayendo información
Label = dataset$Label

#Creando nuevo conjunto de datos
dataset = dataset[, nn.gfr]
dataset = cbind(dataset, Label = Label)
```

En este punto se puede empezar con las labores de validación cruzada de 10 conjunto diviendo el conjunto de datos original en 10 sub-conjuntos.

```{r, eval=FALSE}
cv.sets = CVSet(dataset, k = 10, seed = 22)
length(cv.sets)
```

Adicionalmente se inicializarán algunas variables que nos ayuden a almacenar los resultados obtenidos durante el proceso.

```{r, eval=FALSE}
results = vector(mode = "numeric", length = 10)
list.results = list(0, 0, 0, 0)
names(list.results) = c("results", "best_model", "best_testing_set", "best_predictions")
best.accuracy = 0
```

Se cargarán los parámetros seleccionados correspondientes al número de neuronas seleccionados.

```{r}
hidden.neurons = readRDS("../source/parameter_selection/NN/GFR/tuned_model.rds")
hidden.neurons = hidden.neurons$best.parameters$size
hidden.neurons
```

El número de neuronas seleccionado fue de 28 y este será el utilizado para el entrenamiento del modelo. Ahora podemos realizar el proceso de validación cruzada de 10 conjuntos.

```{r, eval=FALSE}
for (i in 1:10)
{
  #Extracting sets
  testingset = as.data.frame(cv.sets[[i]])
  trainingset = cv.sets
  trainingset[[i]] = NULL
  trainingset = do.call(rbind, trainingset)
  
  #NN Model
  model = nnet(Label ~ .,
               data = trainingset,
               size = hidden.neurons,
               maxit = 100)
  
  #Making predictions
  predictions = predict(model, testingset[, 1:(ncol(testingset)-1)], type = "class")
  
  
  #Calculating accuracy
  accuracy = mean(testingset[, ncol(testingset)] == predictions)
  #Storing results
  results[i] = accuracy
  
  #Storing best results
  if(best.accuracy < accuracy)
  {
    list.results$best_model = model
    list.results$best_testing_set = testingset
    list.results$best_predictions = predictions
    best.accuracy = accuracy
  }
}
```

Este proceso es bastante largo en tiempo, es por ello que que los resultados serpan exportados a un objeto para su posterior análisis.

```{r, eval=FALSE}
list.results$results = results
saveRDS(list.results, "../source/tuned_model/GFR/NN/training_set/list_results.rds")
```

####Evaluación del modelo
Ahora pasaremos a la evaluación del modelo obtenido en la sección anterior. Empezaremos por eliminar variables parciales, cargar el paquete a utilizar y el archivos de funciones correspondiente.

```{r}
rm(list = ls())

#Cargando paquetes
library("nnet")

#Cargando archivo de funciones
source("../source/functions/functions.R")
```

A continuación se carga el archivo que contiene los resultados del paso anterior y veremos los resultados obtenidos.

```{r}
list.results = readRDS("../source/tuned_model/GFR/NN/training_set/list_results.rds")
list.results$results * 100
mean(list.results$results) * 100
```

Se observa una media bastante alta de 99.04%, veamos la matriz de confusión del mejor modelo obtenido.

```{r}
confusion.matrix = table(Real = list.results$best_testing_set[,ncol(list.results$best_testing_set)],
                         Prediction = list.results$best_predictions)
confusion.matrix
```

Se observa una matriz de confusión bastante buena, sin embargo, se puede apreciar que hay mayor cantidad de errores que usando el conjunto de datos original. Veamos las tasas de acierto y de error.

```{r}
accuracy = mean(list.results$best_testing_set[,ncol(list.results$best_testing_set)] == 
                  list.results$best_predictions)

accuracy * 100
ErrorRate(accuracy) * 100
```

El mejor modelo obtuvo una tasa de aciertos de 99.15%. Es bastante alta, veamos la eficacia por etiquetas.

```{r}
AccuracyPerLabel(confusion.matrix, list.results$best_testing_set)
```

Un muy desempeño para la mayoría de las etiquetas, salvo para U2R, que no acertó ninguna clasificación de este tipo. Crearemos una matriz binaria para calcular diferentes medidas de rendimeinto.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(list.results$best_testing_set,
                                                             list.results$best_predictions)
attack.normal.confusion.matrix
```

Se puede apreciar una baja cantidad de falsos positivos y falsos negativos. En general los registros fueron muy bien clasificados. Veamos las medidas binarias.

```{r}
Accuracy(attack.normal.confusion.matrix) * 100
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

El modelo creado es muy preciso a la hora de clasificar los ataques y el tráfico normal. Veamos el comportamiento del modelo a la hora de tomar estas decisiones graficando la respectiva curva ROC.

```{r, fig.align="center"}
probabilities = predict(list.results$best_model,
                        list.results$best_testing_set[, 1:(ncol(list.results$best_testing_set)-1)])

roc.data = DataROC(list.results$best_testing_set, probabilities,
                   list.results$best_predictions)
generate_ROC(scores = roc.data$Prob, real = roc.data$Label,
             pred = roc.data$Prediction)
```

Se observa un resultado de curva ROC mejor que el presentado por PCA y el del conjunto de datos original. Acumulando más del 80% de los aciertos con una alta tasa de certeza en la toma de decisiones.

####Segundo nivel de clasificación (K-Medias)
Ahora añadiremos el segundo nivel de clasificación correspondiente a K-Medias. Para eso empezaremos por crear el conjunto de datos a utilizar, seleccionado aquellos registros seleccionados como normal.

```{r}
kmeans.set = list.results$best_testing_set[list.results$best_predictions == "normal",]
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

Se ven los 35 falsos negativos generados en la sección previa. Precalcularemos los centroides para maximizar la convergencia del modelo al mejor mínimo local.

```{r}
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)
```

Y utilizaremos los centroides para clasificar.

```{r}
matrix.centers = matrix.centers/100
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)
```

Posteriormente, ordenaremos las predicciones realizadas y visualizaremos la matriz de confusión obtenida.

```{r}
predictions = OrderKmeans(kmeans.model)
confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)
confusion.matrix.kmeans.model
```

Se detectaron 11 nuevos ataques, sin embargo, se agregaron 2122 falsos positivos que deterioran el trabajo realizado por el primer nivel de NN. Veamos la tasa de acierto y de error obtenidas.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)*100
```

Se observa un pobre desempeño, con solo el 51.7% de acierto en los resultados. Veamos la tasa de acierto por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

El desempeño es bastante malo para los ataques y el tráfico normal, el deterioro de k-Medias en este caso es el peor presentado hasta el momento.Veamos el resto de las medidas de rendimiento.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

El resto de las medidas de rendimiento refleja lo pobre del modelo para clasificar, en especial los ataques y lo poco confiable que son sus predicciones en dicho caso.

####Estadísticas totales
Combinemos las estadísticas totales obtenidas por ambos niveles de clasificación. Empezaremos por unificar las matrices de confusión.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix,
                                          confusion.matrix.kmeans.model)

confusion.matrix.two.labels
```

Se observa que el segundo nivel deterioró notablemente el desempeño obtrendio por el primer nivel. Veamos la tasa de acierto y la tasa de error.

```{r}
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
```

Se decrementó notablemente la tasa de acierto comparado con la presentada por el primer nivel de clasificación. Veamos el resto de las medidas de clasificación.

```{r}
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
```

Se observa que el modelo clasifica la mayoría de los ataques presentes, sin embargo, la certeza con que lo hace es baja y no se comporta de buena manera para la clasificación del tráfico normal.

####Conclusiones
La reducción de características funciona de buena manera para el primer nivel, sin embargo, parece ser que se necesita mayor número de variables predictoras, debido a que K-Medias deteriora notablemente el rendmiendo del modelo generado en el primer nivel. Este comportamiento no se veía haciendo uso del conjunto de datos por defecto.

###(VI) GFR - SVM -K-Medias
En esta sección se harpa uso del conjunto reducido de datos GFR para el algoritmo SVM. Adicionalmente se hará uso de los parámetros seleccionados para dicho conjunto de datos.

####Entrenamiento del modelo
Empezaremos las labores preparando el ambiente de trabajo eliminando variables parciales, cargando el paquete a utilizar y el archivo de funciones correspondiente.

```{r, eval=FALSE}
rm(list = ls())

#Cargando paquete
library("e1071")

#Cargando archivo de funciones
source("../source/functions/functions.R")
```

Ahora, cargaremos el conjunto de entrenamiento y eliminaremos las etiquetas que no serán utilizadas.

```{r, eval=FALSE}
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
dataset = dataset.training
dataset$Label_Normal_TypeAttack = NULL
dataset$Label_Num_Classifiers = NULL
dataset$Label_Normal_or_Attack = NULL
```

Las variables predictoras deben ser de tipo numérico y la variable objetivo de tipo *factor*. Adicionalmente, el conjunto de datos debe ser escalado para que tengan media cero y desviación estpandar uno.

```{r, eval=FALSE}
for (i in 1 : (ncol(dataset) -1) )
  dataset[,i] = as.numeric(dataset[,i])

dataset[,ncol(dataset)] = as.factor(dataset[,ncol(dataset)])

#Escalando el conjunto de datos
dataset = ScaleSet(dataset)
```

A continuación, se extraen las 9 características seleccionadas por GFR, y se crea el nuevo conjunto de datos.

```{r, eval=FALSE}
svm.gfr = readRDS("../source/feature_selection/SVM/results_GFR.rds")
svm.gfr = rownames(svm.gfr)[1:9]

#Extrayendo información
Label = dataset$Label

#Creando el nuevo conjunto de datos
dataset = dataset[, svm.gfr]
dataset = cbind(dataset, Label = Label)
```

Se divide el conjunto de datos creado en 10 sub-conjuntos para el proceso de validación cruzada de 10 conjuntos.

```{r, eval=FALSE}
cv.sets = CVSet(dataset, k = 10, seed = 22)
length(cv.sets)
```

Se inicializan algunas variables para almacenar los resultados obtenidos durante el proceso mencionado.

```{r, eval=FALSE}
results = vector(mode = "numeric", length = 10)
list.results = list(0, 0, 0, 0)
names(list.results) = c("results", "best_model", "best_testing_set", "best_predictions")
best.accuracy = 0
```

Se cargan los parámetros seleccionados.

```{r, eval=FALSE}
tuned.parameters = readRDS("../source/parameter_selection/SVM/GFR/tuned_model.rds")
tuned.cost = tuned.parameters$best.parameters$cost
tuned.gamma = tuned.parameters$best.parameters$gamma
tuned.cost
tuned.gamma
```

Estos corresponden a *cost* = 6 y a *gamma* = 0.4, sugiriendo abrir el espectro del área de los vectores de soporte y permitiendo mayor cantidad de error. Ahora si se da inicio al proceso de validación cruzada de 10 conjuntos.

```{r, eval=FALSE}
for (i in 1:10)
{
  #Extracting sets
  testingset = as.data.frame(cv.sets[[i]])
  trainingset = cv.sets
  trainingset[[i]] = NULL
  trainingset = do.call(rbind, trainingset)
  
  #SVM Model
  model = svm(Label ~ .,
              data = trainingset,
              kernel = "radial",
              cost = tuned.cost,
              gamma = tuned.gamma,
              scale = FALSE,
              probability = TRUE)
  
  #Making predictions
  predictions = predict(model, testingset[, 1:(ncol(testingset)-1)], type = "class")
  
  
  #Calculating accuracy
  accuracy = mean(testingset[, ncol(testingset)] == predictions)
  #Storing results
  results[i] = accuracy
  
  #Storing best results
  if(best.accuracy < accuracy)
  {
    list.results$best_model = model
    list.results$best_testing_set = testingset
    list.results$best_predictions = predictions
    best.accuracy = accuracy
  }
}
```

Este proceso es bastante largo en tiempo y por eso los resultados serán exportados a un objeto para su posterior análisis.

```{r, eval=FALSE}
list.results$results = results
saveRDS(list.results, "../source/tuned_model/GFR/SVM/training_set/list_results.rds")
```

####Evaluación del modelo
Para la evaluación del modelo obtenido en la sección anterior empezaremos por eliminar variables parciales, cargar el paquete a utilizar y el archivo de funciones correspondiente.

```{r}
rm(list = ls())

#Cargando paquete
library("e1071")

#Cargando archivo de funciones
source("../source/functions/functions.R")
```

Cargamos los resultados obtenido e imprimimos la media obtenida.

```{r}
list.results = readRDS("../source/tuned_model/GFR/SVM/training_set/list_results.rds")

list.results$results
mean(list.results$results) * 100
```

Se observa una media de aciertos muy alta cuyo valor es de 99.28%. Veamos lamatriz de confusión del mejor modelo obtenido.

```{r}
confusion.matrix = table(Real = list.results$best_testing_set[,ncol(list.results$best_testing_set)],
                         Prediction = list.results$best_predictions)
confusion.matrix
```

Se observa una matriz de confusión bastante ordenada con pocos elementos fuera de la diagonal. Veamos la tasa de acierto y la tasa de error.

```{r}
accuracy = mean(list.results$best_testing_set[,ncol(list.results$best_testing_set)] == 
                  list.results$best_predictions)

accuracy * 100
ErrorRate(accuracy) * 100
```

El mejor modelo obtuvo una tasa de aciertos de 99.37%. Ahora veamos la tasa de acierto por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix, list.results$best_testing_set)
```

Para las clases DoS, normal, Probing y R2L la tasa de aciertos es bastante alto. Por otra parte, la clase U2R, vuelve a presentar un pobre desempeño debido a la escaza cantidad de registros que posee en el entrenamiento. Crearemos una matriz binaria para poder sacar otras medidas de rendimiento interesantes.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(list.results$best_testing_set,
                                                             list.results$best_predictions)
attack.normal.confusion.matrix
```

Se observan solo 37 errores en la clasificación son 15 falsos negativos y 22 falsos positivos. Veamos la nueva tasa de acierto y de error en conjunto con las nuevas medidas de rendimiento.

```{r}
Accuracy(attack.normal.confusion.matrix) * 100
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

Las medidas de rendimiento reflejan un modelo muy bueno tanto para clasificar ataques como tráfico normal, veamos la certeza con que el modelo toma sus decisiones haciendo uso de la curva ROC.

```{r, fig.align="center"}
probabilities = attr(predict(list.results$best_model,
                             list.results$best_testing_set[, 1:(ncol(list.results$best_testing_set)-1)],
                             probability = TRUE), "probabilities")

roc.data = DataROC(list.results$best_testing_set, probabilities,
                   list.results$best_predictions)
generate_ROC(scores = roc.data$Prob, real = roc.data$Label,
             pred = roc.data$Prediction)
```

El comportamiento es mejor que el del conjunto de datos por defecto, sin embargo, es mucho más errático que el presentado por NN. Este comportamiento indica que con altos niveles de certaza, el modelo comete bastantes errores.

####Segundo nivel de clasificación (K-Medias)
Añadiremos el segundo nivel de clasificación correspondiente a K-Medias. Empezaremos por crear el nuevo conjunto de datos extrayendo los registros que fueron clasificados como normal y creando el nuevo conjunto de datos a utilizar.

```{r}
kmeans.set = list.results$best_testing_set[list.results$best_predictions == "normal",]
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

Se puede observar qu están presentes los 15 falsos negativos. El objetivo e spoder detectar dichos ataques. Ppara maximizar la eficacia de K-Medias precalcularemos los centroides.

```{r}
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)
```

Y usaremos los centroides obtenidos para poder clasificar.

```{r}
matrix.centers = matrix.centers/100
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)
```

Ahora veamos la matriz de confusión obtenida.

```{r}
predictions = OrderKmeans(kmeans.model)
confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)
confusion.matrix.kmeans.model
```

No se detect+o ningún nuevo ataque, y se agregó una cantidad considerable de falsos positivos. Este rendimeinto es mejor comparado al de NN, sin embargo, empeora la situación con respecto al uso del conjunto de datos original. veamos la tasa de acierto y de error.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)
```

Se logra un 80% de acierto correspondiente a la correcta claificación de los registros normales que corresponde a la mayoría de los registros. Ahora veamos la eficacia por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

Evidentemente se logra un 0% para la detección de los ataques. Veamos el resto de las medidas binarias.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

Las medidas reflejan que el modelo tiene un desempeño aceptable para la clasificación del tráfico normal, sin embargo, un pobre rendimiento a la hora de detectar ataques.

####Estadísticas totales
Uniremos los resultados correspondientes a los dos niveles presentados con aterioridad. Iniciaremos combinando las matrices de confusión.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix,
                                          confusion.matrix.kmeans.model)

confusion.matrix.two.labels
```

Se observa que se agregaron mayor cantidad de falsos positivos, ilustrando que K-Medias deterioró el trabajo realizado por SVM. Terminemos de ver el resto de las medidas de rendimiento.

```{r}
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
```

Se observa que el modelo clasifica la mayoría de los ataques, sin embargo, el rendimiento conjunto es menor que al rendmiento individual presentado por SVM.

####Conclusiones
La reducción de características funcionó de manera positiva para el primer de nivel de SVM, mejorando el desempeño de la curva ROC. Sin embargo, para el segundo nivel de K-Medias, deterioró el desempeño. Este comportamiento se puede deber a la pérdida de información producto de la eliminación de variables predictoras.

###Conclusiones generales
La reducción de características GFR funciona de manera grata para el primer nivel, sin embargo, deteriora notablemente el desempeño del primer nivel. Se necesita de mayor cantidad de variables predictoras para que k-Medias no deteriore el desempeño del primer nivel de los modelos correspondientes a Nn y a SVM.

##Análisis sobre el conjunto de prueba
En esta sección se utilizará el conjunto de entrenamiento completo para entrenar los modelos y se evaluarán los diferentes modelos haciendo uso del conjunto de prueba. Los diferentes conjuntos serán adaptados a PCA y a GFR en en los esquemas II, IV, V y VI. Y solo se hará uso de los conjuntos de datos originales tanto para entrenamiento como para prueba en los esquemas I y II. Es importante resaltar el hecho de que usarán los parámetros seleccionados para cada esquema. En esta sección todos los pasos concernientes al entrenamiento y prueba del modleo serán cronometrados para poder comparar dichos tiempos entre los diferentes esquemas y modelos creados.

### (I) NN - K-Medias
En esta sección se hará el entrenamiento y el análisis del esquema híbrido NM - K-Medias (dos conjuntos).

####Entrenamiento del modelo
Se empezarán las tareas eliminando variables parciales, cargando el paquete a utilziar y el archivo de funciones correspondiente.

```{r, eval=FALSE}
rm(list = ls())

#Cargando paquetes
library("nnet")

#Cargando archivo de funciones
source("../source/functions/functions.R")
```

Se caragará el conjunto de datos de entrenamiento y se eliminarán las etiquetas que no se utilizarán.

```{r, eval=FALSE}
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)

#eliminando etiquetas innecesarias
dataset.training$Label_Normal_TypeAttack = NULL
dataset.training$Label_Num_Classifiers = NULL
dataset.training$Label_Normal_or_Attack = NULL
```

Las variables predictoras deben ser de tipo numérico, la variable objetivo de tipo factor y adicionalmente las variables predictoras deben estar escaladas para que todas tengan media ceor y desviación estándar uno.

```{r, eval=FALSE}
for (i in 1 : (ncol(dataset.training) -1) )
  dataset.training[,i] = as.numeric(dataset.training[,i])

dataset.training[,ncol(dataset.training)] = as.factor(dataset.training[,ncol(dataset.training)])

#Escalando el conjunto de datos
dataset.training = ScaleSet(dataset.training)
```

Se cargarán los mejores parámetros seleccionados durante la selección de parámetros.

```{r}
hidden.neurons = readRDS("../source/parameter_selection/NN/original_set/tuned_model.rds")
hidden.neurons = hidden.neurons$best.parameters$size
hidden.neurons
```

El número de neuronas seleccionado para el conjunto de entrenamiento completo fue de 21 neuronas, este número es el que será pasado como parámetro. A continuación se inicia con el entrenamiento del modelo

```{r, eval=FALSE}
#Tomando el tiempo de inicio
start.time = Sys.time()

#Entrenando el modelo
set.seed(22)
model = nnet(Label ~ .,
             data = dataset.training,
             size = hidden.neurons,
             maxit = 100)

#Guardando el tiempo total
total.time = Sys.time() - start.time
```

Los resultados serán almacenados en una lista, y posteriormente en un objeto para su posterior análisis.

```{r, eval=FALSE}
list.results = list(total.time, model)
saveRDS(list.results, file = "../source/tuned_model/original_set/NN/testing_set/list_results.rds")
```

####Análisis del modelo
Empezaremos con el análisis del modelo creado en la sección anterior. Eliminaremos variables parciales, cargaremos el paquete a utilizar y el archivo de funciones correspondiente.

```{r}
rm(list = ls())

#Cargando paquetes
library("nnet")

#Cargando funciones
source("../source/functions/functions.R")
```

Seguidamente cargaremos el conjunto de prueba, eliminaremos variables parciales y lo escalaremos para que todas las variables predictoras tengan media cero y desviación estándar uno.

```{r}
#Cargando el conjunto de datos
testing.set = read.csv("../dataset/NSLKDD_Testing_New.csv",
                       sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
testing.set$Label_Normal_TypeAttack = NULL
testing.set$Label_Num_Classifiers = NULL
testing.set$Label_Normal_or_Attack = NULL

#Escalando el conjunto de datos
testing.set = ScaleSet(testing.set)
```

Se cargan los resultados y se almacenan en variables para ser utilizadas posteriormente.

```{r}
#Cargando resultados
results = readRDS("../source/tuned_model/original_set/NN/testing_set/list_results.rds")

#Extrayendo resultados
training.time = results[[1]]
model = results[[2]]
```

A continuación se harán las predicciones para NN, este tiempo como se mencionó previamente, será cronometrado.

```{r}
#Inicializando el contador
start.time.predictions = Sys.time()

#Realizando predicciones
predictions = predict(model, testing.set[, 1:(ncol(testing.set)-1)], type = "class")

#Capturando el tiempo total
total.time.predictions = Sys.time() - start.time.predictions
total.time.predictions
```

Ahora veamos los resultados, empecemos por ver la matriz de confusión.

```{r}
confusion.matrix = table(Real = testing.set[,ncol(testing.set)],
                         Prediction = predictions)

confusion.matrix
```

La matriz de confusión está mucho más desordenada que en el análisis sobre el conjunto de entrenamiento, este comportamiento es natural considerando que se agregaron 14 nuevos tipos de ataques. Ahora veamos la tasa de error y de acierto.

```{r}
accuracy = mean(testing.set[,ncol(testing.set)] == predictions)
accuracy * 100
ErrorRate(accuracy) * 100
```

Se alcanza una tasa de acierto de 64.13%. Este resultado bastante discreto y por debajo del análisis sobre el conjunto de prueba haciendo uso de los parámetros por defecto. Ahora veamos la tasa de aciertos por etiquetas.

```{r}
AccuracyPerLabel(confusion.matrix, testing.set)
```

Se obtiene un muy buen resultado a la hora de clasificar el tráfico normal. Se observa un bajón notable para la otra clase dominante que corresponde a DoS. Por otra parte Probing está bien representado, y las clases R2L y U2R tuvieron un pobre rendimiento. A continuación se creará una matriz de confusión binaria para calcular otro tipo de medidas de rendimiento más interesantes.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(testing.set, predictions)
attack.normal.confusion.matrix
```

Se puede observar que el problema es la alta generación de falsos negativos por parte del modelo. Los falsos positivos presentes representan una baja proporción del total de los registros. Veamos las medidas de rendimiento.

```{r}
Accuracy(attack.normal.confusion.matrix) * 100
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

Para la tasa de aciertos, la tasa de aciertos fue de 76.20%. Por otra parte, se detectaron el 63% de los ataques y la certeza con que los ataques son detectados es de 92%. La especificidad de 91.72% indica que el modelo es bastante beuno identificando el tráfico normal. Ahora veamos la certeza de la toma de deicisiones del modelo haceindo uso de la curva ROC.

```{r, fig.align="center"}
#Calculando probabilidades
probabilities = predict(model, testing.set[, 1:(ncol(testing.set)-1)])

#Generando la curva ROC
roc.data = DataROC(testing.set, probabilities, predictions)
generate_ROC(roc.data$Prob, roc.data$Label, roc.data$Prediction)
```

El desempeño es muy malo, mucho peor que el presentado haciendo uso de los parámetros por defecto. Esto queire decir que el problema es muy cambiante y que al ajustar lso parámetros para el conjunto de datos de entrenamiento se está perdiendo generalidad a la hora de detectar nuevos tipos de ataques.

####Segundo nivel de clasificación (k-Medias)
A continuación añadiremos el segundo nivel de clasificación correspondiente a k-Medias (dos conjuntos). Empezaremos por extraer las predicciones hechas como pertenecientes a la clase normal y crear un nuevo conjunto de datos para ser utilizado por el algoritmo.

```{r}
kmeans.set = testing.set[predictions == "normal", ]
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

Se puede apreciar que hay 4659 ataques presentes en este nuevo sub-conjunto. Ahora se precalcularán los centroides, estos se utilizarán para hacer el proceso de clasificación. Este proceso también será cronometrado y será agregado luego al tiempo total de evaluación.

```{r}
#Pre-calculando centroides
start.time.kmeans.training = Sys.time()
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)
matrix.centers = matrix.centers/100
total.time.kmeans.training = Sys.time() - start.time.kmeans.training
```

A continuación se realiza la clasificación, también este proceso será cronometrado.

```{r}
start.time.kmeans.predictions = Sys.time()
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)

total.time.kmeans.predictions = Sys.time() - start.time.kmeans.predictions
```

Ahora veamos los resultados obtenidos. Empezaremos por visualizar la matriz de confusión.

```{r}
#Ordenando predicciones
predictions = OrderKmeans(kmeans.model)

#Creando matriz de confusión
confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)

#Imprimiendo matriz de confusión
confusion.matrix.kmeans.model
```

Se puede observar que el trabajo realizado pro K-Medias fue bastante bueno identificando 3108 nuevos ataques y reduciendo la cantidad de errores en las predicciones. Ahora veamos la atasa de acierto y de error obtenida.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)*100
```

Una tasa de aciertos bastante alta con 80.53% de acierto. Ahora veamos la tasa de aciertos por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

Se detectó el 66.71% de los ataque spresentes y el 87.68% del tráfico normal. Veamos el resto de las medidas.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

Estas indican que el modelo tiene una alta certeza con la identificación de ataques y una alta tasa de aciertos en al clasificación del tráfico normal.

####Estadísticas totales
Se unificarán las matrices de confusión obtenidas.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix, confusion.matrix.kmeans.model)
confusion.matrix.two.labels
```

Como mejoras se observa un incremento en los ataques detectados y un decremento en los falsos negativos. Por otra parte, como puntos negativos se observa un decremento de acierto en el tráfico normal y un incremento de falsos positivos. Sin embargo, la mejora es mayor que el decremento. Veamos el resto de las medidas de rendimiento.

```{r}
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
total.time.predictions + total.time.kmeans.predictions
training.time + total.time.kmeans.training
```

Se observa uan tasa de aciertos bastante buena de 85%, mostrando que las predicciones tomadas sobre la detección de los ataques es alta. El punto débil está asociado al pobre rendimiento presentado por el primer nivel, la curva ROC indica que la certeza de las decisiones tomadas no fue alta, e incluso se podría pensar que fue al azar. Por último se pueden observar los tiempos finales de análisis y de entrenamiento.

####Conclusiones
Con la selección de paŕametros se perdió precisión por parte del primer nivel de NN. Al haber más falsos negativos, K-Medias realizó un mejor trabajo. Por otra parte, el rendimiento de la curva ROC de NN fue muy pobre.

###(II) SVM - K-Medias
En esta sección se hará el entrenamiento y análisis de esquema híbrido SVM - K-Medias (dos conjuntos).

####Entrenamiento del modelo
Empezaremos el entrenamiento del modelo eliminando variables parciales , cargando el paquete a utilizar y cargando el archivo de funciones correspondiente.

```{r, eval=FALSE}
rm(list = ls())

#Cargando paquetes
library("e1071")

#Cargando funciones
source("../source/functions/functions.R")
```

Se cargará el conjunto de entrenamiento y se eliminarán las etiquetas que no serán utilizadas.

```{r, eval=FALSE}
dataset.training = read.csv("dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
dataset.training$Label_Normal_TypeAttack = NULL
dataset.training$Label_Num_Classifiers = NULL
```

Las variables predictoras deben ser transformadas a tipo numérico y la objetivo a tipo factor. Adicionalmente, las variables predictoras deben ser escaladas para que tengan media cero y desviación estándar uno.

```{r, eval=FALSE}
for (i in 1 : (ncol(dataset.training) -1) )
  dataset.training[,i] = as.numeric(dataset.training[,i])

dataset.training[,ncol(dataset.training)] = as.factor(dataset.training[,ncol(dataset.training)])

#Escalando el conjunto de datos
dataset.training = ScaleSet(dataset.training)
```

Se cargarán los parámetros seleccionados.

```{r}
tuned.parameters = readRDS("../source/parameter_selection/SVM/original_set/tuned_model.rds")
tuned.cost = tuned.parameters$best.parameters$cost
tuned.gamma = tuned.parameters$best.parameters$gamma
tuned.cost
tuned.gamma
```

Estos son *cost* = 6 y *gamma* = 0.08. Apuesta por mayor margen de error y mayor espectro de las zonas clasificadoras en comparación con los parámetros por defecto. Todos los pasos relacionados al entrenamiento y prueba del modelo serán cronometrados.

```{r, eval=FALSE}
#Inicio del tiempo de entrenamiento
start.time = Sys.time()

set.seed(22)
model = svm(Label~.,
            data = dataset.training,
            kernel = "radial",
            cost = tuned.cost,
            gamma = tuned.gamma,
            scale = FALSE,
            probability = TRUE)

#Tiempo total de entrenamiento
total.time = Sys.time() - start.time
```

Los resultados serán almacenados en una lista y posteriormente en un objeto para su posterior análisis.

```{r, eval=FALSE}
list.results = list(total.time, model)
saveRDS(list.results, file = "../source/tuned_model/original_set/SVM/testing_set/list_results.rds")
```

####Análisis del modelo
Se iniciará con el análisis del modelo eliminando variables parciales, cargando el paquete a utilizar y cargando el archivo correspondiente de funciones.

```{r}
rm(list = ls())

#Cargando paquetes
library("e1071")

#Cargando funciones
source("../source/functions/functions.R")
```

A continuación se cargará el conjunto de datos de prueba y se eliminarán las etiquetas que no se utilizarán. Adicionalmente, serán escaladas las variables predictoras para que tengan media cero y desviación estándar uno.

```{r}
#Cargando conjunto de prueba
testing.set = read.csv("../dataset/NSLKDD_Testing_New.csv",
                       sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
testing.set$Label_Normal_TypeAttack = NULL
testing.set$Label_Num_Classifiers = NULL
testing.set$Label_Normal_or_Attack = NULL

#Escalando el conjunto de datos
testing.set = ScaleSet(testing.set)
```

Luego se cargan los resultados obtenidos en la sección anterior y se guardan en variables para su posterior uso.

```{r}
results = readRDS("../source/tuned_model/original_set/SVM/testing_set/list_results.rds")

#Extrayendo resultados
training.time = results[[1]]
model = results[[2]]

```

El proceso de predicciones será cronometrado.

```{r}
#Inicializando el tiempo
start.time.predictions = Sys.time()

#Realizando las predicciones
predictions = predict(model, testing.set[, 1:(ncol(testing.set)-1)], type = "class")

#Capturando el tiempo total
total.time.predictions = Sys.time() - start.time.predictions
total.time.predictions
```

Una vez realizadas las predicciones se puede observar la matriz de confusión.

```{r}
confusion.matrix = table(Real = testing.set[,ncol(testing.set)],
                         Prediction = predictions)

confusion.matrix
```

Se observa una matriz de confusión más desordenada que la presentada durante el análisis sobre el conjunto de entrenamiento. Esta situación es entendible considerando que hay 14 nuevos tipos de ataques presentes en el conjunto de prueba. Ahora veamos la tasa de acierto y de error.

```{r}
accuracy = mean(testing.set[,ncol(testing.set)] == predictions)
accuracy * 100
ErrorRate(accuracy) * 100
```

Se muestra una tasa de acierto de 74%. Muy superior a la presentada por NN. Veamos la eficacia por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix, testing.set)
```

El modelo clasifica de muy buena manera las clases DoS y normal. Sin embargo, para Probing tiene un rendimiento muy discreto y para R2L y U2R nulo. Crearemos una matriz de consufión binaria que permita derivar nuevas medidas de rendimiento.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(testing.set, predictions)
attack.normal.confusion.matrix
```

Se observa una alta generación de falsos negativos y una generación mínima de falsos negativos. Veamos el resto de las medidas de rendimiento.

```{r}
Accuracy(attack.normal.confusion.matrix) * 100
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

Las medidas reflejan un modelo con alto acierto a la hora de clasificar el tráfico normal, y una alta precisión con respecto a los ataque identificados. Veamos la curva ROC para ver el comportamiento de la toma de decisiones.

```{r, fig.align="center"}
probabilities = predict(model, testing.set[, 1:(ncol(testing.set)-1)], probability = TRUE)
roc.data = DataROC(testing.set, attr(probabilities, "probabilities"), predictions)
generate_ROC(roc.data$Prob, roc.data$Label, roc.data$Prediction)
```

Se observa una curva ROC que en comparación al análisis presentado haciendo uso de lso parámeros por defecto es peor, mucho más errático. Aparentemente, el ajuste de los parámetros quitaron generalización para los nuevos ataques.

####Segundo nivel de clasificación (K-Medias)
Se iniciará la adición del segundo nivel de clasificación creando el nuevo conjunto de datos para el algoritmo K-Medias, el cual correponde a los registros clasificados como normal.

```{r}
kmeans.set = testing.set[predictions == "normal", ]
dim(kmeans.set)
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

Se puede apreciar que existen 5163 ataques en el sub-conjunto a utilizar. Se pre-calcularán los centroides para tratar de garantizar la convergencia al mejor mínimo local.

```{r}
start.time.kmeans.training = Sys.time()
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)

matrix.centers = matrix.centers/100
total.time.kmeans.training = Sys.time() - start.time.kmeans.training
```

Y posteriormente dichos centroides serán utilizados para el proceso definitivo de clasificación.

```{r}
start.time.kmeans.predictions = Sys.time()
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)

total.time.kmeans.predictions = Sys.time() - start.time.kmeans.predictions
```

Luego, podemos ver los resultados. Empezaremos por visualizar la matriz de confusión.

```{r}
predictions = OrderKmeans(kmeans.model)
confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)
confusion.matrix.kmeans.model
```

Se observa un pobre complemento por parte de K-Medias para detectar ataques. Básicamente no hizo nada, a proporción de su aporte al problema fue nula. Veamos la tasa de acierto y tasa de error.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)*100
```

El 64% de acierto debido a que ese es el porcentaje de la clase normal. Veamos ahora la tasa de acierto por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

Se refleja el pobre rendimiento a la hora de detectar los ataques. Veamos el resto de las medidas binarias.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

Se refleja un pobre resultado a la hora de detectar los ataques y un gran desempeño para la correcta clasificación del tráfico normal.

####Estadísticas totales
Para las estadísticas totales, mezclaremos las matrices de confusión de ambos niveles.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix, confusion.matrix.kmeans.model)
confusion.matrix.two.labels
```

Se observa un nulo aporte por parte del segundo nivel de clasificación referente a K-Medias. Ahora veamos el resto de las estadísticas.

```{r}
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
total.time.predictions + total.time.kmeans.predictions
training.time + total.time.kmeans.training
```

Se pueden apreciar estadísticas similares a las obtenidas por el primer nivel de sVM. Estas sigieren un modelo que es certero a la hora de detectar ataques cuanod los identifica, sin embargo, solo detecta el 60% de estos. Por otra parte, clasifica muy bien el tráfico normal.

####Conclusiones
El ajuste de parámetros deterioró la certeza con la que trabaja el modelo de SVM. Adicionalmente, el segundo nivel de K-Medias no aportó absolutamente nada al complemento del primer nivel.

###(III) PCA - NN - K-Medias
En esta sección se realizará el entrenamiento y análisis del esquema PCA - NN - K-Medias (dos conjuntos).

####Entrenamiento del modelo
Se empezará el entrenamiento del modelo eliminando variables parciales, cargando el paquete a utilizar y el archivo de funciones correspondiente.

```{r, eval=FALSE}
rm(list = ls())

#Cargando paquetes
library("nnet")

#Cargando funciones
source("../source/functions/functions.R")
```

A continuación se cargará el conjunto de entrenamiento, se eliminarán las etiquetas innecesarias.

```{r, eval=FALSE}
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
dataset.training$Label_Normal_TypeAttack = NULL
dataset.training$Label_Num_Classifiers = NULL
dataset.training$Label_Normal_or_Attack = NULL
```

Todas las variables predictoras deben ser de tipo numérico y la variable objetivo debe ser de tipo *factor*. Adicionalmente, las variables predictoras deben ser escaladas para tener media cero y desviación estándar uno.

```{r, eval = FALSE}
for (i in 1 : (ncol(dataset.training) -1) )
  dataset.training[,i] = as.numeric(dataset.training[,i])

dataset.training[,ncol(dataset.training)] = as.factor(dataset.training[,ncol(dataset.training)])

#Escalando el conjunto de datos
dataset.training = ScaleSet(dataset.training)
```

En este punto se puede aplicar PCA y seleccionar las primeras 7 componentes principales.

```{r, eval=FALSE}
pca = prcomp(dataset.training[, -41], scale. = TRUE)
dataset.training = cbind(as.data.frame(pca$x[,1:7]),
                         Label = dataset.training$Label)
```

En este punto ya se tiene preparado el conjunto de datos a utilizar. Vamos a cargar los parámetros seleccionados para el conjunto de datos correspondiente.

```{r}
hidden.neurons = readRDS("../source/parameter_selection/NN/PCA/tuned_model.rds")
hidden.neurons = hidden.neurons$best.parameters$size
hidden.neurons
```

El número de neurona elegido para la capa intermedia es de 30 neuronas. El mismo será pasado como parámetro para el entrenamiento del modelo a continuación. Todas las actividades referentes al entrenamiento y análisis del modelo serán cronometradas.

```{r, eval=FALSE}
#Tomando tiempo de inicio
start.time = Sys.time()

#Entrenando el modelo
set.seed(22)
model = nnet(Label ~ .,
             data = dataset.training,
             size = hidden.neurons,
             maxit = 100)

#Calculating time of training#Calculando el tiempo total
total.time = Sys.time() - start.time
```

Los resultados serán almacenados en una lista y posteriormente en un objeto para su posterior análisis.

```{r, eval=FALSE}
list.results = list(total.time, model)
saveRDS(list.results, file = "s../source/tuned_model/PCA/NN/testing_set/list_results.rds")
```

####Evaluación del modelo
Para la evaluación del modelo iniciaremos eliminando variables parciales, cargando el paquete a utilizar y cargando el archivo de funciones correspondiente.

```{r}
rm(list = ls())

#Cargando paquetes
library("nnet")

#Cargando archivo de funciones
source("../source/functions/functions.R")
```

A continuación el conjunto de datos de prueba es cargado, las etiquetas innecesarias son eliminadas, y el conjunto de datos es escalado para que las variables predictoras tengan media cero y desviación estándar uno.

```{r}
#Cargando conjunto de prueba
testing.set = read.csv("../dataset/NSLKDD_Testing_New.csv",
                       sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
testing.set$Label_Normal_TypeAttack = NULL
testing.set$Label_Num_Classifiers = NULL
testing.set$Label_Normal_or_Attack = NULL

#Escalando conjunto de datos
testing.set = ScaleSet(testing.set)
```

En este punto se puede aplicar PCA y seleccionar las primeras 7 componentes principales.

```{r}
pca = prcomp(testing.set[, -41], scale. = TRUE)
testing.set = cbind(as.data.frame(pca$x[,1:7]),
                         Label = testing.set$Label)
```

Ya se tiene el conjunto de prueba listo, ahora cargaremos los resultados obtenidos en la sección anterior y los extraeremos en variables para su posterior uso.

```{r}
#Cargando resultados
results = readRDS("../source/tuned_model/PCA/NN/testing_set/list_results.rds")

#Extrayendo resultados
training.time = results[[1]]
model = results[[2]]
```

En esta sección se realizará el entrenamiento y análisis del esquema PCA - NN - K-Medias (dos conjuntos). A continuacións e realizan las predicciones del modelo haciendo uso del conjutno de prueba. Este tiempo es cronometrado.

```{r}
#Inicializando el tiempo
start.time.predictions = Sys.time()

#Realizando las predicciones
predictions = predict(model, testing.set[, 1:(ncol(testing.set)-1)], type = "class")

#Capturando el tiempo total
total.time.predictions = Sys.time() - start.time.predictions
total.time.predictions
```

Ahora veamos los resultados, comenzando por la matriz de confusión.

```{r}
confusion.matrix = table(Real = testing.set[,ncol(testing.set)],
                         Prediction = predictions)

confusion.matrix
```

Se observa una matriz de confusión bastante desordenada y con muy poco acierto para las clases R2L y U2R. Veamos la tasa de acierto y la tasa de error.

```{r}
accuracy = mean(testing.set[,ncol(testing.set)] == predictions)
accuracy * 100
ErrorRate(accuracy) * 100
```

Solo se alcanza un acierto de 28% de los registros. Este númeor muy por debajo del alcanzado en el conjunto de entrenamiento y en comparación con el alcazado por el modleo (I). Ahora veamos la tasa de acierto por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix, testing.set)
```

El modelo es regular para la clasificaicón de la clase normal, y muy pobre para el resto de las clases. Transformaremos la matriz de confusión en binaria para poder ver algunas otras estadísticas.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(testing.set, predictions)
attack.normal.confusion.matrix
```

Se observa pla predominancia de los falsos positivos y falsos negativos por encima de los aciertos. Ahora veamos la tasa de aciertos y el resto de las medidas de rendimiento.

```{r}
Accuracy(attack.normal.confusion.matrix) * 100
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

Las medidas indican un pobre modelo para detectar ataques y clasificar el tráfico normal. Dicho esto, el modelo no sirve para nada en este primer nivel. Veamos el desempeño de la curva ROC.

```{r, fig.align="center"}
probabilities = predict(model, testing.set[, 1:(ncol(testing.set)-1)])
roc.data = DataROC(testing.set, probabilities, predictions)
generate_ROC(roc.data$Prob, roc.data$Label, roc.data$Prediction)
```

La curva ROC demuestra que el rendimiento de la curva ROC es similar al d ela línea del azar.

####Segundo nivel de clasificación (K-Medias)
Se añadirá el segundo nivel de clasificación correspondiente a K-Medias. Se iniciará creando el el sub-conjunto a utilizar seleccionado los registros clasificados como normal por el primer nivel.

```{r}
kmeans.set = testing.set[predictions == "normal", ]
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

Se observa la gran cantidad de ataques presentes en este subconjunto. A continuación se precalcularán los centroides a ser pasados al algoritmo para la clasificación.

```{r}
start.time.kmeans.training = Sys.time()
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)

matrix.centers = matrix.centers/100
total.time.kmeans.training = Sys.time() - start.time.kmeans.training
```

Los centroides precalculados, ahora son pasados como parámetros para la clasificación.

```{r}
start.time.kmeans.predictions = Sys.time()
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)

total.time.kmeans.predictions = Sys.time() - start.time.kmeans.predictions
```

Veamos ahora los resultados, comenzando por la matriz de confusión.

```{r}
predictions = OrderKmeans(kmeans.model)
confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)

confusion.matrix.kmeans.model
```

K-Medias hizo un gran trabajo, detectando 3762 nuevos ataques sin generar una gran cantidad de falsos positivos. Veamos la tasa de acierto y de error.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)*100
```

Una tasa acierto bastante buena de 62.07%. Veamos la tasa de acierto por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

Sólo se detectó el 42% de los ataques. Sin embargo se logró identificarlos sin la generación excesiva de falsos positivos. Veamos el resto de las medidas de rendimiento.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

Las medidas ilustran un modelo que es muy bueno para clasificar el tráfico normal y muy certero cuándo detecta un ataque. 

####Estadísticas totales
Uniremos las matrices de confusión de los dos modelos para calcular las estadísticas totales.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix, confusion.matrix.kmeans.model)
confusion.matrix.two.labels
```

Se puede ver la mejora en la matriz de confusión con respecto a la generada en el primer nivel. Veamos el resto de las medidas de rendimiento.

```{r}
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
total.time.predictions + total.time.kmeans.predictions
training.time + total.time.kmeans.training
```

Todas las medidas de rendimiento incrementaron con respecto a las presentadas en el primer nivel. En esta ocasión, la inclusión de K-Medias fue bastante positiva, esto debido al pobre rendimiento del primer nivel.

####Conclusiones
La reducción de características PCA con 7 componentes principales no aporta la suficiente información al modelo para poder generalizar con el conjunto de prueba. Depende de los resultados que arroe el modelo (IV), es posible que se necesite de un número mayor de componentes principales que aporten mayor información para la generalización.

###(IV) PCA - SVM - K-Medias
En esta sección se realizará el entrenamiento y análisis del esquema PCA - SVM - K-Medias (dos conjuntos).

####Entrenamiento del modelo
Se empezará el entrenamiento del modelo eliminando variables parciales, cargando el paquete a utilizar y el archivo de funciones correspondiente.

```{r, eval=FALSE}
rm(list = ls())

#Cargando paquetes
library("e1071")

#Cargando archivo de funciones
source("../source/functions/functions.R")
```

A continuación se carga el conjunto de datos de entrenamiento y se eliminan las etiquetas innecesarias.

```{r, eval=FALSE}
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
dataset.training$Label_Normal_TypeAttack = NULL
dataset.training$Label_Num_Classifiers = NULL
dataset.training$Label_Normal_or_Attack = NULL
```

Todas las variables predictoras deben ser de tipo numérico y la variable objetivo de tipo *factor*. Adicionalmente las variables debens er escaladas para que estas tengan media cero y desviación estándar uno.

```{r, eval=FALSE}
for (i in 1 : (ncol(dataset.training) -1) )
  dataset.training[,i] = as.numeric(dataset.training[,i])

dataset.training[,ncol(dataset.training)] = as.factor(dataset.training[,ncol(dataset.training)])

#Escalando el conjunto de datos
dataset.training = ScaleSet(dataset.training)
```

Luego, se puede aplicar PCA y seleccionar las primeras 7 componentes principales.

```{r, eval=FALSE}
pca = prcomp(dataset.training[, -41], scale. = TRUE)
dataset.training = cbind(as.data.frame(pca$x[,1:7]),
                         Label = dataset.training$Label)
```

Se cargarán los parámetros seleccionados durante el procesos de selección de parámetros.

```{r}
tuned.parameters = readRDS("../source/parameter_selection/SVM/PCA/tuned_model.rds")
tuned.cost = tuned.parameters$best.parameters$cost
tuned.gamma = tuned.parameters$best.parameters$gamma
tuned.cost
tuned.gamma
```

Estos parámetros son *cost* = 6 y *gamma* = 0.4. Esta configuración propone un radio espectral más rande y un rango de error mayor que el de los parámetros por defecto. Las actividades referentes a entrenamiento y predicción por parte del modleo serán cronometrados.

```{r, eval=FALSE}
#Iniciando cronómetro
start.time = Sys.time()

set.seed(22)
model = svm(Label~.,
            data = dataset.training,
            kernel = "radial",
            cost = tuned.cost,
            gamma = tuned.gamma,
            scale = FALSE,
            probability = TRUE)

#Tiempo total de entrenamiento
total.time = Sys.time() - start.time
```

Los resultados obtenidos son guardados en una lista y posteriormente en un objeto para su posterior análisis.

```{r, eval=FALSE}
list.results = list(total.time, model)
saveRDS(list.results, file = "../source/tuned_model/PCA/SVM/testing_set/list_results.rds")
```

####Evaluación del modelo
Se iniciará con el análisis del modelo eliminando variables parciales, cargando el paquete a utilizar y el archivo de funciones correspondiente.

```{r}
rm(list = ls())

#Cargando paquetes
library("e1071")

#Cargandoa rchivo de funciones
source("../source/functions/functions.R")
```

Luego, se carga el conjunto de datos de prueba, se eiliminan las etiquetas que no se utilizarán y se escala el conjunto de datos para que las variabels predictoras tengan media cero y desviación estándar uno.

```{r}
testing.set = read.csv("../dataset/NSLKDD_Testing_New.csv",
                       sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
testing.set$Label_Normal_TypeAttack = NULL
testing.set$Label_Num_Classifiers = NULL
testing.set$Label_Normal_or_Attack = NULL

#Escalando el conjunto de datos
testing.set = ScaleSet(testing.set)
```

En este punto se puede aplicar PCA al conjunto de datos y seleccionar las primeras 7 componentes principales.

```{r}
pca = prcomp(testing.set[, -41], scale. = TRUE)
testing.set = cbind(as.data.frame(pca$x[,1:7]),
                    Label = testing.set$Label)
```

Luego, los resultados obtenidos en la sección anterior son cargados y almacenados en variables para su posterior uso.

```{r}
results = readRDS("../source/tuned_model/PCA/SVM/testing_set/list_results.rds")

#Extrayendo resultados
training.time = results[[1]]
model = results[[2]]
```

Se realizarán las predicciones, como se mecionó previamente, este tiempo será cronometrado.

```{r}
#Inicializando 
start.time.predictions = Sys.time()

#Realizando las predicciones
predictions = predict(model, testing.set[, 1:(ncol(testing.set)-1)], type = "class")

#Calculando el tiempo total
total.time.predictions = Sys.time() - start.time.predictions
total.time.predictions
```

Veamos los resultados obtenidos, empezando por calcular la matriz de ocnfusión.

```{r}
confusion.matrix = table(Real = testing.set[,ncol(testing.set)],
                         Prediction = predictions)

confusion.matrix
```

Se observa una matriz de confusión bastante desordenada la mayoría de los elementos fuera de la diagonal. Veamos las tasas de acierto y de error.

```{r}
accuracy = mean(testing.set[,ncol(testing.set)] == predictions)
accuracy * 100
ErrorRate(accuracy) * 100
```

Se ibserva un desempeño terrible con solo 13.34% de aciertos. Veamos la eficacia por etiquetas.

```{r}
AccuracyPerLabel(confusion.matrix, testing.set)
```

Todas las clases tuvieron una tasa de acierto muy muy mala. Ahora crearemos una matriz binaria para calcular otras medidas de rendimiento.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(testing.set, predictions)
attack.normal.confusion.matrix
```

Se observa claramente la predominancia de los falsos positivos y falsos negativos. Veamos el resto de las medidas de rendimiento.

```{r}
Accuracy(attack.normal.confusion.matrix) * 100
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

Todas las medidas sugieren que este modelo no se ajusta para nada al problema. Veamos el comportamiento de la curva ROC.

```{r, fig.align="center"}
probabilities = predict(model, testing.set[, 1:(ncol(testing.set)-1)], probability = TRUE)
roc.data = DataROC(testing.set, attr(probabilities, "probabilities"), predictions)
generate_ROC(roc.data$Prob, roc.data$Label, roc.data$Prediction)
```

Claramente el desempeño es por que la línea del azar. Reflejando el pobre rendimiento del modelo.

####Segundo nivel de clasificación
Para añadir el segundo nivel de clasificación, se tomarán aquellos registros clasificados como normal y se tomará ese sub-conjunto para aplicar K-Medias.

```{r}
kmeans.set = testing.set[predictions == "normal", ]
dim(kmeans.set)
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

Se observa que hay 11266 ataques presentes en el sub-conjunto que se refieren a los falsos negativos generados por el primer nivel. Se precalcularán los centroides para garantizar la convergencia a un buen mínimo local.

```{r}
start.time.kmeans.training = Sys.time()
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)

matrix.centers = matrix.centers/100
total.time.kmeans.training = Sys.time() - start.time.kmeans.training
```

Los centroides obtenidos ahora son pasados como parámetros para clasificar de manera definitiva.

```{r}
start.time.kmeans.predictions = Sys.time()
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)

total.time.kmeans.predictions = Sys.time() - start.time.kmeans.predictions
```

Veamos los resultados obtenidos, empezando por ver la matriz de confusión.

```{r}
predictions = OrderKmeans(kmeans.model)
confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)
confusion.matrix.kmeans.model
```

Se puede observar que se detectaron 2188 ataques sin apenas falsos positivos. Por esta parte, se puede apreciar una mejora en el desempeño por parte de este segundo nivel. Veamos la tasa de acierto y de error.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)*100
```

Se logró una tasa de acierto de 35%. Estpa está muy por debajo de lo que es un buen rendimiento. Veamos la tasa de aciertpo por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

Solo un 19% de acierto para los ataques y una alta tasa para el tráfico normal que está sesgada ocnsiderando que apun existen una gran cantidad de falsos negativos. Veamos el resto de las medidas de rendimiento.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

Se observa un modelo que clasifica muy pocos ataques, sin embargo, al hacerlo es muy certero. Por otra parte, identifica la mayorpia del tráfico normal presente.

####Estadísticas totales
Para las estadísticas totales combinaremos las matrices de confusión generadas en ambos niveles.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix, confusion.matrix.kmeans.model)
confusion.matrix.two.labels
```

Se puede apreciar una leve mejora con respecto a los resultados obtenidos en el primer nivel, sin embargo, siguen predominando los falsos negativos y positivos. Veamos la tasa de acierto y la tasa de error.

```{r}
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
```

Solo 28% de acierto. Veamos el resto de las medidas de rendimiento.

```{r}
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
total.time.predictions + total.time.kmeans.predictions
training.time + total.time.kmeans.training
```

Se observan medidas muy pobres, parecidas a las obtenidas en el primer nivel. K-Medias mejoró un poco el rendimiento global.

####Conclusiones
La reducción de características haciendo uso de PCA y als primeras 7 componentes principales no fue nada bueno, el modleo comete más errores que acierto. El segundo nivel de K-Medias logró aportar un poco de mejora detectando algunos ataques sin generar en exceso falsos positivos.

###(V) GFR - NN - K-Medias
En esta sección se hará el entrenamiento y análisis del modelo GFR - NN - K-Medias (dos conjuntos).

####Entrenamiento del modelo
Se empezará por el entrenamiento del modelo eliminando variables parciales, cargando el paquete a utilizar, y cargando el archivo de funciones corresondiente.

```{r, eval=FALSE}
rm(list = ls())

#Cargando paquetes
library("nnet")

#Cargando archivo de funciones
source("../source/functions/functions.R")
```

Luego, se cargará el conjunto de entrenamiento y se eliminarán las etiquetas que no se utilizarán.

```{r, eval=FALSE}
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
dataset.training$Label_Normal_TypeAttack = NULL
dataset.training$Label_Num_Classifiers = NULL
dataset.training$Label_Normal_or_Attack = NULL
```

Todas las variables predictoras deben ser de tipo numérico y la variabl objetivo de tipo *factor*. Adicinalmente las variables predictoras serán escaladas para que tengan media cero y desviación estándar uno.

```{r, eval=FALSE}
for (i in 1 : (ncol(dataset.training) -1) )
  dataset.training[,i] = as.numeric(dataset.training[,i])

dataset.training[,ncol(dataset.training)] = as.factor(dataset.training[,ncol(dataset.training)])

#Scaling dataset
dataset.training = ScaleSet(dataset.training)
```

A continuación se pueden cargar las características seleccionar por GFR para NN, y seleccionar las primeras 9. Adicionalmente, dichas características serán seleccionadas para generar el nuevo conjunto de datos

```{r, eval=FALSE}
#Extrayendo características
nn.gfr = readRDS("../source/feature_selection/NN/results_GFR.rds")
nn.gfr = rownames(nn.gfr)[1:9]

#Extrayendo información
Label = dataset.training$Label

#Creando el nuevo conjunto de datos
dataset.training = dataset.training[, nn.gfr]
dataset.training = cbind(dataset.training, Label = Label)
```

Luego se pueden cargar los parámetros seleccionados para el modelo.

```{r}
hidden.neurons = readRDS("../source/parameter_selection/NN/GFR/tuned_model.rds")
hidden.neurons = hidden.neurons$best.parameters$size
hidden.neurons
```

Se seleccionaron 28 neuronas para ser pasadas como parámetro para la capa intermedia. Dicho parámetro será pasado para el entrenamiento de la red neuronal. El tiempo será cronometraod para su posterior análisis.

```{r, eval=FALSE}
#Inicio del cronómetro
start.time = Sys.time()

#Entrenamiento del modelo
set.seed(22)
model = nnet(Label ~ .,
             data = dataset.training,
             size = hidden.neurons,
             maxit = 100)

#Calculando el tiempo total
total.time = Sys.time() - start.time
```

Los resultados serán guardados en una lista y posteriormente en un objeto para su posterior análisis.

```{r, eval=FALSE}
list.results = list(total.time, model)
saveRDS(list.results, file = "../source/tuned_model/GFR/NN/testing_set/list_results.rds")
```

####Evaluación del modelo
Para la evaluación del modelo se iniciará con la eliminación de variables parciales, la craga del paquete a utilizar y del archivo de funciones correspondiente.

```{r}
rm(list = ls())

#Cargando paquete
library("nnet")

#Cargando archivo de funciones
source("../source/functions/functions.R")
```

A continuación, se cargará el conjunto de prueba, se eliminarás las etiquetas innecesarias y se escalara el mismo para que todas las variables predictoras tengan media 0 y desviación estándar uno.

```{r}
#Cargando el conjunto de datos
testing.set = read.csv("../dataset/NSLKDD_Testing_New.csv",
                       sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
testing.set$Label_Normal_TypeAttack = NULL
testing.set$Label_Num_Classifiers = NULL
testing.set$Label_Normal_or_Attack = NULL

#Escalando el conjunto de datos
testing.set = ScaleSet(testing.set)
```

Ahora se pueden cargar las características selecciondas por GFR para NN, y luego usar las primeras 9 características para crear el nuevo conjunto de datos.

```{r}
#Seleccionando las características de GFR
nn.gfr = readRDS("../source/feature_selection/NN/results_GFR.rds")
nn.gfr = rownames(nn.gfr)[1:9]

#Extracting info
Label = testing.set$Label

#Creando el nuevo conjunto de datos
testing.set = testing.set[, nn.gfr]
testing.set = cbind(testing.set, Label = Label)
```

Luegom los resultados obtenidos en la fase de entrenamiento son extraídos a variables para su posterior uso.

```{r}
#Cargando los resultados
results = readRDS("../source/tuned_model/GFR/NN/testing_set/list_results.rds")

#Extrayendo los resultados
training.time = results[[1]]
model = results[[2]]
```

Se harán las predicciones. Las mismas serán cronometradas para su posterior análisis.

```{r}
#Iniciando cronómetro
start.time.predictions = Sys.time()

#Realizando las prediciiones
predictions = predict(model, testing.set[, 1:(ncol(testing.set)-1)], type = "class")

#Calculando el tiempo total
total.time.predictions = Sys.time() - start.time.predictions
total.time.predictions
```

Veamos los resultado, comenzando por ver la matriz de confusión.

```{r}
confusion.matrix = table(Real = testing.set[,ncol(testing.set)],
                         Prediction = predictions)

confusion.matrix
```

Se observa un desempeño al menos muy preciso a la hora de detectar tráfico normal. Al menos superior al presentado por PCA para los modelos (III) y (IV). 

```{r}
accuracy = mean(testing.set[,ncol(testing.set)] == predictions)
accuracy * 100
ErrorRate(accuracy) * 100
```

La tasa de acierto refleja lo comoentado en el párrafo anterior, donde se nota una tasa de acierto de 66%. Ahora veamos la tasa de acierto por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix, testing.set)
```

Para las clases normal y Probing el rendimeinto es muy bueno, mpara DoS bastante regular, y para el resto de las etiquetas bastante malo. Creemos la matriz de ocnfusión binaria para ver los resultados obtenidos.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(testing.set, predictions)
attack.normal.confusion.matrix
```

Se crea una alta cantidad de falsos negativos, sin embargo, los ataques y el tráfico normal estpan bien distribuidos. Adicionalmente, no hay una presencia muy grande de falsos positivos. Veamos el resto de las medidas de rendimiento.

```{r}
Accuracy(attack.normal.confusion.matrix) * 100
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

S epresenta una tasa de acierto de 77%. A pesar de que el modelo clasifica bien los tipos de ataque, si lo hace para separar los ataques del tráfico normal. Veamos la curva ROC.

```{r, fig.align="center"}
probabilities = predict(model, testing.set[, 1:(ncol(testing.set)-1)])
roc.data = DataROC(testing.set, probabilities, predictions)
generate_ROC(roc.data$Prob, roc.data$Label, roc.data$Prediction)
```

La curva ROC es mucho mejor a la presentada por el modelo haciendo uso del conjunto de datos de entrenamiento original.

####Segundo nivel de clasificación
Añadiremos el segundo nivel de K-Medias empezando por crear un sub-conjunto correspondiente a los registros que fueron clasificados como normal.

```{r}
kmeans.set = testing.set[predictions == "normal", ]
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

Se puede apreciar que en dicho sub-conjunto hay 4532 ataques. Ahora precalcularemos los centroides.

```{r}
start.time.kmeans.training = Sys.time()
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)
matrix.centers = matrix.centers/100
total.time.kmeans.training = Sys.time() - start.time.kmeans.training
```

Y utilizaremos los centroides pre-calculados para la clasificación definitiva.

```{r}
start.time.kmeans.predictions = Sys.time()
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)

total.time.kmeans.predictions = Sys.time() - start.time.kmeans.predictions
```

Veamos los resultados obtenidos, empezando por visualizar la matriz de confusión.

```{r}
predictions = OrderKmeans(kmeans.model)
confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)

confusion.matrix.kmeans.model
```

La inclusión de K-Medias no fue positiva debido a la alta generación de falsos positivos que se generó y la escaza cantidad de ataques detectados. Veamos la tasa de aciertos, y la tasa de error.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)*100
```

Se observa que hay más errores que aciertos por parte de este segundo nivel. Ahora veamos la tasa de acierto por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

Se puede observar el pobre desempeño alcanzado, en particular que solo el 20% de los ataques fueron detectados. Veamos el resto de las medidas de rendimiento.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

Estas sugieren un modelo que no es bueo para clasificar ataques ni tráfico normal.

####Estadísticas totales
Unificaremos las matrices de confusión de los dos niveles.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix, confusion.matrix.kmeans.model)
confusion.matrix.two.labels
```

Si se compara con la matriz de confusión obtenida en el primer nivel, se puede observar el deterioro ocasionado por K-Medias con la alta generación de falsos positivos. Ahora veamos el resto de las medidas de rendimiento.

```{r}
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
total.time.predictions + total.time.kmeans.predictions
training.time + total.time.kmeans.training
```

Disminuyó considerablemente la tasa de aciertos en comparación con la tasa de aciertos presentada por el primer nivel de clasificación, así mismo, el resto de las medidas de rendimiento se vieron afectadas notablemente.

####Conclusiones
La reducción de características funcionó de una mejor manera para el primer nivel. Sin embargo, a la hora de unificar las matrices de confusión el incremento de acierto fue muy grande, hay que procurar que la tasa de acierto de cinco clases y dos clases sea lo más parecida posible para tener un modelo que sea consecuente. De nuevo, K-Medias rindió por debajo de los esparado, es claro que la reducción de información para K-Medias deteriora mucho su desempeño tanto para el análisis para el conjunto de entrenamiento como para el análisis para el conjunto de prueba.

###(VI) GFR - SVM - K-Medias
en esta sección se realizará el entrenamiento y análisis del modelo GFR - SVM - K-Medias (dos grupos).

####Entrenamiento del modelo
Se iniciará con el entrenamiento del modelo eliminando variables parciales, cargando el paquete a utilizar y el correspondiente archivo de funciones.

```{r, eval=FALSE}
rm(list = ls())

#Cargando paquetes
library("e1071")

#Cargando archivo de funciones
source("../source/functions/functions.R")
```

A continuación se carga el conjunto de datos de entrenamietno y se eliminan las etiquetas que no serán utilizadas.

```{r, eval=FALSE}
dataset.training = read.csv("../dataset/NSLKDD_Training_New.csv",
                            sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
dataset.training$Label_Normal_TypeAttack = NULL
dataset.training$Label_Num_Classifiers = NULL
dataset.training$Label_Normal_or_Attack = NULL
```

Todas las variables predictoras deben ser de tipo numérico, y la variable objetivo de tipo factir. Adicionalmente, el conjunto de datos será escalado para que todas las variables predictoras tengan media cero y desviación estándar uno.

```{r, eval=FALSE}
for (i in 1 : (ncol(dataset.training) -1) )
  dataset.training[,i] = as.numeric(dataset.training[,i])

dataset.training[,ncol(dataset.training)] = as.factor(dataset.training[,ncol(dataset.training)])

#Escalando el conjunto de datos
dataset.training = ScaleSet(dataset.training)
```

A continuación se extraerán las etiquetas seleccionadad por GFR para SVM y se creará el nuevo conjunto de datos reducido.

```{r, eval=FALSE}
#Seleccionando características
svm.gfr = readRDS("../source/feature_selection/SVM/results_GFR.rds")
svm.gfr = rownames(svm.gfr)[1:9]

#Extrayendo información
Label = dataset.training$Label

#Creando nuevo conjunto de datos
dataset.training = dataset.training[, svm.gfr]
dataset.training = cbind(dataset.training, Label = Label)
```

Luego, se cargarán los parámetros seleccionados.

```{r}
tuned.parameters = readRDS("../source/parameter_selection/SVM/GFR/tuned_model.rds")
tuned.cost = tuned.parameters$best.parameters$cost
tuned.gamma = tuned.parameters$best.parameters$gamma
tuned.cost
tuned.gamma
```

Se puede observar que *cost* = 6 y *gamma* = 0.4. De esta manera se aumentó el espectro de las sircunferencias y el error permitido en comparación con los parámetros por defecto. Dichos parámetros serán pasado a continuación al modelo. Los timepos de entrenamiento y de predicciones serán cronometrados.

```{r, eval=FALSE}
#Iniciando cronómetro
start.time = Sys.time()

set.seed(22)
model = svm(Label~.,
            data = dataset.training,
            kernel = "radial",
            cost = tuned.cost,
            gamma = tuned.gamma,
            scale = FALSE,
            probability = TRUE)

#Calculando el tiempo total de entrenamiento
total.time = Sys.time() - start.time
```

Los resultados serán almacenados en una lista y seguidamente en un objeto para su posterior análisis.

```{r, eval=FALSE}
list.results = list(total.time, model)
saveRDS(list.results, file = "../source/tuned_model/GFR/SVM/testing_set/list_results.rds")
```

####Evaluación del modelo
La evaluaciónd el modelo dará inicio eliminando variables parciales, cargando el archivo de funciones a utilizar y cargando el archivo de funciones correspondiente.

```{r}
rm(list = ls())

#Cargando paquetes
library("e1071")

#Cargando archivo de funciones
source("../source/functions/functions.R")
```

A continuación se cargará el conjunto de datos de prueba, se eliminarán las etiquetas que no se utilizarán y se escalará eñ conjunto de datos para que todas estas tengan media cero y desviación estándar uno.

```{r}
testing.set = read.csv("../dataset/NSLKDD_Testing_New.csv",
                       sep = ",", header = TRUE)

#Eliminando etiquetas innecesarias
testing.set$Label_Normal_TypeAttack = NULL
testing.set$Label_Num_Classifiers = NULL
testing.set$Label_Normal_or_Attack = NULL

#Escalando el conjunto de datos
testing.set = ScaleSet(testing.set)
```

En este punto se pueden seleccionar las características para SVM seleccionadas por GFR y de esta manera crear el conjunto de datos definitivo a ser utilizado.

```{r}
#Seleccionando características
svm.gfr = readRDS("../source/feature_selection/SVM/results_GFR.rds")
svm.gfr = rownames(svm.gfr)[1:9]

#Extrayendo información
Label = testing.set$Label

#Creando el nuevo conjunto de datos
testing.set = testing.set[, svm.gfr]
testing.set = cbind(testing.set, Label = Label)
```

Se cargarán los resultados obtenidos en la sección anterior y se pasarán a variabels para posteriormente ser utilizadas.

```{r}
#Cargando resultados
results = readRDS("../source/tuned_model/GFR/SVM/testing_set/list_results.rds")

#Extrayendo resultados
training.time = results[[1]]
model = results[[2]]
```

A continuación se realizarán las predicciones. Los tiempos para las predicciones serán cronometrados.

```{r}
#Inicializando el contador
start.time.predictions = Sys.time()

#Realizando las predicciones
predictions = predict(model, testing.set[, 1:(ncol(testing.set)-1)], type = "class")

#Capturando el tiempo total
total.time.predictions = Sys.time() - start.time.predictions
total.time.predictions
```

Veamos los resultados obtenidos, empezando por la matriz de confusión.

```{r}
confusion.matrix = table(Real = testing.set[,ncol(testing.set)],
                         Prediction = predictions)

confusion.matrix
```

Se puede observar que la mayoría de los registros se concentran en la diagonal. Adicionalmente, se observa que hay una alta presencia de falsos negativos. Veamos la tasa de acierto y de fallos.

```{r}
accuracy = mean(testing.set[,ncol(testing.set)] == predictions)
accuracy * 100
ErrorRate(accuracy) * 100
```

Una muy buena tasa de acierto con 72% de acierto. Ahora veamos la tasa de acierto por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix, testing.set)
```

Se observa que el modelo muestra muy malos resultados para las clases R2L y U2R. Luego para Probing, el rendimeinto es regular y más destacado para el resto de las clases. Ahora veamos la matriz de confusión binaria.

```{r}
attack.normal.confusion.matrix = AttackNormalConfusionMatrix(testing.set, predictions)
attack.normal.confusion.matrix
```

Se observa una poca generación de falsos positivos y una alta tasa de falsos negativos. Veamos el resto de las medidas de rendimiento.

```{r}
Accuracy(attack.normal.confusion.matrix) * 100
Sensitivity(attack.normal.confusion.matrix) * 100
Especificity(attack.normal.confusion.matrix) * 100
Precision(attack.normal.confusion.matrix) * 100
```

Se observa un modelo bastante bueno para detectar tráfico normal, y certero cuando clasifica un ataque como tal. Sin embargo, solo detecta el 55% de los ataques presentes. Veamos la curva ROC.

```{r, fig.align="center"}
probabilities = predict(model, testing.set[, 1:(ncol(testing.set)-1)], probability = TRUE)
roc.data = DataROC(testing.set, attr(probabilities, "probabilities"), predictions)
generate_ROC(roc.data$Prob, roc.data$Label, roc.data$Prediction)
```

La curva se ve bastante errática y algunos puntos hasta se piensa que acierta por azar. En comparación al conjunto de datos por defecto se ve que el desempeño no es nada bueno.

####Segundo nivel de clasificación (K-Medias)
Para añadir el segundo nivel de K-Medias empezaremos por extraer el subconjunto de registros que fueron clasificados como normal.

```{r}
kmeans.set = testing.set[predictions == "normal", ]
dim(kmeans.set)
kmeans.set[,ncol(kmeans.set)] = as.character(kmeans.set[,ncol(kmeans.set)])
kmeans.set[kmeans.set[,ncol(kmeans.set)] != "normal",ncol(kmeans.set)] = "Attack"
SumLabels(kmeans.set, ncol(kmeans.set))
```

Se precalcularán los centroides para garantizar la convergencia al mejor mínimo local.

```{r}
start.time.kmeans.training = Sys.time()
matrix.centers = FindCentersKmeans(set = kmeans.set, clusters = 2,
                                   iterations = 100, iter.max = 100)

matrix.centers = matrix.centers/100
total.time.kmeans.training = Sys.time() - start.time.kmeans.training
```

Los centroides obtenidos serán pasados como parámetro para la clasificación final.

```{r}
start.time.kmeans.predictions = Sys.time()
kmeans.model = kmeans(kmeans.set[,1:(ncol(kmeans.set)-1)], centers = matrix.centers,
                      iter.max = 100)

total.time.kmeans.predictions = Sys.time() - start.time.kmeans.predictions
```

Veamos la matriz de confusión empezando por visualizar la matroz de confusión.

```{r}
predictions = OrderKmeans(kmeans.model)
confusion.matrix.kmeans.model = table(Real = kmeans.set[,ncol(kmeans.set)],
                                      Prediction = predictions)
confusion.matrix.kmeans.model
```

Se identifican 1230 nuevos ataques. Sin embargo se generan mayor cantidad de falsos positivos. Veamos la tasa de acierto y de error.

```{r}
accuracy.kmeans.model = mean(predictions == kmeans.set[,ncol(kmeans.set)])
accuracy.kmeans.model*100
ErrorRate(accuracy.kmeans.model)*100
```

Se logra apenas 41% de acierto, esto debido a la excesiva generación de falsos positivos. Ahora veamos la tasa de acierto por etiqueta.

```{r}
AccuracyPerLabel(confusion.matrix.kmeans.model, kmeans.set)
```

Solo se detectan 21.7% de los nuevos ataques. Y la tasa de acierto sobre el tráfico normal decrementa de forma notable. Veamos el resto de las medidas de rendimiento.

```{r}
Sensitivity(confusion.matrix.kmeans.model) * 100
Especificity(confusion.matrix.kmeans.model) * 100
Precision(confusion.matrix.kmeans.model) * 100
```

Estas sugieren un pobre desempeño por parte del segundo nivel de clasificación.

####Estadísticas totales
Veamos las estadísticas totales unificando las matrices de confusión.

```{r}
confusion.matrix.two.labels = TwoLevelsCM(attack.normal.confusion.matrix, confusion.matrix.kmeans.model)
confusion.matrix.two.labels
```

Se observa que se detectan más ataques correctamente, que lo que clasifica correntamente el tráfico normal. Si se compara con la matriz de confusión del primer nivel hay más falsos positivos y menor cantidad de acierto sobre el tráfico normal. Veamos el resto de las estadísticas.

```{r}
accuracy.total = Accuracy(confusion.matrix.two.labels)
accuracy.total * 100
ErrorRate(accuracy.total) * 100
Sensitivity(confusion.matrix.two.labels) * 100
Especificity(confusion.matrix.two.labels) * 100
Precision(confusion.matrix.two.labels) * 100
total.time.predictions + total.time.kmeans.predictions
training.time + total.time.kmeans.training
```

Se disminuyó la tasa de acierto y la precisión a ala hora de detectar los ataques. De esta manera el segundo nivel de K-MEdias no aportó mejora.

####Conclusiones
La reducción de GFR presenta un desempeño aceptable. Mucho mejor que el obtenido para su homólogo usando PCA. La curva ROC es el verdadero problema. El comportamiento del modelo es bastante errático. Para el segundo nivel, de nuevo se bserva que la reducción de características no va de la mano con K-Medias.

###Conclusiones generales
Para los modelos (I) y (II), la selección de parámetros disminuyó acierto con respecto a lso parámetros por defecto usados anteriormente. Dicho esto, la selección de parámetros será buena si el problema no es tan cambiante, situación que no es la presentada en este probelma donde se agregan 14 nuevos tipos de ataques. Dicho esto, se sugiere la utilización de los parámetros por defecto, al menos hasta que se tenga una base de conocimeinto lo bastante amplia que pueda abarcar todos los ataques disponibles. La situación presentada anteriormente es utópica, ya que constantemente lso ataques evolucionan.

Para os modelos (III) y (IV) se observa que la reducción de PCA no funciona para nada para la generalización del primer nivel. K-Medias en estos modelos aporta mayor cantidad de información.

Para los modelos (V) y (VI), se observa que la reducción de características fue más fructífera que lso resultados obtenidos por PCA. Sin emabrgo, K-Medias presenta un pobre rendimiento. Se puede observar un patrón en la evaluación sobre el conjunto de datos de entrenamiento que sugiere que K-Medias deteriora su desempeño utilizando conjunto de datos reducidos en contraste con el uso del conjunto de datos completo.

Por otra parte, se observa que SVM en algunas ocasiones tiene ma

Dicho esto, se sugiere que para el primer de clasificación se utilicen mayor cantidad de características. Para PCA existe otro criterio que consiste en utilizar el número de componentes principales que hacen que se alcance el 95% de la varianza acumulada. Por otra parte, para GFR, se sugiere que se haga uso de mayor cantidad de variables predictoras ¿Cuántas? Bueno, Li Yinhui y colaboradores expone en su trabajo *An Efficient Intrusion Detection system Based on Support Vector Machines and Gradually Feature Removal Method* que con 19 características puede ser un buen número de características a seleccionar. 
Para el segundo nivel de clasificación se observa que lo mejor sería dejar el conjunto de características original y solo usar la reducción de características para el primer nivel. De esta manera se ppodría sacar lo mejor de ambos escenarios.

##Conclusiones Generales

La selección de parámetros no es una buena opción para este tipo de problemas ocnsiderando lo cambiante que es. La reducción de características debe hacerse con mayor número de características tanto para PCA (95% de varianza acumulada) como para GFR (al menos 19 características). Por otra parte, los reusltados indican que para el segundo nivel de clasificación es recomendable utilizar todas las características originales para k-Medias.
